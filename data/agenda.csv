,text,label
0,"This paper proposes a hierarchical learning architecture for solving the task of word sense disambiguation with world knowledge, specifically targeting task-specific and background data. The proposed learning architecture leverages both task-specific training data and background data to perform hierarchical semantic classification, which involves the identification and classification of lexical semantic classification problems. The learning architecture is designed to learn representations of words and their contexts that incorporate world knowledge, allowing for more accurate word sense disambiguation. The proposed architecture is evaluated on standard benchmarks and achieves state-of-the-art performance, demonstrating the effectiveness of hierarchical learning for this task. Overall, this paper presents a novel approach for improving word sense disambiguation performance by leveraging world knowledge and hierarchical learning.",1
1,"we present a learning architecture for lexical semantic classification problems that supplements task-specific training data with background data encoding general '' world knowledge '' . the learning architecture compiles knowledge contained in a dictionary-ontology into additional training data , and integrates task-specific and background data through a novel hierarchical learning architecture . experiments on a word sense disambiguation task provide empirical evidence that this '' hierarchical learning architecture '' outperforms a state-of-the-art standard '' flat '' one .",0
2,"Thompson Sampling is a popular Bayesian algorithm for the stochastic multi-armed bandit problem that has shown impressive empirical performance. In this paper, we provide novel regret bounds for Thompson Sampling that are both prior-free and prior-dependent. The prior-free regret bound assumes a non-Bayesian stochastic bandit, making no assumption about the prior distribution. The prior-dependent regret bound, on the other hand, is derived by assuming a prior distribution on the reward distributions of the bandit problem. We also provide distribution-free and distribution-dependent bounds, offering a comprehensive analysis of the performance of Thompson Sampling. Furthermore, we show that our prior-dependent regret bound is tight for a wide range of prior distributions. Finally, we demonstrate the practical usefulness of our bounds through simulations on various bandit problem instances. Our results indicate that our proposed Thompson Sampling algorithm with prior-dependent regret bounds significantly outperforms other state-of-the-art algorithms. Overall, this paper contributes to a better understanding of the performance of Thompson Sampling in stochastic multi-armed bandit problems and provides practical guidelines for its use.",1
3,"we consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions . we are interested in studying prior-free and prior-dependent regret bounds , very much in the same spirit than the usual distribution-free and distribution-dependent bounds for the non-bayesian stochastic bandit . we first show that thompson sampling attains an optimal prior-free bound in the sense that for any prior distribution its bayesian regret is bounded from above by 14 √ nk . this result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a bayesian regret bounded from below by 1 20 √ nk . we also study the case of thompson sampling for the setting of bubeck et al. -lsb- 2013 -rsb- -lrb- where the optimal mean is known as well as a lower bound on the smallest gap -rrb- and we show that in this case the regret of thompson sampling is in fact uniformly bounded over time , thus showing that thompson sampling can greatly take advantage of the nice properties of these thompson sampling .",0
4,"This paper presents an end-to-end spoken information retrieval solution for a personal memory aid system designed to assist the elderly in managing their personal audio archive. The system is built using speech processing components that enable conversational speech to be transcribed and retrieved for later use. The speech processing flow includes automatic speech processing, speaker tracking, and audio transcription. The automatic speech processing module extracts features such as pitch, energy, and spectral information, while the speaker tracking module identifies and separates speakers in the audio stream. The transcription module then converts the audio stream into text, which is stored in a searchable database. The system also includes a user-friendly interface that allows the elderly to retrieve specific spoken conversations from their personal audio archive. We demonstrate the effectiveness of our approach through experiments on real-world data and show that our system achieves high accuracy in speech processing and retrieval. Our system offers a promising solution for the ambient assisted living area, providing elderly individuals with an easy-to-use tool for managing their personal audio archive.",1
5,"the paper presents a new application of automatic speech processing in the ambient assisted living area , developed in the course of a three year research project . recording and automatic processing of spoken conversations plays a major role in this solution enabling effective search in a personal audio archive and fast browsing of conversations . processing of elderly conversational speech recorded by a distant pda microphone poses a great challenge . the speech processing flow includes transcription , speaker tracking and combined indexing and search of spoken terms and participating speakers identity extracted from the audio . we present the entire application and individual speech processing components as well as evaluation results of the individual components and of the end-to-end spoken information retrieval solution .",0
6,"This paper proposes a method for estimating speech recognition error rate without requiring acoustic test data. The proposed method utilizes probabilistic phoneme sequence conversion rules and phonemic confusion models that are learned from phonemic transcription pairs. The confusion models are derived from leave-one-out decoding, which involves iteratively leaving out each transcription pair and decoding the remaining pairs to obtain a probability distribution over word hypotheses. The phonemic transcription pairs are obtained from a text test corpus, which is used to evaluate the performance of the automatic speech recognition system at the phonemic level. The proposed method does not require any acoustic data, making it particularly useful for evaluating the performance of speech recognition systems that operate in noisy environments. We also investigate the impact of different factors, such as grammar design and dialog strategy, on the estimated word error rate. The results demonstrate that our method provides accurate estimates of word error rate and is able to capture important factors that affect the performance of automatic speech recognition systems. Overall, our method offers a practical approach for estimating speech recognition error rate without requiring acoustic test data, facilitating the design cycle of speech recognition systems.",1
7,we address the problem of estimating the word error rate of an automatic speech recognition system without using acoustic test data . this is an important problem which is faced by the designers of new applications which use automatic speech recognition system . quick estimate of word error rate early in the design cycle can be used to guide the decisions involving dialog strategy and grammar design . our approach involves estimating the probability distribution of the word hypotheses produced by the underlying automatic speech recognition system given the text test corpus . a critical component of this system is a phonemic confusion model which seeks to capture the errors made by automatic speech recognition system on the acoustic data at a phonemic level . we use a confusion model composed of probabilistic phoneme sequence conversion rules which are learned from phonemic transcription pairs obtained by leave-one-out decoding of the training set . we show reasonably close estimation of word error rate when applying the system to test sets from different domains .,0
8,"This paper proposes a novel approach to speech recognition based on the product of power spectrum and modified group delay function. Traditional speech recognition systems typically use features such as magnitude spectrum and mel-frequency cepstral coefficients to represent the speech signal. However, these features do not capture the phase spectrum of the signal, which can contain valuable information about the speech. The proposed approach addresses this limitation by incorporating the phase spectrum through the modified group delay function, which is derived from the group delay function of the signal. The modified group delay function is then combined with the power spectrum to generate cepstral features that capture both the magnitude and phase information of the signal. We demonstrate the effectiveness of our approach through experiments on a speech recognition task and show that our method outperforms traditional methods based on magnitude spectrum and mel-frequency cepstral coefficients. Our method offers a promising approach for speech recognition that captures both the magnitude and phase information of the speech signal, enabling more accurate recognition.",1
9,"mel-frequency cepstral coefficients -lrb- mel-frequency cepstral coefficients -rrb- are the most widely used features for speech recognition . these are derived from the power spectrum of the speech signal . recently , the cepstral features derived from the modified group delay function have been studied by murthy and gadde -lsb- 6 -rsb- for speech recognition . in this paper , we propose to use the product of the power spectrum and the group delay function , and derive the mel-frequency cepstral coefficients from the product mel-frequency cepstral coefficients . this mel-frequency cepstral coefficients combines the information from the magnitude spectrum as well as the phase spectrum . the mel-frequency cepstral coefficients of the modified group delay function are also investigated in this paper . results show that the cepstral features derived from the power spectrum perform better than that from the modified group delay function , and the product mel-frequency cepstral coefficients based features provide the best performance .",0
10,"This paper presents a novel approach for streaming three-dimensional graphic scenes in resource-constrained environments. Specifically, we propose a sender-driven mechanism called Partially Ordered Delivery (PODs) that optimizes the delivery of 3D scene data within a limited network bandwidth of 100-kb bit rate. PODs utilizes the decoding independencies of the 3D scene data to prioritize the delivery of certain parts of the scene that are required for the scene to be rendered. This approach improves the rendering quality of the scene while minimizing the amount of data that needs to be transmitted over the network. In addition, we propose a heuristic for selecting the parts of the scene that should be prioritized for delivery based on their importance for rendering the scene. Our experiments demonstrate that PODs outperforms existing approaches for streaming 3D scenes in resource-constrained environments, achieving higher rendering quality while reducing the amount of data transmitted. Our work provides a promising solution for delivering high-quality 3D scenes over limited network bandwidths in resource-constrained environments.",1
11,"three-dimensional -lrb- 3d -rrb- graphic scenes require considerable network bandwidth to be transmitted and computing power to be rendered on users ' terminals . toward high-quality display in real time , we propose a sender-driven mechanism for streaming 3d scenes in a resource-constrained environment . by pre-processing the database , objects in the scene are properly weighted upon their rendering importance , and their resolutions are selected accordingly to reduce the bit rate . partially ordered delivery is then performed using decoding independencies between the objects . simulation results show the efficacy of the proposed sender-driven mechanism . for a test benchmark , for example , the proposed algorithm outperforms the comparing heuristic by 4 db under a 100-kb bit rate .",0
12,"This paper proposes a novel approach to model the geometric structure and illumination variation of a scene from real images. The goal is to obtain the 3D geometric information of the scene structure and the low-dimensional linear space of the illumination conditions. To achieve this, the authors combine structure-from-motion and correlation-based stereo techniques to estimate the position and structure of the objects in the scene. They also use basis images to represent the illumination variations. The resulting virtual reality scene model can be used for object recognition and photometric information analysis. The proposed method is tested on real images and shows promising results in capturing the geometric structure and illumination variation of the scene. The approach has potential applications in product advertisement and virtual reality.",1
13,"we present in this paper a system which automatically builds , from real images , a scene model containing both 3d geometric information of the scene structure and its photometric information under various illumination conditions . the geometric structure is recovered from images taken from distinct viewpoints . structure-from-motion and correlation-based stereo techniques are used to match pix-els between images of different viewpoints and to reconstruct the scene in 3d space . the geometric structure is extracted from images taken under different illumination conditions -lrb- orientation , position and intensity of the light sources -rrb- . this is achieved by computing a low-dimensional linear space of the spatio-illumination volume , and is represented by a set of basis images . the model that has been built can be used to create realistic renderings from different viewpoints and illumination conditions . applications include object recognition , virtual reality and product advertisement .",0
14,"This paper proposes a new approach for object detection using contextual models based on boosted random fields. The approach utilizes local image data to model the object detection problem as a dense graph, with graph fragments used to capture contextual information. Boosting is used to increase the detection accuracy while maintaining computational efficiency through a computational cascade. The resulting model is an additive combination of the dense graph and graph fragments, with boosted random fields used to learn the graph structure. The proposed approach achieves state-of-the-art results on benchmark datasets while maintaining real-time speed, making it a promising solution for object detection in various applications.",1
15,"we seek to both detect and segment objects in images . to exploit both local image data as well as contextual information , we introduce boosted random fields , which uses boosting to learn the graph structure and local evidence of a conditional random field . the graph structure is learned by assembling graph fragments in an additive model . the connections between individual pixels are not very informative , but by using dense graphs , we can pool information from large regions of the image ; dense models also support efficient inference . we show how contextual information from other objects can improve detection performance , both in terms of accuracy and speed , by using a computational cascade . we apply our system to detect stuff and things in office and street scenes . 1 .",0
16,"This paper discusses the computational complexity of dominance and consistency in CP-nets, which are graphical models used to represent preferences in decision-making problems. CP-nets are defined in terms of a directed acyclic graph that encodes dependencies among variables and a set of conditional preference statements. The authors consider the computational complexity of determining dominance and consistency in CP-nets, which are important properties for solving decision problems. They analyze the complexity of these problems in different restricted classes of CP-nets, including those with restricted dependency graphs, restricted conditional preference statements, and restricted topologies. The authors show that dominance and consistency are both in the complexity class PSPACE-complete, which means that they are computationally expensive to solve. The paper provides important insights into the computational complexity of dominance and consistency in CP-nets, which can inform the design of algorithms for solving decision problems.",1
17,"we investigate the computational complexity of testing dominance and consistency in cp-nets . up until now , the complexity of dominance has been determined only for restricted classes in which the dependency graph of the cp-nets is acyclic . however , there are preferences of interest that define cyclic dependency graphs ; cyclic dependency graphs are modeled with general cp-nets . we show here that both dominance and consistency testing for general cp-nets are pspace-complete . the reductions used in the proofs are from strips planning , and thus establish strong connections between both areas .",0
18,"This paper presents an automatic classification approach for distinguishing between oral and nasal snoring sounds based on their acoustic properties. The study utilizes k-nearest neighbor classifier and cross validation evaluations to categorize snoring sounds. The proposed approach can potentially aid in the diagnosis of sleep disorders such as sleep apnea syndrome, sleep disordered breathing, and heavy snoring. It also has practical implications for differentiating between benign snorers and those who may require medical treatment. The results suggest that acoustic properties can be a reliable indicator for differentiating between oral and nasal snoring sounds.",1
19,"snoring was once regarded as an indication of good sleep . but recently it has been known to be one of the symptoms which indicate sleep disordered breathing such as sleep apnea syndrome . moreover , heavy snoring caused by oral breathing sometimes leads benign snorers to be apneics . thus , it is important to detect oral snoring for medical treatment in the earlier stage , but we can not know our own snoring . this paper describes a method to detect oral snoring by extracting the acoustic properties of snoring sounds and using the k-nearest neighbor classifier . as a result , over 92 % of snoring sounds are successfully classified under the various cross validation evaluations .",0
20,"This paper proposes a method for automatically extracting resource terms for sentiment analysis. The method is based on a bipartite graph model that represents the relationships between resource terms (such as gas, water, and electricity) and sentiment words (such as positive or negative opinions). The proposed iterative algorithm uses real-life sentiment corpora to extract resource terms that are most commonly associated with positive or negative sentiment. The algorithm is evaluated using several documents containing sentiments about different resources. Results show that the proposed method effectively identifies relevant resource terms for sentiment analysis.",1
21,"existing research on sentiment analysis mainly uses sentiment words and phrases to determine sentiments expressed in documents and sentences . techniques have also been developed to find such words and phrases using dictionaries and domain corpora . however , there are still other types of words and phrases that do not bear sentiments on their own , but when they appear in some particular contexts , they imply positive or negative opinions . one class of such words or phrases is those that express resources such as water , electricity , gas , etc. . for example , '' this washer uses a lot of electricity '' is negative but '' this washer uses little water '' is positive . extracting such resource words and phrases are important for sentiment analysis . this paper formulates the problem based on a bipartite graph and proposes a novel iterative algorithm to solve the problem . experimental results using diverse real-life sentiment corpora show good results .",0
22,"This paper proposes a method for regularized Laplacian estimation and fast eigenvector approximation. The authors introduce regularized semi-definite programs for Laplacian estimation problems, which can be 2-regularized or 1-regularized. They also present a fast diffusion-based procedure for computing nontrivial eigenvectors of the regularized Laplacian matrix, which is based on the Mahoney-Orecchia approximate eigenvector computation algorithm. The proposed method can be applied to various regression problems, including ridge regression and lasso regression. The authors evaluate the method on 2-regression data graphs and real-world networks and show that their approach outperforms other diffusion-based procedures and regularized estimates with Gaussian prior.",1
23,"recently , mahoney and orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph laplacian exactly solve certain regularized semi-definite programs . in this paper , we extend that result by providing a statistical interpretation of their approximation procedure . our interpretation will be analogous to the manner in which 2-regularized or 1-regularized 2-regression -lrb- often called ridge regression and lasso regression , respectively -rrb- can be interpreted in terms of a gaussian prior or a laplace prior , respectively , on the coefficient vector of the regression problem . our framework will imply that the solutions to the mahoney-orecchia regularized sdp can be interpreted as regularized estimates of the pseu-doinverse of the data graph laplacian . conversely , it will imply that the solution to this regularized estimation problem can be computed very quickly by running , e.g. , the fast diffusion-based pagerank procedure for computing an approximation to the first nontrivial eigenvector of the data graph laplacian . empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization , relative to running the corresponding exact algorithm .",0
24,"This paper proposes an approach to network alarm triage that combines the strengths of both human decision-making and machine learning. The system integrates low-level device health information with interactive machine learning techniques to improve the accuracy and speed of triaging decisions made by operators. Specifically, the system uses rule-based tools to group alarms and then employs a CuET algorithm for interactive machine learning to fine-tune the triage process. The authors demonstrate that their human-guided machine learning approach results in improved accuracy and faster triage times compared to traditional rule-based or machine learning methods alone.",1
25,"network alarm triage refers to grouping and prioritizing a stream of low-level device health information to help operators find and fix problems . today , this process tends to be largely manual because existing rule-based tools can not easily evolve with the network . we present cuet , a cuet that uses interactive machine learning to constantly learn from the triaging decisions of operators . cuet then uses that learning in novel visualizations to help them quickly and accurately triage alarms . unlike prior interactive machine learning systems , cuet handles a highly dynamic environment where the groups of interest are not known a priori and evolve constantly . our evaluations with real operators and data from a large network show that cuet significantly improves the speed and accuracy of alarm triage .",0
26,"This paper proposes a new algorithm for clustering subspaces based on fitting, differentiating, and dividing polynomials. The proposed algorithm, called GPCA, utilizes algebraic techniques such as linear and polynomial algebra, polynomial factorization, and iterative techniques such as subspace initialization and k-subspace PCA. The algorithm is evaluated on several computer vision problems such as news video segmentation, face clustering, and vanishing point detection, and is compared to other algebraic algorithms. The paper also discusses the use of GPCA in interactive machine learning systems for triaging network alarms. The algorithm achieves high accuracy in clustering subspaces and is shown to be effective in solving clustering problems with polynomial structures.",1
27,"we consider the problem of clustering data lying on multiple subspaces of unknown and possibly different dimensions . we show that one can represent the subspaces with a set of polynomials whose derivatives at a data point give normal vectors to the subspace associated with the data point . since the polynomials can be estimated linearly from data , subspace clustering is reduced to classifying one point per subspace . we do so by choosing points in the data set that minimize a distance function . a basis for the complement of each subspace is then recovered by applying standard pca to the set of derivatives -lrb- normal vectors -rrb- at those points . the final result is a new gpca algorithm for subspace clustering based on simple linear and polynomial algebra . our experiments show that our gpca algorithm outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as k-subspace and em . we also present applications of pca on computer vision problems such as vanishing point detection , face clustering , and news video segmentation .",0
28,This paper proposes a method to estimate the semantic case of Japanese dialogue using a measure of distance derived from the statistics of dependent noun-particle-verb triples. The authors use the ATR dialogue corpus and apply clustering and correlation analysis to evaluate the accuracy of their method. They report consistent rates for single-case clusters and show that their method achieves high estimation accuracies. The study demonstrates the potential of using statistics of dependency for analyzing Japanese dialogue.,1
29,"in an attempt to estimate the semantic cases for noun-particle-verb triples in the atr dialogue corpus , the authors propose a measure of distance based on statistics of dependent noun-particle-verb triples . a clustering analysis of all the triples in the corpus was conducted using the measure of distance . competence of the proposed measure of distance is verified by examination of the distribution of the single-case clusters . by use of the score derived from the measure of distance of the training corpus , the authors conducted the estimation of the correct semantic case for a given noun-particle-verb triples in the test corpus . the result remarkably differentiates the particles with respect to the estimation accuracies . for instance , particle ` wo ' has accuracies over 80 % , while ` de ' has accuracies less than 40 % . the correlation analysis between the accuracy and the consistency rates indicates that the particles of higher consistency have also tendencies to higher accuracies .",0
30,"This paper presents a signal processing perspective on moving shape dynamics, specifically on the characterization and recognition of human motion sequences. The use of sophisticated signal processing techniques has been an area of interest in the human motion analysis and motion dynamics community. The paper discusses various signal transform methods such as discrete Fourier transform and discrete wavelet transform, and how they are used in discriminating features in the articulated motion of human beings. The authors highlight the importance of using these signal processing techniques in motion analysis, and their potential impact in future research. The paper also mentions the use of large data sets to demonstrate the effectiveness of these methods in human motion analysis.",1
31,"this paper provides a new perspective on human motion analysis , namely regarding human motions in video as general discrete time signals . while this seems an intuitive idea , research on human motion analysis has attracted little attention from the signal processing community . sophisticated signal processing techniques create important opportunities for new solutions to the problem of human motion analysis . this paper investigates how the deformations of human silhouettes -lrb- or shapes -rrb- during articulated motion can be used as discriminating features to implicitly capture motion dynamics . in particular , we demonstrate the applicability of two widely used signal transform methods , namely the discrete fourier transform and discrete wavelet transform , for characterization and recognition of human motion sequences . experimental results show the effectiveness of the proposed method on two state-of-the-art data sets .",0
32,"This paper proposes a novel approach to improve the channel selection of sound coding algorithms in resource-limited and time critical devices such as cochlear implants. Commercial cochlear implant processors often use spectral maxima sound coding algorithms that may not always provide the best speech intelligibility for cochlear implant users, particularly in noisy scenarios. This paper presents a sound coding framework that incorporates a channel selection criterion based on the formant location of speech and the discrimination of noise-dominant channels. The proposed approach improves speech intelligibility in noise scenarios by selectively stimulating channels that provide higher instantaneous signal-to-noise ratio. The paper evaluates the proposed framework using n-of-m strategies and time-frequency unit perception quality measures and shows that it outperforms the state-of-the-art sound coding algorithms in cochlear implants.",1
33,"spectral maxima sound coding algorithms , for example n-of-m strategies , used in commercial cochlear implant devices rely on selecting channels with the highest energy in each frequency band . this spectral maxima sound coding algorithms works well in quiet , but is inherently problematic in noisy conditions when noise dominates the target , and noise-dominant channels are mistakenly selected for stimulation . a new channel selection criterion is proposed to addresses this shortcoming which adaptively assigns weights to each time-frequency unit based on the formant location of speech and instantaneous signal to noise ratio . the performance of the proposed spectral maxima sound coding algorithms is evaluated acutely with three cochlear implant users in different noise scenarios . results indicate that the proposed spectral maxima sound coding algorithms improves speech intelligibility and perception quality , particularly at low signal-to-noise ratio . significance of the proposed spectral maxima sound coding algorithms lies in its ability to be integrated with the existing sound coding framework employed within commercial cochlear implant processors , making spectral maxima sound coding algorithms easier to adapt for resource-limited and time critical devices .",0
34,"This paper presents a novel approach to jointly estimate disparity and optical flow by correspondence growing. The proposed method reduces the inherent search space of temporally coherent stereo disparity and optical flow maps by using a seed growing algorithm. By reconstructing the 3D point of the scene flow, the proposed method estimates the 3D velocity vector. The approach is demonstrated on a binocular stereo setup and evaluated on disparity maps and optical flow images. The experimental results show that the proposed method can effectively estimate joint disparity and optical flow and outperforms state-of-the-art methods. The proposed approach has the potential to improve calibration, scene flow, and other related computer vision tasks.",1
35,"the scene flow in binocular stereo setup is estimated using a seed growing algorithm . a pair of calibrated and synchronized cameras observe a scene and output a sequence of images . the algorithm jointly computes a disparity map between the stereo images and optical flow maps between consecutive frames . having the calibration , this is a representation of the scene flow , i.e. a 3d velocity vector is associated with each reconstructed 3d point . the proposed algorithm starts from correspondence seeds and propagates the correspondences to the neighborhood . it is accurate for complex scenes with large motion and produces temporally coherent stereo disparity and optical flow results . the algorithm is fast due to inherent search space reduction .",0
36,"This paper proposes an approach for in-service adaptation of multilingual hidden-Markov-models (HMMs) for automatic speech recognition (ASR) systems. The approach aims to improve recognition accuracy in target tasks by incrementally adapting task-dependent models from a task-independent seed model. The proposed method uses unsupervised online adaptation techniques to adapt the HMM parameters using acoustic data from the target task. The system is evaluated on recognizing Slovene digits using a multilingual modeling approach with a Romano-Germanic seed model, and the results demonstrate the effectiveness of the proposed approach, especially when using language-dependent models. The paper shows that this approach can reduce the inherent limitations of multilingual models and improve the recognition accuracy of ASR systems in target tasks.",1
37,"in this paper we report on advances regarding our approach to porting an automatic speech recognition system to a new target task . in case there is not enough acoustic data available to allow for thorough estimation of hmm parameters it is impossible to train an appropriate model . the basic idea to overcome this problem is to create a task independent seed model that can cope with all tasks equally well . however , the performance of such generalist model is of course lower than the performance of task dependent models -lrb- if these were available -rrb- . so , the task independent seed model is gradually enhanced by using its own recognition results for incremental online task adaptation . here , we use a multilingual romanic/germanic seed model for a slavic target task . in tests on slovene digits multilingual modeling yields the best recognition accuracy compared to other language dependent models . applying unsupervised online task adaptation we observe a remarkable boost of recognition performance .",0
38,"This paper presents a novel approach for lexicalized phonotactic word segmentation using a discriminative model of boundary marking. The proposed method is capable of inferring word boundaries in morphologically complex words, and it is evaluated on miniature datasets of transcribed adult conversations as well as child language acquisition. The approach relies on phone n-grams and phonetic transcriptions to improve speech understanding, and it is shown to outperform unsupervised algorithms for word segmentation in both English and Arabic. The results demonstrate the effectiveness of the proposed approach for improving the accuracy of word segmentation by using information about the phonotactics of words.",1
39,"this paper presents a new unsupervised algorithm -lrb- wordends -rrb- for inferring word boundaries from transcribed adult conversations . phone ngrams before and after observed pauses are used to bootstrap a simple dis-criminative model of boundary marking . this fast algorithm delivers high performance even on morphologically complex words in english and arabic , and promising results on accurate phonetic transcriptions with extensive pronunciation variation . expanding training data beyond the traditional miniature datasets pushes performance numbers well above those previously reported . this suggests that wordends is a viable model of child language acquisition and might be useful in speech understanding .",0
40,"This study investigates the perception of non-native phonemes in noisy environments. Specifically, the authors examine the identification of vowels and consonants in multispeaker babble for Dutch listeners compared to native listeners of American English. The study evaluates the effect of signal-to-noise ratios and syllable position on phoneme identification accuracy. The results indicate that consonants are more challenging to identify than vowels in noise, with Dutch listeners performing worse than American English native listeners across both consonants and vowels. These findings provide insights into the perception of non-native phonemes in noisy environments and have implications for speech perception and communication.",1
41,"we report an investigation of the perception of american english phonemes by dutch listeners proficient in english . listeners identified either the consonant or the vowel in most possible english cv and vc syllables . the syllables were embedded in multispeaker babble at three signal-to-noise ratios -lrb- 16 db , 8 db , and 0 db -rrb- . effects of signal-to-noise ratio on vowel and consonant identification are discussed as a function of syllable position and of relationship to the native phoneme inventory . comparison of the results with previously reported data from native listeners reveals that noise affected the responding of native and non-native listeners similarly .",0
42,"This paper proposes a new method for weakly-supervised object detection called ""Track and Transfer."" The status quo approach relies on weakly-labeled images, but this new method leverages weakly-labeled videos to simulate strong human supervision. The proposed method uses a Hough transform algorithm to track object boxes in videos and generate pseudo ground-truth boxes, which are then transferred to weakly-labeled images to train object detectors. The PASCAL 2007 and 2010 datasets, which have manually annotated bounding boxes, are used to evaluate the discriminative regions learned by the proposed method. The experimental results show that the proposed method outperforms the status quo approach in terms of mean average precision.",1
43,"the status quo approach to training object detectors requires expensive bounding box annotations . our status quo approach takes a markedly different direction : we transfer tracked object boxes from weakly-labeled videos to weakly-labeled images to automatically generate pseudo ground-truth boxes , which replace manually annotated bounding boxes . we first mine discriminative regions in the weakly-labeled image collection that frequently/rarely appear in the posi-tive/negative images . we then match those regions to videos and retrieve the corresponding tracked object boxes . finally , we design a hough transform algorithm to vote for the best box to serve as the pseudo gt for each image , and use hough transform algorithm to train an object detector . together , discriminative regions lead to state-of-the-art weakly-supervised detection results on the pascal 2007 and 2010 datasets .",0
44,"This paper investigates the effects of age and culture on the perception of emotional speech in game-playing children using audiovisual stimuli. Cross-cultural perception studies were conducted with Pakistani children, and the classification accuracy of their perception of emotional speech was evaluated. The results of the study suggest that age and culture can have an impact on the perception of emotional speech in game-playing children, and cross-cultural differences should be taken into account in the design of multimedia applications aimed at children from diverse cultural backgrounds.",1
45,"in this paper we study how children of different age groups -lrb- 8 and 12 years old -rrb- and with different cultural backgrounds -lrb- dutch and pakistani -rrb- signal positive and negative emotions in audiovisual speech . data was collected in an ethical way using a simple but surprisingly effective game in which pairs of participants have to guess whether an upcoming card will contain a higher or lower number than a reference card . the data thus collected was used in a series of cross-cultural perception studies , in which dutch and pakistani observers classified emotional expressions of dutch and pakistani speakers . results show that classification accuracy is uniformly high for pakistani children , but drops for older and for winning dutch children 1 .",0
46,"This paper compares the performance of two methods, Expectation-Maximization (EM) and Gaussian Mixture Vector Quantization (GMVQ), for estimating Gaussian mixture models. The focus of the study is on their application to probabilistic image retrieval, which requires efficient compression and classification of image data. The paper evaluates the two methods in terms of their computational complexity, convergence speed, and accuracy of parameter estimation. The results show that while EM provides better accuracy, GMVQ offers faster convergence and lower computational complexity. The study provides useful insights for researchers and practitioners working on image retrieval systems that require efficient modeling of complex data distributions.",1
47,"the expectation-maximization is the dominant algorithm for estimating the parameters of a gauss mixture . recently , gauss mixture vector quantization based on the lloyd algorithm has been applied successfully as an alternative for both compression and classification . we investigate the performance of the two algorithms for gm 's in image retrieval . the asymptotic likelihood approximation is used as a similarity criterion to compare gm 's directly . the two algorithms result in very close retrieval performance . we demonstrate that the closeness comes from the close mutual approximation of the estimated gm parameter values and that the two algorithms have similar convergence speed . our analysis shows that gauss mixture vector quantization has roughly half the computational complexity of em .",0
48,"This paper proposes a new approach to guiding semi-supervised learning algorithms with the use of domain knowledge and constraint-driven learning. The proposed method aims to improve the performance of structured learning tasks, such as natural language processing and information extraction, by incorporating domain-specific constraints to the learning process. By leveraging the available domain knowledge, the algorithm can effectively guide the semi-supervised learning process, even in cases where only limited labeled data is available. The proposed method is evaluated on several benchmark datasets, demonstrating its effectiveness in improving classification accuracy and outperforming existing semi-supervised learning approaches. Overall, this work presents a promising approach for leveraging domain knowledge to improve machine learning models in domains where labeled data is scarce.",1
49,"over the last few years , two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce , and the study of ways to exploit knowledge and global information in structured learning tasks . in this paper , we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms . our novel framework unifies and can exploit several kinds of task specific constraints . the experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning , and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks .",0
50,"This paper presents a kernel-based reinforcement learning approach to solve average-cost problems in the context of optimal portfolio choice. The problem is modeled as a Markov decision process, and the learning algorithm is based on temporal-difference learning with parametric function approximators. The proposed approach uses a kernel-based method to improve the generalization performance of the learning algorithm, which is tested using neural networks. The results show that the kernel-based approach outperforms the traditional method in terms of convergence speed and overall performance. The study highlights the potential of using kernel-based reinforcement learning in financial decision-making problems.",1
51,"many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-diierence learning to estimate the value function of a markov decision process . a signiicant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable . in this work , we present a new , kernel-based approach to reinforcement learning which overcomes this diiculty and provably converges to a unique solution . by contrast to existing learning algorithms , our kernel-based approach can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically . our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem .",0
52,"This paper proposes a block-iterative quadratic signal recovery algorithm for solving quadratic minimization problems subject to convex constraints. The algorithm is based on a block-parallel structure, which allows for efficient constraint enforcement through approximate enforcement. Additionally, the algorithm is able to handle multi-constraint problems by linearizing them. The proposed method is evaluated on deconvolution problems and is shown to outperform existing approaches in terms of convergence speed and accuracy. The block-iterative parallel decomposition method is identified as the key component of the algorithm, enabling it to efficiently solve quadratic signal recovery problems.",1
53,we propose a block-iterative parallel decomposition method to solve quadratic signal recovery problems under convex constraints . the idea of the block-iterative parallel decomposition method is to disintegrate the original multi-constraint problem into a sequence of simple quadratic minimizations over the intersection of two half-spaces constructed by linearizing blocks of constraints . the implementation of the block-iterative parallel decomposition method is quite exible thanks to its block-parallel structure . in addition a wide range of complex constraints can be incorporated since the block-iterative parallel decomposition method does not require exact constraint enforcement at each step but merely approximate enforcement via linearization . an application to deconvolution is demonstrated .,0
54,"This paper presents an optimization method for layered neural networks based on the modified information criterion. The proposed method aims to determine the optimal number of hidden layers and the number of neurons in each layer. The modified information criterion is used to evaluate the performance of the neural network, which is based on the likelihood function of the model and a penalty term for model complexity. The optimization method is evaluated on several benchmark datasets and compared with other existing methods. The experimental results show that the proposed method can effectively improve the performance of the neural network by finding the optimal architecture. The modified information criterion is shown to be a useful measure for assessing the complexity of the neural network and avoiding overfitting.",1
55,"this paper proposes a practical optimization method for layered neural networks , by which the optimal model and parameter can be found simultaneously . ` i \ te modify the conventional information criterion into a differentiable function of parameters , and then , minimize information criterion , while controlling information criterion back to the ordinary form . effectiveness of this optimization method is discussed theoretically and experimentally .",0
56,"This paper presents a speech-to-speech translation system for monologues using a data-driven approach. Specifically, the system focuses on translating Japanese-to-English in the context of TV news and commentary programs, which have unique speaking styles and require the realization of simultaneity. The system is based on a parallel corpus and uses a data-driven approach, with translation rules and absolute size incorporated. The effectiveness of the system is demonstrated through experiments and evaluations, showing improved translation accuracy for these types of monologue speech.",1
57,"this paper describes ongoing research on a japanese-to-english speech-to-speech translation system for '' controlled monologue '' , such as tv news and commentary programs in which the speaking styles are controlled as a monologue . we have adopted the data-driven approach since the tv programs in question cover a wide range of topics , and because it seems much too labor intensive to handcraft translation rules . the data-driven approach therefore requires a gigantic parallel corpus for the target domain , although the absolute size needed is not so easy to obtain . there are also difficulties inherent in monologue such as the need to handle long sentences , averaging over 25 words , and the realization of simultaneity . these problems are presented in the light of our available corpora and we go on to present the kinds of problems we have to solve . finally , we present our prospective system architecture and introduce the present status of the work .",0
58,"This paper presents an efficient approach for designing nearly perfect-reconstruction cosine-modulated and modified DFT filter banks. The design aims to minimize aliasing and amplitude errors, subject to constraints such as the maximum allowable aliasing. The proposed two-step algorithms involve optimizing the filter orders and lengths, and the prototype filter, respectively. The start-up solution is obtained through nonlinear optimization, and the minimax stopband response is achieved through linear programming. The approach is compared with perfect-reconstruction filter banks, and it is shown to be more efficient in terms of both computational complexity and filter lengths. The results demonstrate that the proposed approach can achieve nearly perfect reconstruction with low aliasing errors and start-up transient.",1
59,"efficient two-step algorithms are described for optimizing the stopband response of the prototype filter for cosine-modulated and modified dft filter banks either in the minimax or in the least-mean-square sense subject to the maximum allowable aliasing and amplitude errors . the first step involves finding a good start-up solution using a simple technique . this start-up solution is improved in the second step by using nonlinear optimization . several examples are included illustrating the flexibility of the proposed start-up solution for making compromises between the required filter lengths and the aliasing and amplitude errors . these examples show that by allowing very small amplitude and aliasing errors , the stopband performance of the resulting prototype filter is significantly improved compared to the corresponding perfect-reconstruction filter bank . alternatively , the filter orders and , consequently , the overall delay can be significantly reduced to achieve practically the same performance .",0
60,This paper proposes an improved speaker diarization system for meeting speech. The system employs a recurrent selection of representative speech segments and participant interaction pattern modeling to improve the accuracy of speaker clustering. The proposed method takes into account robust cluster modeling perspective and conversation patterns in meeting speech corpora to extract priori information for speaker clustering procedures. Experimental results show that the proposed method outperforms the baseline system in terms of di-arization error rate. The study provides evidence that the use of representative speech segments and participant interaction pattern modeling can significantly improve the accuracy of speaker diarization in meeting speech.,1
61,"in this work we describe two distinct novel improvements to our speaker diarization system , previously proposed for analysis of meeting speech . the first approach focuses on recurrent selection of representative speech segments for speaker clustering while the other is based on participant interaction pattern modeling . the former selects speech segments with high relevance to speaker clustering , especially from a robust cluster modeling perspective , and keeps updating them throughout clustering procedures . the latter statistically models conversation patterns between meeting participants and applies it as a priori information when refining diarization results . experimental results reveal that the two proposed approaches provide performance enhancement by 29.82 % -lrb- relative -rrb- in terms of di-arization error rate in tests on 13 meeting excerpts from various meeting speech corpora .",0
62,"This paper presents a study on modeling complex cells in an awake macaque during natural image viewing. The authors explore the classical energy mechanism and nonclassical gain control, as well as suppressive surround effects and delayed suppressive surround in the visual area VI. The study is conducted using a stimulus sequence of natural images to observe the response of the cells. The results show that the nonclassical gain control plays a critical role in the cells' responses, indicating the importance of incorporating this mechanism into future models of visual processing.",1
63,"we model the responses of cells in visual area vi during natural vision . our model consists of a classical energy mechanism whose output is divided by nonclassical gain control and texture contrast mechanisms . we apply this model to review movies , a stimulus sequence that replicates the stimulation a cell receives during free viewing of natural images . data were collected from three cells using five different review movies , and the model was fit separately to the data from each movie . for the energy mechanism alone we find modest but significant correlations -lrb- re = 0.41 , 0.43 , 0.59 , 0.35 -rrb- between model and data . these correlations are improved somewhat when we allow for suppressive surround effects -lrb- re + g = 0.42 , 0.56 , 0.60 , 0.37 -rrb- . in one case the inclusion of a delayed suppressive surround dramatically improves the fit to the data by modifying the time course of the model 's response .",0
64,"This paper presents a novel method for speech analysis using the short-time chirp transform (STCT), a time-frequency analysis tool based on quadratic chirps. The STCT is used to obtain a segment-by-segment spectral representation of voiced speech, allowing for a more precise analysis of pitch tendency and other acoustic features. Compared to the traditional short-time Fourier transform, the STCT provides a more consistent time-frequency localization and avoids inconsistent bins. The blurry harmonic representation of the STCT can also be used for filtering purposes. Overall, the results demonstrate the potential of the STCT as a powerful analysis tool for speech processing.",1
65,"the most popular time-frequency analysis tool , the short-time fourier transform , suffers from blurry harmonic representation when voiced speech undergoes changes in pitch . these relatively fast variations lead to inconsistent bins in frequency domain and can not be accurately described by the fourier analysis with high resolution both in time and frequency . in this paper a new analysis tool , called short-time fourier transform is presented , offering more precise time-frequency representation of speech signals . the base of this short-time fourier transform is composed of quadratic chirps that follow the pitch tendency segment-by-segment . comparative results between the proposed short-time fourier transform and popular time-frequency techniques reveal an improvement in time-frequency localization and finer spectral representation . since the signal can be resynthesized from its short-time fourier transform , the proposed analysis tool is also suitable for filtering purposes .",0
66,"This paper proposes a one-microphone algorithm for enhancing reverberant speech. Reverberation can degrade speech quality and intelligibility, making it difficult to understand in noisy environments. The proposed method estimates the reverberation time using a pitch-based measure and then uses this estimate to separate the direct and reflected sound components. By considering the relative time lags between these components, the method is able to suppress the effects of reverberation and enhance speech intelligibility. The algorithm also takes into account the harmonic structure and pitch strength of the speech signal to remove any residual echo components caused by reverberation. The experimental results demonstrate the effectiveness of the proposed method in improving the quality and intelligibility of reverberant speech using only one microphone.",1
67,we present an algorithm for reverberant speech enhancement using one microphone . we first propose a novel pitch-based reverberation measure for estimating reverberation time based on the distribution of relative time lags . this measure of pitch strength correlates with reverberation and decreases systematically as detrimental effects of reverberation on harmonic structure increase . then a reverberant speech enhancement method is developed to estimate and subtract later echo components . the results show that our approach appreciably reduces reverberation effects .,0
68,"This paper presents a theoretical analysis of fundamental properties of non-negative impulse response filters, focusing on the theoretical bounds of frequency-domain performance. The non-negative impulse response filter is defined in both continuous and discrete-time domains, and its geometrically spaced frequency regions are analyzed in terms of their maximally allowable power attenuation. The paper derives upper-bounds on power spectral attenuation and power spectral gain, which provide a useful tool for evaluating the performance of non-negative impulse response filters. The results demonstrate that the theoretical bounds of frequency-domain performance are important for understanding the behavior of these filters, and provide insights into their design and implementation.",1
69,"this paper presents several fundamental frequency-domain bounds for a non-negative impulse response filter . upper-bounds on power spectral attenuation and power spectral gain in geometrically spaced frequency regions are derived , when power spectral attenuation near frequency zero is limited . by analyzing the tightnesses of these bounds , the relationship between the maximally allowable power attenua-tion and gain is also treated . all results hold for both continuous and discrete-time domains .",0
70,"This paper presents language splitting and relevance-based belief change in Horn logic. It introduces a relevance-based partial meet Horn contraction operator based on Parikh's relevance criterion and Parikh's relevance postulate. The paper also examines the parallel interpolation theorem in propositional Horn logic and its relationship to classical propositional logic. Additionally, the paper provides a representation theorem for Horn formulae that allows for a succinct description of Horn logic theories. These results advance our understanding of Horn logic and its applications in belief change and language processing.",1
71,"this paper presents a framework for relevance-based belief change in propositional horn logic . we firstly establish a parallel interpolation theorem for horn logic and show that parikh 's finest splitting theorem holds with horn formulae . by reformulating parikh 's relevance criterion in the setting of horn belief change , we construct a relevance-based partial meet horn contraction operator and provide a representation theorem for the operator . interestingly , we find that this relevance-based partial meet horn contraction operator can be fully characterised by delgrande and wassermann 's postulates for partial meet horn contraction as well as parikh 's relevance postulate without requiring any change on the postulates , which is qualitatively different from the case in classical propositional logic .",0
72,"This study investigates the location of prosodic boundaries in Chinese Mandarin using a large speech corpus and serial acoustic parameters. The study focuses on the type and location of prosodic boundaries in relation to acoustic parameters, such as syllable duration and intensity. A statistical model is developed to evaluate boundary efficiency and acoustic features, and the study uses the CART algorithm to analyze the data. The results provide insights into the relationship between prosodic boundaries and acoustic parameters, which could have implications for speech recognition and other applications.",1
73,"in this paper , based on large speech corpus with prosodic structure label -lrb- asccd -rrb- , we present some statistic result on acoustic parameter at prosodic boundary . we study the syllable duration , intensity and pitch at the boundary and select a serial acoustic parameter to train a cart . then the cart was employed to classify the prosodic boundary type . the result shows that the parameter characterize acoustic feature of the prosodic boundary and the trained cart can classify different boundary efficiency . so it is possible to train statistical model for prosodic boundary location in mandarin , this is very important both for speech recognition and synthesis .",0
74,"This paper presents a super-resolution algorithm for enhancing the quality of noisy images. The algorithm consists of two main steps: denoising and super-resolution. In the denoising step, a convex combination of different denoising algorithms is applied to recover the missing signal in the image. Then, in the super-resolution step, a patch-similarity-based algorithm is used to recover the missing textural details of the high-resolution image. The algorithm employs numerical metrics to evaluate the quality of the noisy and denoised images. The proposed algorithm shows improved performance in terms of super-resolving noisy images and restoring lost signal and textural details in the HR domain.",1
75,"our goal is to obtain a noise-free , high resolution image , from an observed , noisy , low resolution -lrb- lr -rrb- image . the conventional approach of preprocessing the image with a denoising algorithm , followed by applying a super-resolution algorithm , has an important limitation : along with noise , some high frequency content of the image -lrb- particularly textural detail -rrb- is invariably lost during the denoising step . this ` denoising loss ' restricts the performance of the subsequent sr step , wherein the challenge is to synthesize such textural details . in this paper , we show that high frequency content in the noisy image -lrb- which is ordinarily removed by denoising algorithms -rrb- can be effectively used to obtain the missing textural details in the hr domain . to do so , we first obtain hr versions of both the noisy and the denoised images , using a patch-similarity based sr algorithm . we then show that by taking a convex combination of orientation and frequency selective bands of the noisy and the denoised hr image , we can obtain a desired hr image where -lrb- i -rrb- some of the textural signal lost in the denoising step is effectively recovered in the hr domain , and -lrb- ii -rrb- additional textures can be easily synthesized by appropriately constraining the parameters of the convex combination . we show that this part-recovery and part-synthesis of textures through our algorithm yields hr image that are visually more pleasing than those obtained using the conventional processing pipeline . furthermore , our results show a consistent improvement in numerical metrics , further corroborating the ability of our algorithm to recover lost signal .",0
76,"This paper explores why many decisions in sequential decision problems, such as the popular game Tetris, are relatively easy to make. The authors suggest that these decisions are easy due to the game's design, which promotes cumulative dominance and does not require domain knowledge. The paper discusses these concepts in the context of decision problems, learning algorithms, and evaluation functions. Additionally, the authors argue that the concept of noncompensation, where decisions in one part of the game do not affect future decisions, also contributes to the ease of decision-making in Tetris. The findings of this study may have implications for other sequential decision problems beyond Tetris.",1
77,"we examined the sequence of decision problems that are encountered in the game of tetris and found that most of the problems are easy in the following sense : one can choose well among the available actions without knowing an evaluation function that scores well in the game . this is a consequence of three conditions that are prevalent in the game : simple dominance , cumulative dominance , and noncompensation . these conditions can be exploited to develop faster and more effective learning algorithms . in addition , they allow certain types of domain knowledge to be incorporated with ease into a learning algorithms . among the sequential decision problems we encounter , it is unlikely that tetris is unique or rare in having these properties .",0
78,"This paper presents an illumination-independent mapping approach for minimally processed sensor responses, allowing for mapping between global and illumination-specific sensor color responses. The approach is based on linear and non-linear transformations of raw-rgb values and is evaluated for computer vision tasks using consumer cameras with different spectral sensitivities. By mapping between raw images, the approach avoids the need for white-balancing and can handle images of arbitrary scenes and illuminations. Mapping strategies are compared and analyzed for their effectiveness in achieving illumination-independent mappings. The results show that the proposed approach can successfully map between sensor responses in a minimally processed and illumination-independent manner, making it a useful tool for various computer vision tasks.",1
79,"camera images saved in raw format are being adopted in computer vision tasks since raw values represent minimally processed sensor responses . camera manufacturers , however , have yet to adopt a standard for raw images and current raw-rgb values are device specific due to different sensors spectral sensitivities . this results in significantly different raw images for the same scene captured with different cameras . this paper focuses on estimating a mapping that can convert a raw image of an arbitrary scene and illumination from one camera 's raw space to another . to this end , we examine various mapping strategies including linear and non-linear transformations applied both in a global and illumination-specific manner . we show that illumination-specific mappings give the best result , however , at the expense of requiring a large number of transformations . to address this issue , we introduce an illumination-independent mapping approach that uses white-balancing to assist in reducing the number of required transformations . we show that this illumination-independent mapping approach achieves state-of-the-art results on a range of consumer cameras and images of arbitrary scenes and illuminations .",0
80,"This paper discusses the use of modal operators in terminological logics for representing terminological knowledge in multi-agent environments. It explores different formalisms and languages for representing terminological knowledge and introduces modal logics with domain assumptions and concept expressions. The paper also discusses the syntax and semantics of modal operators, and their application in representing belief and intentions in the application domain.",1
81,"terminological knowledge representation formalisms can be used to represent objective , time-independent facts about an application domain . notions like belief , intentions , and time which are essential for the representation of multi-agent environments can only be expressed in a very limited way . for such notions , modal logics with possible worlds semantics provides a formally well-founded and well-investigated basis . this paper presents a framework for integrating modal operators into terminological knowledge representation languages . these modal operators can be used both inside of concept expressions and in front of termino-logical and assertional axioms . we introduce syntax and semantics of the extended language , and show that satisfiability of finite sets of formulas is decidable , provided that all modal operators are interpreted in the basic logic k , and that the increasing domain assumption is used .",0
82,"This paper proposes a method for selecting effective contextual information for automatic synonym acquisition. The study focuses on using contextual clues of words to modify categories of contextual information. The authors argue that selecting the right type of contextual information can improve the performance of automatic synonym acquisition systems. The paper examines various types of lexical knowledge, sentence co-occurrence, and dependency relations to determine effective contextual information. The study uses subject-object combination to demonstrate how proximity of contex-tual information can improve synonym acquisition. The results of the study show that the selection of appropriate contextual information improves the accuracy of synonym acquisition and provides insights into the relationship between different types of contextual information and synonym acquisition.",1
83,"various methods have been proposed for automatic synonym acquisition , as synonyms are one of the most fundamental lexical knowledge . whereas many methods are based on contextual clues of words , little attention has been paid to what kind of categories of contex-tual information are useful for the purpose . this study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity . the evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance . we 've further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subject-object combination .",0
84,"This paper proposes a method for incorporating transition constraints into parallel planning using the planner named tcpp. The approach is based on a constraint model that includes table constraints and wild cards, and is solved using the constraint solver Minion. The table constraints are used to specify transition constraints and are compared with other constraint-based planners. The method is evaluated using domain transition graphs and shows promising results, demonstrating the effectiveness of incorporating transition constraints into parallel planning.",1
85,we present a planner named transition constraints for parallel planning . tcpp constructs a new constraint model from domain transition graphs of a given planning problem . tcpp encodes the constraint model by using table constraints that allow do n't cares or wild cards as cell values . tcpp uses minion the constraint solver to solve the constraint model and returns the parallel plan . empirical results exhibit the efficiency of our constraint model over state-of-the-art constraint-based planners .,0
86,"This paper presents an unsupervised learning approach for discovering action classes in images, using a combination of linear programming relaxation technique, clustering, and image labeling. The proposed method leverages human figures and their coarse shape to discover action classes without the need for explicit supervision. A spectral clustering algorithm is used to group similar images, and a pruning method is employed to remove outliers and noisy data. The approach is evaluated using training images, and results show that the proposed method outperforms existing techniques in discovering action classes from images. The use of unsupervised learning and clustering provides a promising direction for automated discovery of action classes in images.",1
87,"in this paper we consider the problem of describing the action being performed by human figures in still images . we will attack this problem using an unsupervised learning approach , attempting to discover the set of action classes present in a large collection of training images . these action classes will then be used to label test images . our unsupervised learning approach uses the coarse shape of the human figures to match pairs of images . the distance between a pair of images is computed using a linear programming relaxation technique . this is a computationally expensive process , and we employ a fast pruning method to enable its use on a large collection of images . spectral clustering is then performed using the resulting distances . we present clustering and image labeling results on a variety of datasets .",0
88,"This paper proposes a self-training approach for enhancing and adapting statistical parsers trained on small datasets, with a focus on reducing annotation costs. The approach uses a small amount of manually annotated seed data to train a statistical PCFG parser and then iteratively improves the parser's performance by adding automatically labeled data to the training set. The proposed method is evaluated in both in-domain and out-of-domain adaptation scenarios, demonstrating that self-training significantly improves parser performance while reducing annotation costs. Results show that the proposed approach outperforms existing methods in enhancing and adapting statistical parsers. The use of self-training provides a promising direction for enhancing and adapting statistical parsers trained on small datasets.",1
89,"creating large amounts of annotated data to train statistical pcfg parsers is expensive , and the performance of such parsers declines when training and test data are taken from different domains . in this paper we use self-training in order to improve the quality of a parser and to adapt parser to a different domain , using only small amounts of manually annotated seed data . we report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario . in particular , we achieve 50 % reduction in annotation cost for the in-domain case , yielding an improvement of 66 % over previous work , and a 20-33 % reduction for the domain adaptation case . this is the first time that self-training with small labeled datasets is applied successfully to these tasks . we were also able to formulate a characterization of when self-training is valuable .",0
90,"This paper discusses the use of mechanical vocal-tract models for studying speech dynamics. Two types of models are considered: the sliding three-tube model and the flexible-tongue model. The sliding three-tube model is a physical model of the human vocal tract that uses three tubes to simulate the pharynx, oral cavity, and nasal cavity. The flexible-tongue model incorporates a computer-controlled version of a sliding tongue that can be moved in real-time to simulate the movements of the tongue during speech. The paper compares and contrasts the two models, discussing the advantages and limitations of each. The sliding three-tube model is found to be more useful for studying the effects of changes in vocal-tract shape on speech acoustics, while the flexible-tongue model is better for studying the dynamic aspects of speech production. The paper concludes by highlighting the potential for future research using these and other mechanical vocal-tract models.",1
91,"arai has developed several physical models of the human vocal tract for education and has reported that physical models are intuitive and helpful for students of acoustics and speech science . we first reviewed dynamic models , including the sliding three-tube model and the flexible-tongue model . we then developed a head-shaped model with a sliding tongue , which has the advantages of both the s3t and flexible-tongue models . we also developed a computer-controlled version of the umeda & teranishi model , as the original head-shaped model was hard to manipulate precisely by hand . these head-shaped model are useful when teaching the dynamic aspects of speech .",0
92,"This paper presents a study of linear classification and selective sampling under low noise conditions. The goal is to develop versions of the margin-based algorithm that can handle low noise conditions and achieve a faster convergence rate. The proposed approach uses selective sampling to improve the performance of the classifier by identifying and focusing on the most informative instances. The paper analyzes the bayes risk of the selective sampler and shows that it achieves logarithmic factors in the instance distribution. The proposed approach is evaluated using textual data, demonstrating that the selective sampler significantly outperforms existing approaches in terms of convergence rate and classification accuracy. The study also compares the performance of the selective sampler with the bayes risk, highlighting the benefits of the proposed approach. The results suggest that selective sampling can be an effective technique for linear classification under low noise conditions, and can achieve better performance with faster convergence rates.",1
93,"we provide a new analysis of an efficient margin-based algorithm for selective sampling in classification problems . using the so-called tsybakov low noise condition to parametrize the instance distribution , we show bounds on the convergence rate to the bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm . our analysis reveals that , excluding logarithmic factors , the average risk of the selective sampler converges to the bayes risk at rate n − -lrb- 1 + α -rrb- -lrb- 2 + α -rrb- / 2 -lrb- 3 + α -rrb- where n denotes the number of queried labels , and α > 0 is the exponent in the low noise condition . for all α > √ 3 − 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate n − -lrb- 1 + α -rrb- / -lrb- 2 + α -rrb- achieved by the fully supervised version of the same classifier , which queries all labels , and for α → ∞ the two rates exhibit an exponential gap . experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors .",0
94,"This paper presents a data-driven approach to phone set construction for code-switching speech recognition based on context-sensitive articulatory attributes. The proposed method uses a hierarchical phone unit clustering algorithm that leverages cross-lingual context-sensitive articulatory features to address the data sparseness problem inherent in large-scale code-switching speech databases. The paper evaluates the proposed approach and compares it with other phone set construction methods. The results show that the data-driven approach outperforms existing methods and improves the recognition accuracy. Additionally, the paper analyzes the importance of acoustic features in the model construction process and proposes a distance measure based on KL-divergence to measure the similarity between the bilingual speakers' articulatory attributes. The proposed approach can improve the recognition accuracy of code-switching speech recognition systems by constructing a phone set that better represents the underlying phonetic characteristics of the input speech.",1
95,"bilingual speakers are known for their ability to code-switch or mix their languages during communication . this phenomenon occurs when bilinguals substitute a word or phrase from one language with a phrase or word from another language . for code-switching speech recognition , it is essential to collect a large-scale code-switching speech database for model training . in order to ease the negative effect caused by the data sparseness problem in training code-switching speech recognizers , this study proposes a data-driven approach to phone set construction by integrating acoustic features and cross-lingual context-sensitive articulatory features into distance measure between phone units . kl-divergence and a hierarchical phone unit clustering algorithm are used in this study to cluster similar phone units to reduce the need of the training data for model construction . the experimental results show that the proposed data-driven approach outperforms other traditional phone set construction methods .",0
96,"This paper proposes a new approach for discourse generation using utility-trained coherence models. The authors introduce stochastic models of discourse coherence that are trained to maximize the expected utility of a generated discourse. The models are log-linear coherence models, and they are trained using a utility function that evaluates the coherence of the generated discourse. The paper compares the proposed utility-trained coherence models with other existing coherence models and shows that the proposed models outperform the other models in terms of coherence and fluency of the generated discourse. Overall, the paper demonstrates the effectiveness of the proposed utility-trained coherence models for discourse generation.",1
97,we describe a generic framework for integrating various stochastic models of discourse coherence in a manner that takes advantage of their individual strengths . an integral part of this framework are algorithms for searching and training these stochastic models of discourse coherence . we evaluate the performance of our models and algorithms and show empirically that utility-trained log-linear coherence models out-perform each of the individual coherence models considered .,0
98,"This paper presents a collaborative prediction approach using ensembles of Maximum Margin Matrix Factorizations (MMMF). MMMF is a matrix factorization technique that has shown to be effective for collaborative prediction tasks. In this work, fast gradient-based methods are used to train MMMF models, and an ensemble approach is proposed to further improve prediction performance. The paper evaluates the proposed method on various benchmarks and compares it against other methods. The results show that the ensemble approach significantly improves prediction accuracy, while the total training time remains reasonable. Additionally, various evaluation metrics are used to demonstrate the effectiveness of the proposed method.",1
99,"fast gradient-based methods for maximum margin matrix factorization were recently shown to have great promise -lrb- rennie & srebro , 2005 -rrb- , including significantly outperforming the previous state-of-the-art methods on some standard collaborative prediction benchmarks -lrb- including movielens -rrb- . in this paper , we investigate ways to further improve the performance of maximum margin matrix factorization , by casting maximum margin matrix factorization within an ensemble approach . we explore and evaluate a variety of alternative ways to define such ensembles . we show that our resulting ensembles can perform significantly better than a single mmmf model , along multiple evaluation metrics . in fact , we find that ensembles of partially trained mmmf model can sometimes even give better predictions in total training time comparable to a single mmmf model .",0
100,"This paper investigates the role of jaw movements in forming the syllable nucleus of vowels and liquids. Specifically, it explores the relationship between phonemic length distinction and phonemic length effects in vowels and the rising-falling sonority profile of syllable nuclei. The study examines the main lingual articulation and syllabic liquids in the nucleus position, and measures jaw movements such as jaw opening and jaw activity during speech production. The findings suggest that jaw movements play an important role in shaping the syllable nucleus, and that they are closely related to the phonemic and sonority characteristics of the speech sounds involved.",1
101,"this paper investigates jaw movements in the production of slovak syllables with and without vowels . we test the hypothesis that / l , r / in the syllable nucleus position show a degree of jaw opening comparable to vowels , therefore providing a rising-falling sonority profile even in syllables lacking vowels . we also investigate whether the phonemic length distinction occurring for both vowels and syllabic consonants is implemented in a similar fashion for the different nucleus types . our articulatory data show that the jaw activity during syllabic liquids is indeed comparable to that of vowels , and that the jaw is recruited to help maintain the main lingual articulation . this became evident in particular in an interaction between nucleus type and phonemic length effects .",0
102,"This paper focuses on wavelet systems with zero moments, specifically exploring the relationship between the zero scaling function and wavelet moments. The study compares these wavelets to other wavelet systems, such as Coifman and Daubechies wavelets, highlighting the nonunique solutions that arise. The paper's findings provide insights into the properties of these wavelets and their potential applications in signal processing, image compression, and other related fields.",1
103,the coifman wavelets created by daubechies have more zero moments than imposed by specifications . this results in systems with approximately equal numbers of zero scaling function and wavelet moments and gives a partitioning of the systems into three well defined classes . the nonunique solutions are more complex than for daubechies wavelets .,0
104,"This paper proposes a probabilistic topic model that can capture both general and specific aspects of documents' topical or semantic content. The model reduces the dimensionality of sparse count data to a lower-dimensional latent variable representation, enabling effective modeling of the mixture distribution of word-level background distribution and topic models. This approach is compared to the widely used latent-semantic indexing technique for abstraction of common words. The proposed probabilistic topic model shows promising results in information retrieval tasks. The paper presents experimental evaluations and analyses that demonstrate the effectiveness of the model. The model's ability to capture the latent structure of documents could have significant implications for a range of natural language processing tasks.",1
105,"techniques such as probabilistic topic models and latent-semantic indexing have been shown to be broadly useful at automatically extracting the topical or semantic content of documents , or more generally for dimension-reduction of sparse count data . these types of models and algorithms can be viewed as generating an abstraction from the words in a document to a lower-dimensional latent variable representation that captures what the document is generally about beyond the specific words it contains . in this paper we propose a new probabilistic model that tempers this probabilistic model by representing each document as a combination of -lrb- a -rrb- a background distribution over common words , -lrb- b -rrb- a mixture distribution over general topics , and -lrb- c -rrb- a distribution over words that are treated as being specific to that document . we illustrate how this probabilistic model can be used for information retrieval by matching documents both at a general topic level and at a specific word level , providing an advantage over techniques that only match documents at a general level -lrb- such as topic models or latent-sematic indexing -rrb- or that only match documents at the specific word level -lrb- such as tf-idf -rrb- .",0
106,"This paper introduces an extension to hierarchical task network (HTN) planners that allows for planning in dynamic environments with nonlinear continuous effects. While previous HTN planners have only considered linear continuous effects or discrete effects, this new approach incorporates both continuous and discrete effects, allowing for more accurate and effective planning in dynamic environments. The paper presents a state projection algorithm that maps continuous effects onto the discrete state space of the planner, and a set of benchmark domains and evaluation metrics, including a Navy training simulation, to demonstrate the effectiveness of the approach. The results show that incorporating nonlinear continuous effects into HTN planning significantly improves the planner's performance and accuracy in dynamic continuous environments.",1
107,"planning in dynamic continuous environments requires reasoning about nonlinear continuous effects , which previous hierarchical task network planners do not support . in this paper , we extend an existing htn planner with a new state projection algorithm . to our knowledge , this is the first htn planner that can reason about nonlinear continuous effects . we use a wait action to instruct this htn planner to consider continuous effects in a given state . we also introduce a new planning domain to demonstrate the benefits of planning with nonlinear continuous effects . we compare our approach with a linear continuous effects planner and a discrete effects htn planner on a benchmark domain , which reveals that its additional costs are largely mitigated by domain knowledge . finally , we present an initial application of this algorithm in a practical domain , a navy training simulation , illustrating the utility of this approach for planning in dynamic continuous environments .",0
108,"This paper focuses on the word-level fundamental frequency (F0) range in Mandarin Chinese and its application to inserting words into a sentence. The authors discuss the automatic voice response application and its relevance in analyzing the F0 contour and context of an inserted word. They also examine the tone combination and word utterance, which are factors that can influence F0 range. The paper highlights the importance of considering F0 range when inserting words into a sentence to ensure natural-sounding speech. The results suggest that using F0 range can lead to better-sounding inserted words, improving the overall quality of speech synthesis.",1
109,"this paper considers an automatic voice response application in which a word utterance is inserted into a fixed carrier sentence . an important task here is to adjust the f 0 contour of the inserted word according to the f 0 context of the carrier sentence . instead of generating the f 0 contour on syllable basis , we employ an approach to adjust the f 0 contour of the whole word . in this approach , two questions arise : -lrb- a -rrb- how to evaluate the f 0 context and -lrb- b -rrb- how to adjust the f 0 contour suitably for the context . we have found that the f 0 contour of a word can be appropriately regulated in a tone-independent word-level f 0 range -lrb- wf 0 r -rrb- . after estimating the wf 0 rs of the preceding and succeeding words , the wf 0 r of the inserted word is set at the mean of these wf 0 rs . the f 0 contour of the inserted word is then mapped to the wf 0 r taking into account the tone combination of the word . a perceptual evaluation experiment showed that the adjusted f 0 was coordinated well with the context .",0
110,"This paper proposes an integration of speech into computer-assisted translation using finite-state automata. The system utilizes automatic speech recognition (ASR) models, statistical machine translation models, and a statistical prediction engine to improve translation efficiency. The authors describe a n-best rescoring approach that utilizes ASR word graphs and a finite-state automaton to select the most likely translation from a set of candidates generated by the translation engine. The proposed system is evaluated on human speech data and achieves improved translation accuracy compared to the traditional approach. The paper discusses the different types of human translators that can benefit from this technology and provides insights into future directions for research in this area.",1
111,"state-of-the-art computer-assisted translation engines are based on a statistical prediction engine , which interactively provides completions to what a human translator types . the integration of human speech into a computer-assisted translation engines is also a challenging area and is the aim of this paper . so far , only a few methods for integrating statistical machine translation models with automatic speech recognition models have been studied . they were mainly based on n-best rescoring approach . n-best rescor-ing is not an appropriate search method for building a computer-assisted translation engines . in this paper , we study the incorporation of automatic speech recognition models and asr models using finite-state automata . we also propose some transducers based on automatic speech recognition models for rescoring the asr word graphs .",0
112,"This paper presents an analysis of a modulated orthogonal sequence. The paper explores various characteristics of the sequence, such as autocorrelation, cross-correlation, and symbol error probability. The paper also discusses information construction methods used in the sequence and the use of modular techniques, including integer sums and dummy symbol generation. The analysis provides insights into the properties and performance of the modulated orthogonal sequence, which can be useful for various applications in communications and signal processing.",1
113,"one dummy symbol has to be included for the sequence in this paper , a new generation and an information construction methods for a modulated orthogonal sequence are suggested the sequence is generated by only integer sums and modular techniques . the autocorrelation and cram correlation characteristics of the sequence are investigated via a new procedure . a modified sequence also having the orthogonality and satisfying the mathematical lower bound of the cross-correlation is proposed , and the symbol error probability of the sequence is investigated .",0
114,"This paper analyzes the passive ranging capability of a towed multi-module array in underwater acoustic environments for passive source localization. The array's source localization capability is determined by the acoustic-field's spatial coherence and the environmental uncertainties, while the cramer-rao lower bound is used to evaluate the array's source localization performance. A two-stage approach is proposed for passive ranging, which involves generating a spatial coherence matrix based on the array modules and using it to estimate the source range. The study considers the effects of environmental uncertainties on the array's performance and evaluates the impact of these uncertainties on the proposed ranging approach. The results show that the proposed method can effectively estimate source range with high accuracy, even in the presence of environmental uncertainties.",1
115,"this work presents our research results on the capability of passive source localization using a towed multi-module array in underwater acoustic environments . we developed new cramer-rao lower bound results for assessing the array 's source localization capability under environmental uncertainties , such as array modules ' positions and acoustic-field 's spatial coherence . we also developed a two-stage approach and provide details for passive ranging by utilizing data either non-coherently or coherently , depending on the availability of spatial coherence in the received data from multiple modules of towed array .",0
116,"This paper proposes a novel approach for automatic road extraction from aerial imagery using multi-scale and snake-based edge extraction techniques. The proposed method combines the strengths of both approaches to achieve more accurate and efficient results. The snakes are guided by geometric constraints and the scale-space behavior of roads, enabling them to accurately trace the edges of roads even in the presence of shadows. The bridging of shadows is achieved through the use of multi-scale analysis. The effectiveness of the proposed method is demonstrated through experiments on aerial imagery, which show that the method outperforms existing approaches in terms of accuracy and efficiency. The results of this study have significant implications for applications that rely on accurate road extraction, such as urban planning and traffic analysis.",1
117,"this paper proposes an approach for automatic road extraction in aerial imagery which exploits the scale-space behavior of roads in combination with geometric constrained snake-based edge extraction . the approach not only has few parameters to be adjusted , but for the first time allows for a bridging of shadows and partially occluded areas using the heavily disturbed evidence in the image . the road network is constructed after extracting crossings of various shape and topology . reasonable results are obtained which are evaluated based on ground truth .",0
118,This paper proposes a novel approach for image content classification using multistage residual vector quantization (RVQ). RVQ is a data compression technique that is optimized for input space with structured patterns. The proposed method utilizes stage-wise codebooks and a direct sum decoder to achieve fine-grained feature attribution and natural/man-made structure recognition. The system is designed based on a Bayesian framework that combines RVQ and Markov random fields for maximum a-posteriori sense classification. Voronoi cell partitions are used to optimize classification and improve image understanding. Experimental results demonstrate the effectiveness of the proposed approach for class conditional pattern recognition and multistage structure image-content classification.,1
119,"multistage residual vector quantizers -lrb- multistage residual vector quantizers -rrb- with optimal direct sum decoder codebooks have been successfully designed and implemented for data compression . due to its multistage structure , multistage residual vector quantizers has the ability to densely populate the input space with voronoi cell partitions . the same design concept has yielded good results in the application of image-content classification -lsb- 1 -rsb- . furthermore , the multistage rvq , with stage-wise codebooks , provides an opportunity to perform fine-grained feature attribution for image understanding , in general , and feature foundation data generation for natural and man-made structure recognition , in specific . in -lsb- 1 -rsb- , the information at the stages of multistage residual vector quantizers is heuristically integrated to perform class conditional pattern recognition ; hence the process is not robust . markov random field provides a suitable bayesian framework to integrate the information available at the various stages of multistage residual vector quantizers to achieve optimized classification in the maximum a-posteriori sense .",0
120,"This paper presents a new approach for depth estimation in road scene imagery, by utilizing a surface normal classifier that takes into account the direction of the surface normals. The proposed method builds upon state-of-the-art global stereo matching approaches, which use binocular stereo matching to estimate depth from images. The classifier responses are based on data-driven classification, and are evaluated on real-world datasets to benchmark the effectiveness of the approach. The paper also explores the use of surface orientation images and scene geometry to improve depth estimation, particularly in indoor environments. Experimental results show that considering the direction of surface normals in the classification process improves single view depth estimation, and outperforms previous methods that do not consider the direction of surface normals.",1
121,"in this work we make use of recent advances in data driven classification to improve standard approaches for binocular stereo matching and single view depth estimation . surface normal direction estimation has become feasible and shown to work reliably on state of the art benchmark datasets . information about the surface orientation contributes crucial information about the scene geometry in cases where standard approaches struggle . we describe , how the responses of such a classifier can be included in global stereo matching approaches . one of the strengths of our classifier is , that we can use the classifier responses for a whole set of directions and let the final optimization decide about the surface orientation . this is important in cases where based on the classifier , multiple different surface orientations seem likely . we evaluate our classifier on two challenging real-world datasets for the two proposed applications . for the binocular stereo matching we use road scene imagery taken from a car and for the single view depth estimation we use images taken in indoor environments .",0
122,"This paper proposes an Entity-Based Local Coherence Modelling technique that utilizes Topological Fields to improve coherence in natural language generation systems. Topological Fields provide high-level clausal structures, entity grid models and constituent orders, which are crucial for generating coherent text. The proposed model uses manual annotations of grammatical roles to establish the coherent logical order of constituent orders in the text. The technique is demonstrated on German text and shows significant improvements in local coherence using Topological Fields. The results suggest that incorporating Topological Fields in natural language generation systems can enhance the quality of generated text.",1
123,"one goal of natural language generation is to produce coherent text that presents information in a logical order . in this paper , we show that topological fields , which model high-level clausal structure , are an important component of local coherence in german . first , we show in a sentence ordering experiment that topologi-cal field information improves the entity grid model of barzilay and lapata -lrb- 2008 -rrb- more than grammatical role and simple clausal order information do , particularly when manual annotations of this information are not available . then , we incorporate the model enhanced with topolog-ical fields into a natural language generation system that generates constituent orders for german text , and show that the added coherence component improves performance slightly , though not statistically significantly .",0
124,"This paper presents a study on the Graceful Degradation of Speech Recognition Performance over Lossy Packet Networks in client-server automatic speech recognition systems. The performance of the system is analyzed under low and medium loss channel conditions, and packet loss rates are simulated using various packet loss recovery techniques such as forward error correction systems. The effect of data acquisition delay and channel loss models on the system's performance is also investigated. Results show that the proposed method of graceful degradation of speech recognition performance over lossy packet networks can effectively maintain a reasonable recognition accuracy under different levels of packet loss. The findings of this study can be used to enhance the robustness and reliability of speech recognition systems in real-world network environments.",1
125,"this paper explores packet loss recovery in client-server automatic speech recognition systems . a forward error correction system is designed and tested over several channel loss models , at variable amounts of data acquisition delay . in experiments with simulated packet loss , the client-server automatic speech recognition systems provides robust asr performance which degrades gracefully as packet loss rates increase . comparing this client-server automatic speech recognition systems to several alternatives under low and medium loss channel conditions , we found one approach -lrb- multiple transmission plus interpolation -rrb- that yielded similar performance , but the client-server automatic speech recognition systems should scale better to lower bit rate conditions .",0
126,"This paper proposes the use of Deep Belief Nets (DBNs) for Natural Language Call-Routing in support of speech classification. The DBN-initialized neural network is compared to other text classification algorithms such as support vector machines and maximum entropy. The proposed technique employs the multi-layer generative model of DBNs and uses unlabeled data features to improve the accuracy of the natural language call-routing system. The backpropagation and boosting methods are used for training the DBN-initialized neural network, and the results show that the proposed method outperforms other text classification algorithms. The findings suggest that DBNs are a promising learning technique for improving the accuracy of natural language call-routing in image, audio, and speech classification systems.",1
127,"this paper considers application of deep belief nets to natural language call routing . deep belief nets have been successfully applied to a number of tasks , including image , audio and speech classification , thanks to the recent discovery of an efficient learning technique . deep belief nets learn a multi-layer generative model from unlabeled data and the features discovered by this multi-layer generative model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation . we compare a dbn-initialized neural network to three widely used text classification algorithms ; support vector machines , boosting and maximum entropy . the multi-layer generative model gives a call -- routing classification accuracy that is equal to the best of the other models even though multi-layer generative model currently uses an impoverished representation of the input .",0
128,"This paper investigates the role of localization cues in binaural segregation of reverberant speech. The study focuses on the localization of groups of time-frequency units, which are crucial for binaural and stereo speech segregation. The paper proposes a novel method for sequential grouping of time-frequency objects, which uses the localization information to improve speech segregation in reverberant environments. The proposed technique exploits the room reverberation characteristics to enhance the time-frequency grouping and localization information. The results demonstrate that the proposed method of sequential grouping of time-frequency objects improves the localization and segregation of reverberant speech. The findings suggest that localization cues play a significant role in binaural segregation of reverberant speech and should be considered in the design of speech enhancement algorithms.",1
129,"approaches to binaural and stereo speech segregation have often assumed that localization information can be used as a primary cue to achieve segregation of a target signal . results produced by these systems degrade significantly in the presence of room reverberation . in this work , we present an alternative framework to achieve localization of groups of time-frequency units . we show that grouping across time and frequency allows the use of localization information as an important cue for sequential grouping of time-frequency objects . we analyze the level of time-frequency grouping needed to achieve accurate object localization and show preliminary binaural segregation results using the proposed framework . results indicate that both localization information and segregation performance can be improved by grouping across time and frequency .",0
130,"This paper proposes a Unification-Based Semantic Interpretation for Coordinate Constructs, focusing on the Montagovian semantics of coordination. The paper proposes a first-order unification-based semantic interpretation of coordination using lambda expressions. The proposed method employs lambda reduction steps to perform partial execution of the semantic interpretation, resulting in a more efficient and accurate interpretation of coordinate constructs. The results demonstrate that the proposed method outperforms existing methods in terms of accuracy and computational efficiency. The findings suggest that the proposed unification-based semantic interpretation can be applied to other natural language processing tasks that involve coordination. The paper provides a comprehensive framework for the semantic interpretation of coordinate constructs, which can be used to improve the accuracy of natural language processing systems.",1
131,"this paper shows that a first-order unification-based semantic interpretation for various coordinate constructs is possible without an explicit use of lambda expressions if we slightly modify the standard montagovian semantics of coordination . this modification , along with partial execution , completely eliminates the lambda reduction steps during semantic interpretation .",0
132,"This paper presents an evaluation of scan-line optimization for 3D medical image registration, focusing on the integration of a semi-global cost integration strategy into the optimization technique. The study evaluates the performance of the optimization technique on 3D CT scan pairs in the context of pulmonary motion analysis, using the COPDGene study archive as a test bed. The results demonstrate that the proposed optimization technique, which combines scan-line optimization with a coarse-to-fine strategy and cost accumulation, significantly reduces registration errors compared to existing techniques. The study also investigates the effect of consecutive pyramid levels and search space dimension on the performance of the optimization technique. The findings suggest that the proposed optimization technique is effective for high-dimensional data and can be applied to other clinical applications. The paper provides a comprehensive evaluation of scan-line optimization for 3D medical image registration, which can be used to improve the accuracy of computer vision applications in the medical domain.",1
133,"scan-line optimization via cost accumulation has become very popular for stereo estimation in computer vision applications and is often combined with a semi-global cost integration strategy , known as sgm . this paper introduces this combination as a general and effective optimization technique . it is the first time that this concept is applied to 3d medical image registration . the presented algorithm , sgm-3d , employs a coarse-to-fine strategy and reduces the search space dimension for consecutive pyramid levels by a fixed linear rate . this allows it to handle large displacements to an extent that is required for clinical applications in high dimensional data . sgm-3d is evaluated in context of pulmonary motion analysis on the recently extended dir-lab benchmark that provides ten 4d computed tomography -lrb- ct -rrb- image data sets , as well as ten challenging 3d ct scan pairs from the copdgene study archive . results show that both registration errors as well as run-time performance are very competitive with current state-of-the-art methods .",0
134,"This paper proposes a multi-view domain generalization approach for visual recognition that leverages the inherent cluster structures in the unlabeled target domain samples. The approach is designed to improve the domain generalization capability of the system, enabling it to recognize objects in unseen target domains. An alternating optimization algorithm is used to learn weight matrices for different views of the data, which capture the latent domains present in the data. These weight matrices are used to train svm classifiers with weight vectors that enable robust classification in the presence of domain shift. The approach is evaluated on several datasets, and the results demonstrate that it outperforms other state-of-the-art methods, including low-rank representation and exemplar svms. The proposed approach is shown to be effective in improving the generalization capability of the system, making it suitable for real-world applications.",1
135,"in this paper , we propose a new multi-view domain generalization approach for visual recognition , in which we aim to use the source domain samples with multiple types of features -lrb- i.e. , multi-view features -rrb- to learn robust classifiers that can generalize well to any unseen target domain . considering the recent works show the domain generalization capability can be enhanced by fusing multiple svm classifiers , we build upon exemplar svms to learn a set of svm classifiers by using one positive sample and all negative samples in the source domain each time . when the source domain samples come from multiple latent domains , we expect the weight vectors of exemplar svm clas-sifiers can be organized into multiple hidden clusters . to exploit such cluster structure , we organize the weight vectors learnt on each view as a weight matrix and seek the low-rank representation by reconstructing this weight matrix using itself as the dictionary . to enforce the consistency of inherent cluster structures discovered from the weight matrices learnt on different views , we introduce a new multi-view domain generalization approach to minimize the mismatch between any two representation matrices on different views . we also develop an efficient alternating optimization algorithm and further extend our multi-view domain generalization approach for visual recognition by exploiting the manifold structure of unlabeled target domain samples . comprehensive experiments for visual recognition clearly demonstrate the effectiveness of our multi-view domain generalization approach for domain generalization and visual recognition .",0
136,"This paper proposes a cost-sensitive stacking approach for the audio tag annotation and retrieval problem. Audio tagging is a multi-label learning problem where tags need to be assigned to an audio clip. The proposed method is based on the MIREX 2009 winning method and addresses the problem of noisy and correlated tag information by incorporating cost-sensitive classification techniques. The approach takes into account the tag correlation and the tag count, as well as the mood and keywords associated with the music clip. Experiments on a real-world dataset demonstrate the effectiveness of the proposed method in achieving improved tag annotation and retrieval performance.",1
137,"audio tags correspond to keywords that people use to describe different aspects of a music clip , such as the genre , mood , and instrumentation . since social tags are usually assigned by people with different levels of musical knowledge , they inevitably contain noisy information . by treating the tag counts as costs , we can model the audio tagging problem as a cost-sensitive classification problem . in addition , tag correlation is another useful information for automatic audio tagging since some tags often co-occur . by considering the co-occurrences of tags , we can model the audio tagging problem as a cost-sensitive classification problem . to exploit the tag count and correlation information jointly , we formulate the audio tagging problem as a novel cost-sensitive multi-label learning problem . the results of audio tag annotation and retrieval experiments demonstrate that the new approach outperforms our mirex 2009 winning method .",0
138,"This paper proposes a method called Supervised Bipartite Graph Inference, which is designed to infer the network topology of bipartite graphs. The focus is on the compound-protein interaction network reconstruction problem, which involves chemical structure data and genomic sequence data. The proposed method uses a reproducing kernel Hilbert space to embed the data into a unified Euclidean space, and then learns a distance metric using supervised learning. The optimization problem is formulated as a graph inference problem, and the bipartite graph is inferred using the learned distance metric. Experimental results demonstrate that the proposed method outperforms existing methods on the compound-protein interaction network reconstruction problem.",1
139,"we formulate the problem of bipartite graph inference as a supervised learning problem , and propose a new method to solve it from the viewpoint of distance metric learning . the method involves the learning of two mappings of the heterogeneous objects to a unified euclidean space representing the network topology of the bipartite graph , where the graph is easy to infer . the algorithm can be formulated as an optimization problem in a reproducing kernel hilbert space . we report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data .",0
140,"This paper presents a method for sampling and reconstructing spatial fields using mobile sensors. The problem addressed is the accurate measurement of temperature field values over a wide spatial area using mobile sensors. The method employs time-domain signal processing techniques, including anti-aliasing filtering, to address the measurement problem in the presence of out-of-band noise. The use of mobile sensors allows for a more efficient and cost-effective approach compared to traditional static sensing methods. The results show that the proposed method achieves high reconstruction accuracy for the temperature field using a small number of mobile sensors. Overall, this work provides a promising solution for sampling and reconstructing spatial fields using mobile sensors.",1
141,the classical approach to sampling time-invariant spatial fields uses static sensors distributed over space . we study a new approach involving mobile sensors that move through space measuring the field values along their paths . a single moving sensor can take measurements over a wide spatial area thus acting as a substitute for a potentially large number of static sensors . a moving sensor encounters the spatial field in its path in the form of a time-domain signal . hence a time-domain anti-aliasing filter can be employed at the mobile sensors to limit the amount of out-of-band noise prior to sampling . we analytically quantify the advantage of mobile sensors over static sensing in rejecting out-of-band noise . we also demonstrate via simulations the improvement in reconstruction accuracy that can be obtained using mobile sensors and filtering in a temperature measurement problem .,0
142,"This paper presents a novel method called Compositional High Order Pattern Potentials (CHOPP) for structured output learning, which explores high-order potential functions to model complex high-level structures in image-dependent tasks. The proposed method combines linear deviation pattern potentials and pattern-like high order potentials to create image-dependent high order potentials, which are used to model the quantitative variability of high variability datasets. CHOPP utilizes a loss-sensitive joint learning procedure to optimize the internal pattern parameters and the image-dependent mapping, which leads to accurate structure prediction. The method is evaluated on image segmentation tasks and achieves state-of-the-art performance on various datasets, demonstrating the effectiveness of the proposed approach in dealing with high variability datasets and complex structures. The tractable models developed in this work can be used for other structured output learning tasks as well.",1
143,"when modeling structured outputs such as image seg-mentations , prediction can be improved by accurately mod-eling structure present in the labels . a key challenge is developing tractable models that are able to capture complex high level structure like shape . in this work , we study the learning of a general class of pattern-like high order potential , which we call compositional high order pattern potentials . we show that compositional high order pattern potentials include the linear deviation pattern potentials of rother et al. -lsb- 26 -rsb- and also restricted boltzmann machines ; we also establish the near equivalence of these two models . experimentally , we show that performance is affected significantly by the degree of variability present in the datasets , and we define a quantitative variability measure to aid in studying this . we then improve compositional high order pattern potentials performance in high variability datasets with two primary contributions : -lrb- a -rrb- developing a loss-sensitive joint learning procedure , so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss ; and -lrb- b -rrb- learning an image-dependent mapping that encourages or inhibits patterns depending on image features . we also explore varying how multiple patterns are composed , and learning convolutional patterns . quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance .",0
144,"This paper explores the corpus-based induction of syntactic structures using models of dependency and constituency. The authors propose unsupervised learning of dependency structures as well as a model of linear constituency for unsupervised constituency parsing. Evaluation metrics are used to assess the performance of generative models on parsing tasks. The paper also discusses distributional regularities and the importance of using a generative model in unsupervised parsing. Overall, this work contributes to the development of techniques for automatic extraction of syntactic structures from natural language text.",1
145,"we present a generative model for the unsupervised learning of dependency structures . we also describe the multiplicative combination of this generative model with a model of linear constituency . the generative model outperforms both components on their respective evaluation metrics , giving the best published figures for un-supervised dependency parsing and unsupervised constituency parsing . we also demonstrate that the combined generative model works and is robust cross-linguistically , being able to exploit either attachment or distributional regularities that are salient in the data .",0
146,"This paper proposes a method for constructing deterministic sensing matrices for compressed sensing using additive character sequences with small alphabets. The resulting matrices are incoherent and form tight frames, achieving an asymptotically optimal coherence bound. The method is demonstrated to work well with sparse signals under undersampled measurements, and the performance is evaluated using the l1-minimization method. The proposed approach provides a new way to generate measurement matrices that can be used for efficient and noiseless measurements in compressed sensing applications.",1
147,"compressed sensing is a novel technique where one can recover sparse signals from the undersampled measurements . in this paper , a k × n measurement matrix for compressed sensing is deterministically constructed via additive character sequences . the weil bound is then used to show that the deterministic sensing matrix has asymptotically optimal coherence for n = k 2 , and that it is a tight frame . a sparse recovery guarantee for the incoherent tight frame is also discussed . numerical results show that the deterministic sensing matrix guarantees empirically reliable recovery performance via an l 1-minimization method for noiseless measurements .",0
148,"This paper proposes a nonparametric, wavelet-based density estimation method for inferring queuing delay distributions in the global internet. By using host-based, end-to-end measurements and network level NS simulations, the proposed method is able to estimate internal delay distributions with high accuracy. The method utilizes a fast Fourier transform implementation to efficiently perform an expectation-maximization optimization algorithm to estimate the unknown parameters of the delay distributions. Additionally, the method incorporates spatially localized information as a priori limit to further improve the accuracy of the estimation procedure. The proposed method shows promising results in global internet monitoring, providing a reliable and accurate method for inferring queuing delay distributions in the absence of parametric assumptions.",1
149,"the substantial overhead of performing global internet monitoring motivates techniques for inferring spatially localized information about performance using only host-based , end-to-end measurements . in this paper , we present a novel methodology for inferring queuing delay distributions across internal links in the network based solely on unicast , end-to-end measurements . a key feature of our new approach is that it is nonparametric , meaning that no a priori limit is placed on the number of unknown parameters used to model the delay distributions . the nonparametric approach is required in order to accurately estimate the wide variety of internal delay distributions . the methodology is formulated according to a recently proposed nonparametric , wavelet-based density estimation method in combination with an expectation-maximization optimization algorithm that employs a novel fast fourier transform implementation . we perform network level ns simulations to verify the accuracy of the estimation procedure .",0
150,This paper proposes a probabilistic inference based message-passing algorithm for resource-constrained distributed constraint optimization problems (DCOPs). The algorithm uses expectation-maximization and convex optimization machinery to make near-optimal decisions in a coordinated multi-agent setting. The algorithm is designed to overcome the coordination problem caused by the failure rate of centralized solvers. The paper presents an inference technique based on wave passing that ensures a convergent message-passing algorithm. The technique is evaluated using DCOP algorithms and achieves high accuracy in solving coordination problems in resource-constrained settings.,1
151,"distributed constraint optimization -lrb- distributed constraint optimization -rrb- is an important framework for coordinated multiagent decision making . we address a practically useful variant of distributed constraint optimization , called distributed constraint optimization , which takes into account agents ' consumption of shared limited resources . we present a promising new class of algorithm for distributed constraint optimization by translating the underlying coordination problem to probabilistic inference . using inference techniques such as expectation-maximization and convex optimization machinery , we develop a novel convergent message-passing algorithm for distributed constraint optimization . experiments on standard benchmarks show that our approach provides better quality than previous best dcop algorithms and has much lower failure rate . comparisons against an efficient centralized solver show that our approach provides near-optimal solutions , and is significantly faster on larger instances .",0
152,"This paper proposes a novel approach for training Conditional Random Fields (CRFs) using multivariate evaluation measures for sequential segmentation tasks. The traditional error minimization approach used in CRF training often leads to poor performance, particularly when the target evaluation measure is non-linear. This paper addresses this issue by incorporating multiple evaluation measures into the loss function of the CRF training. The proposed approach is evaluated on two widely used sequential segmentation tasks: named entity recognition and text chunking. Experimental results show that the proposed approach outperforms traditional approaches in terms of both f-score and non-linear measures. The findings of this study demonstrate the effectiveness of using multivariate evaluation measures for training CRFs and highlight the importance of considering multiple evaluation measures in the evaluation of segmentation models.",1
153,"this paper proposes a framework for training conditional random fields to optimize multivariate evaluation measures , including non-linear measures such as f-score . our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure . specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these sequential segmentation tasks , namely , segmentation f-score . our experiments show that our method performs better than standard crf training .",0
154,"This paper introduces elimination ordering in lifted first-order probabilistic inference, which aims to solve non-relational probabilistic inference problems on generated relational graphs with parameterized random variables. The use of relational models with logical variables and population sizes can lead to NP-complete problems. The proposed method uses elimination ordering to transform the lifted inference problem into a series of smaller non-relational inference problems, which can be solved efficiently. The paper shows experimental results that demonstrate the effectiveness of the proposed method in terms of accuracy and speed. This method can potentially enable more efficient and scalable inference in various applications of probabilistic programming.",1
155,"various representations and inference methods have been proposed for lifted probabilistic inference in rela-tional models . many of these inference methods choose an order to eliminate -lrb- or branch on -rrb- the parameterized random variables . similar to such inference methods for non-relational probabilistic inference , the order of elimination has a significant role in the performance of the inference methods . since finding the best order is np-complete even for relational models , heuristics have been proposed to find good orderings in the relational models . in this paper , we show that these heuristics are inefficient for relational models , because they fail to consider the population sizes associated with logical variables . we extend existing heuristics for relational models and propose new heuristics for relational models . we evaluate the existing and new heuristics on a range of generated relational graphs .",0
156,"This paper presents two algorithms for solving risk-sensitive partially observable Markov decision processes (POMDPs) with and without cost observations. The first algorithm is a cumulative cost search-based algorithm that can handle risk-sensitive POMDPs with user-defined cost thresholds, while the second algorithm uses policy iteration to solve risk-sensitive POMDPs without cost observations. The algorithms are evaluated in both a synthetic domain and a real-world data set from the taxi domain. Results show that the cumulative cost search-based algorithm outperforms the policy iteration algorithm in terms of accuracy and computation time, while the latter is more suitable for POMDPs without cost observations. The study demonstrates the importance of considering risk and cost in decision making under uncertainty.",1
157,"partially observable markov decision processes -lrb- pomdps -rrb- are often used to model planning problems under uncertainty . the goal in risk-sensitive pomdps is to find a policy that maximizes the probability that the cumulative cost is within some user-defined cost threshold . in this paper , unlike existing risk-sensitive pomdps , we distinguish between the two cases of whether costs can or can not be observed and show the empirical impact of cost observations . we also introduce a new search-based algorithm to solve risk-sensitive pomdps and show that search-based algorithm is faster and more scalable than existing approaches in two synthetic domains and a taxi domain generated with real-world data .",0
158,"This paper presents a scalable solution to the Poisson phase retrieval problem using nuclear norm constraints and non-Lipschitz continuous gradient objectives. The Poisson noise model and the nuclear norm constraint lead to a convex program with an objective function that is not Lipschitz continuous. The proposed approach, named PhaSeLift, is based on the Frank-Wolfe algorithm and provides global convergence guarantees with an iteration counter that is independent of the problem size. The paper shows that PhaSeLift outperforms other state-of-the-art methods, including the Lanczos method, in terms of both computational efficiency and solution quality. The experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed approach for solving the Poisson phase retrieval problem with maximum-likelihood estimator.",1
159,"we study a phase retrieval problem in the poisson noise model . motivated by the phaselift approach , we approximate the maximum-likelihood estimator by solving a convex program with a nuclear norm constraint . while the frank-wolfe algorithm , together with the lanczos method , can efficiently deal with nuclear norm constraints , our objective function does not have a lipschitz continuous gradient , and hence existing convergence guarantees for the frank-wolfe algorithm do not apply . in this paper , we show that the frank-wolfe algorithm works for the poisson phase retrieval problem , and has a global convergence rate of o -lrb- 1/t -rrb- , where t is the iteration counter . we provide rigorous theoretical guarantee and illustrating numerical results .",0
160,This paper presents a method for analyzing speaking styles by visualizing the aggregate of acoustic models using a two-dimensional map. The authors use a multidimensional scaling technique to create a visual map that displays the similarity between different speaking styles represented by Gaussian distributions of multidimensional vectors in an acoustic model library. This map is designed to take advantage of human visual perception to make it easier to analyze the acoustic models. The authors apply this technique to a speech recognition system and show how the two-dimensional map can be used to identify marginal regions that are difficult for the system to recognize. They demonstrate the effectiveness of their approach using the Cosmos map and show that it provides a useful tool for analyzing and understanding different speaking styles.,1
161,"to ensure high enough recognition performance from the outset of usage of the speech recognition system , prior development of highly precise acoustic model library is necessary . the analysis of hmm acoustic models expressed with gaussian distributions of multidimensional vectors is typically a difficult task . the cosmos -lrb- acoustic space map of sound -rrb- method featuring the visualization of distributions of the hmm acoustic models in a two dimensional space by utilizing multidimensional scaling technique is proposed in order to support the analysis through capability of human visual perception . the effectiveness of the proposed technique is reviewed based on an analysis on speaking styles . the marginal region within the two-dimensional visual map -lrb- called cosmos map -rrb- obtained by the proposed method the contains hmm acoustic models with lower recognition performance . it is possible to improve recognition performance by dividing the marginal region into several smaller zones in which separate hmm acoustic models is trained and provided to the speakers belonging to the same zone .",0
162,"This paper proposes an EM clustering model with rich linguistic features for Chinese verb sense discrimination. The method utilizes electronic Chinese semantic dictionaries to extract predicate-argument structure information and fine-grained semantic categories for Chinese verbs. Normalized mutual information is used to evaluate the semantic similarity between verb senses. The proposed model incorporates lexical sets and semantic features to improve the clustering performance. A semantic taxonomy is also introduced to facilitate the evaluation of the clustering results. Experimental results show that the proposed method outperforms several baseline methods in terms of accuracy and F1-score, demonstrating the effectiveness of the proposed approach.",1
163,"this paper discusses the application of the expectation-maximization clustering algorithm to the task of chinese verb sense discrimination . the expectation-maximization clustering algorithm utilized rich linguistic features that capture predicate-argument structure information of the target verbs . a semantic taxonomy for chinese nouns , which was built semi-automatically based on two electronic chinese semantic dictionaries , was used to provide semantic features for the expectation-maximization clustering algorithm . purity and normalized mutual information were used to evaluate the clustering performance on 12 chinese verbs . the experimental results show that the expectation-maximization clustering algorithm can learn sense or sense group distinctions for most of the verbs successfully . we further enhanced the expectation-maximization clustering algorithm with certain fine-grained semantic categories called lexical sets . our results indicate that these lexical sets improve the expectation-maximization clustering algorithm 's performance for the three most challenging verbs chosen from the first set of experiments .",0
164,"This paper discusses the generation of synthetic disfluent speech in the context of automatic film dubbing and spoken translation. The authors focus on local prosodic modifications caused by the insertion of editing terms in the synthetic speech. The paper proposes a method for incorporating disfluency elements into segmental units of the synthetic speech, with the aim of producing a more natural-sounding speech. The proposed method involves modifying the prosody of the synthetic speech to mimic the local modifications observed in real disfluent speech. The effectiveness of the proposed method is evaluated through subjective listening tests. The results suggest that the proposed method can significantly improve the naturalness of synthetic disfluent speech. The paper concludes with a discussion of future research directions in the field of disfluent speech synthesis.",1
165,"disfluent speech synthesis is necessary in some applications such as automatic film dubbing or spoken translation . this paper presents a model for the generation of synthetic disfluent speech based on inserting each element of a disfluency in a context where they can be considered fluent . prosody obtained by the application of standard techniques on these new sentences is used for the synthesis of the disfluent sentence . in addition , local modifications are applied to segmental units adjacent to disfluency elements . experiments evidence that duration follows this behavior , what supports the feasibility of the model .",0
166,"This paper proposes a method for computing the object pose and velocity simultaneously from a single view captured by a rolling shutter camera. The method uses 2D-3D point correspondences and a closed-form linear solution to estimate the 3D pose of planar objects and their full 3D velocity. It takes into account the rolling shutter phenomenon and the resulting image deformations caused by the camera's motion. The proposed method uses bundle adjustment and non-linear optimization to refine the estimates, achieving high accuracy. The results show that the method can handle the rolling shutter effect and provide reliable pose and velocity estimates even for fast-moving objects captured with CMOS image sensors.",1
167,"an original concept for computing instantaneous 3d pose and 3d velocity of fast moving objects using a single view is proposed , implemented and validated . it takes advantage of the image deformations induced by rolling shutter in cmos image sensors . first of all , after analysing the rolling shutter phenomenon , we introduce an original model of the image formation when using such a camera , based on a general model of moving rigid sets of 3d points . using 2d-3d point correspondences , we derive two complementary methods , compensating for the rolling shutter deformations to deliver an accurate 3d pose and exploiting them to also estimate the full 3d velocity . the first solution is a general one based on non-linear optimization and bundle adjustment , usable for any object , while the second one is a closed-form linear solution valid for planar objects . the resulting closed-form linear solution enable us to transform a cmos low cost and low power camera into an innovative and powerful velocity sensor . finally , experimental results with real data confirm the relevance and accuracy of the closed-form linear solution .",0
168,"This paper presents a new feature extraction method called the Two-Dimensional Maximum Margin Criterion (2DMMC) for face recognition tasks. The method aims to find an orthogonal projection matrix that maximizes the margin between classes while also minimizing the within-class scatter. The 2DMMC method is designed for low-dimensional matrix subspace representation, and its theoretical analysis shows its superiority over some existing approaches. Experiments on benchmark face recognition datasets demonstrate the effectiveness of the proposed 2DMMC method, achieving competitive results with state-of-the-art methods with fewer dimensions. Overall, the 2DMMC method shows great potential for efficient and effective feature extraction and dimensionality reduction in image analysis tasks.",1
169,"maximum margin criterion is a well-known method for feature extraction and dimensionality reduction . in this paper , we propose a novel feature extraction method , namely two dimensional maximum margin criterion , specifically for matrix representation data , e.g. images . 2dmmc aims to find two orthogonal projection matrices to project the original matrices to a low dimensional matrix subspace , in which a sample is close to those in the same class but far from those in different classes . both theoretical analysis and experiments on benchmark face recognition data sets illustrate that the proposed method is very effective and efficient .",0
170,"This paper investigates the acoustic characteristics of /th/ lenition in Brunei Mandarin using conversational speech data of female speakers. The study analyzes the spectral properties of the sound change and uses both perceptual judgments and spectrographic inspection to examine the correctness of the classification features such as closure identification, frication, and burst. The results of the spectrographic analysis show that there are clear acoustic differences between the voiced and voiceless variants of /th/ in Brunei Mandarin, and the perceptual judgments are consistent with the spectrographic inspection. Overall, this study provides important insights into the sound change of /th/ in Brunei Mandarin and highlights the usefulness of combining acoustic and perceptual approaches to examine the phonetic features of Mandarin Chinese spoken by Bruneians.",1
171,"this study investigates the acoustic characteristics of / t h / lenition in conversational speech of brunei mandarin , a variety of mandarin chinese . based on data from 20 chinese bruneians , / t h / lenition was found in the third-person pronoun tā / t h a / , which is frequently pronounced as hā -lsb- ha -rsb- . perceptual judgments , spectrographic analysis and acoustic characteristics were conducted to examine the features of this sound change . in comparison with the perceptual judgments , it was found that the spectrographic inspection yielded 83.6 % correct classification of -lsb- t h -rsb- and -lsb- h -rsb- for female speakers and 77.2 % for male speakers , indicating there is reasonably high reliability in identification in terms of spectral properties . results of the acoustic characteristics showed that there is an increase in high frequency intensity after the release of the closure for -lsb- t h -rsb- while there is little change in intensity during the frication for -lsb- h -rsb- . the results showed that the lack of burst and little increase in intensity are reasonably reliable cues for stop lenition .",0
172,"In this paper, we propose a method for generating comparative news summaries using linear programming. Our approach leverages semantic-related cross-topic concept pairs to identify topic-related concepts and generate summaries that compare news articles across different topics. We formulate the summarization problem as an optimization problem and use linear programming to find the optimal summary that satisfies a set of constraints. Experimental results show that our approach outperforms several state-of-the-art methods on a dataset of news articles. Our approach provides a new way of generating comparative summaries and can be applied to a wide range of text summarization tasks.",1
173,"comparative news summarization aims to highlight the commonalities and differences between two comparable news topics . in this study , we propose a novel approach to generating comparative news summaries . we formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics . we consider semantic-related cross-topic concept pairs as comparative evidences , and consider topic-related concepts as representative evidences . the optimization problem is addressed by using a linear programming model . the experimental results demonstrate the effectiveness of our proposed model .",0
174,"This paper proposes a novel approach for stochastic optimization based on the Laplace transform order and its application to precoder designs. Specifically, the paper investigates the stochastic power allocation problem for multi-antenna channels, which involves maximizing the ergodic or effective capacity subject to an average power constraint. The nonconvex structure of the objective function poses significant challenges for optimization, and the Laplace transform order is used to transform the problem into an equivalent optimization problem with a convex objective function. The proposed approach is shown to outperform existing methods in terms of both solution accuracy and computational efficiency. The paper also considers the problem of relaying precoding for multi-antenna channels and provides numerical results demonstrating the effectiveness of the proposed approach in this context. Overall, this paper presents a valuable contribution to the field of stochastic optimization for multi-antenna channels.",1
175,"stochastic optimization arising from precoding in a multi-antenna fading channel with channel mean feedback to maximize data rates is important but challenging . the use of relaying further complicates the situation , as it may induce a nonconvex structure in the objective function , thereby excluding the use of existing approaches which require convexity or concavity . to deal with challenges as such , this paper presents a new framework for solving a class of stochastic optimization problems . the analysis here involves the comparison of two nonnegative random variables in the laplace transform order . our framework is particularized to optimal precoding for maximum ergodic or effective capacity in multi-antenna channels with or without relaying assuming channel mean feedback , where the objectives may or may not have convexity or concavity . the application to stochastic power allocation is also discussed .",0
176,"This paper presents a study on design tradeoffs for mobile single-carrier frequency-division multiple-access (SC-FDMA) with application to Long-Term Evolution (LTE)-Advanced. The paper focuses on open-loop space-frequency block coding (SFBC) scheme and examines its impact on the performance of the SC-FDMA uplink. The authors investigate the design tradeoffs between the SFBC codeword and the SC-FDMA uplink, with regards to peak-to-average-power ratio, practical decoding complexity, full spatial diversity, and Doppler robustness. The paper also analyzes the performance degradation caused by SFBC, and compares it with other uplink transmit diversity schemes. The results of the study show that SFBC can provide significant benefits in terms of full spatial diversity and peak-to-average-power ratio, but at the cost of practical decoding complexity and performance degradation. Finally, the authors provide some guidelines for the design of SFBC in mobile SC-FDMA systems.",1
177,"single-carrier frequency-division multiple-access -lrb- sc-fdma -rrb- has been adopted in the uplink of the lte standard due to its lower peak-to-average-power ratio compared to orthogonal frequency-division multiple access . recent activities in the lte-advanced standardization have focused on effective uplink transmit diversity schemes with low papr -lsb- 1 -rsb- but without considering the performance degradation due to doppler despite the fact that orthogonal frequency-division multiple access is required to support high mobility . in this paper , we present an open-loop space-frequency block coding scheme for the sc-fdma uplink and demonstrate its robustness to high doppler and large multipath delay spread while enjoying full spatial diversity , low papr and practical decoding complexity by a suitable design of the frequency span of each sfbc codeword . in this paper , we study the design tradeoffs involved in single-carrier frequency-division multiple-access for mobile sc-fdma .",0
178,"This paper presents a global real-time face tracking application using a two-step approach of real-time facial segmentation and liptracking. The system employs active contour models for real-time facial segmentation and a global analysis/synthesis chain for 3D synthesis of facial animation. The paper also introduces feedback control for improved performance in face tracking and facial segmentation. The proposed approach is applied in the context of augmented reality, demonstrating its potential for real-time image analysis and 3D synthesis.",1
179,"this article deals with facial segmentation and liptracking with feedback control by face model synthesis . on this topic , the search community is divided into two parts : analysis and synthesis . we want to use all the knowledge to create a global analysis/synthesis chain where the image analysis needs the 3d synthesis and conversely . as it happens , applications like face tracking or augmented reality need a rapid , robust and descriptive-enough solution . our solution is based on a two step approach : the first step is a real-time facial segmentation with active contour models and the second step recovers a 3d-face model in order to extract more precise parameters to adjust the first step . the contribution of this paper is to couple two research fields for creating a real time application . the results obtained show rapid and robust performances which could be exploited in a more global real-time face tracking application .",0
180,"This paper proposes SenseRelate, a generalized framework for Word Sense Disambiguation (WSD). SenseRelate uses WordNet-based measures of semantic relatedness to identify the most appropriate sense of a word in context. It provides a flexible and extensible framework for WSD, which can be easily adapted to different languages and domains. SenseRelate includes a Perl package called TargetWord, which provides a user-friendly interface for accessing and using the different WSD algorithms implemented in the system. The paper describes the architecture and design of SenseRelate and presents experimental results on various benchmark datasets, demonstrating the effectiveness of the system.",1
181,"we have previously introduced a method of word sense disambiguation that computes the intended sense of a target word , using wordnet-based measures of semantic relatedness -lrb- patwardhan et al. , 2003 -rrb- . senserelate : : targetword is a perl package that implements this algorithm . the word sense disambiguation is carried out by selecting that sense of the target word which is most related to the context words . relatedness between word senses is measured using the senserelate : : similarity perl modules .",0
182,"This paper proposes a multichannel speech enhancement approach for reverberant environments using convolutive transfer function approximation. The method involves the use of transfer-function generalized sidelobe canceler (TF-GSC) beamformers to suppress both reverberations and noise in speech signals. The paper compares the proposed approach with delay-and-sum beamformers and shows that TF-GSC is more effective in suppressing reflections and reducing noise. The structure of the TF-GSC is also described, and the paper demonstrates that the relative transfer functions can be used for speech signal reflections. The proposed approach shows significant improvements in noise reduction and suppression of reverberations, making it a promising technique for multichannel speech enhancement in reverberant environments.",1
183,"recently , we have presented a transfer-function generalized sidelobe canceler beamformer in the short time fourier transform domain , which relies on a convolutive transfer function approximation of relative transfer functions between distinct sensors . in this paper , we combine a delay-and-sum beamformer with the tf-gsc structure in order to suppress the speech signal reflections captured at the sensors in reverberant environments . we demonstrate the performance of the proposed transfer-function generalized sidelobe canceler beamformer and compare transfer-function generalized sidelobe canceler beamformer with the tf-gsc . we show that the proposed transfer-function generalized sidelobe canceler beamformer enables suppression of reverberations and further noise reduction compared with the tf-gsc beamformer .",0
184,"This paper proposes a representation theory for ranking functions, which is useful for supervised learning tasks such as ranking. The theory is based on the natural symmetricity assumption of exchangeable listwise ranking functions, and De Finetti theorems are used to prove that the Bayes optimal ranking functions can be represented as permutation-valued functions. The paper shows that listwise loss functions can be written as tensor products of point-wise ranking functions, and provides a framework for analyzing and designing list-wise ranking functions. The proposed representation theory also allows for the development of reranking methods based on functional analysis. Overall, the paper provides a comprehensive approach to understanding and designing ranking functions for various applications in machine learning and information retrieval.",1
185,"this paper presents a representation theory for permutation-valued functions , which in their general form can also be called listwise ranking functions . point-wise ranking functions assign a score to each object independently , without taking into account the other objects under consideration ; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole . in many supervised learning to rank tasks , it might be of interest to use listwise ranking functions instead ; in particular , the bayes optimal ranking functions might themselves be listwise , especially if the loss function is listwise . a key caveat to using list-wise ranking functions has been the lack of an appropriate representation theory for such functions . we show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions . our analysis draws from the theories of tensor analysis , functional analysis and de finetti theorems . we also present experiments using a novel reranking method motivated by our representation theory .",0
186,"This paper proposes an approach to improve speech understanding by incorporating dialogue expectation in sentence parsing. Specifically, the authors address the problem of speech recognition errors by leveraging higher level information, such as dialogue history and the dialogue state, to inform the recognition process. The paper describes a dialogue manager that takes into account the concept of recognition pragmatics, which considers the relationship between the recognition process and the overall efficiency of the dialogue system. By using dialogue expectation and discourse knowledge, the proposed system is able to better parse sentences and improve speech understanding. The authors demonstrate the effectiveness of their approach through experiments on a flight reservation task, showing significant improvement over baseline systems.",1
187,"in dialogue systems , speech recognition errors force the user to repeat information resulting in more turns , lower dialogue efficiency and maybe complete failure . higher level information such as history , expectation , discourse knowledge and pragmatics can improve performance but are hard to quantify and effectively include in the recognition process . in this paper , dialogue expectation is used to improve recognition . by permitting the dialogue manager to guide the interaction it is possible to track the dialogue state and thus estimate the expected semantic content of the user 's response . the dialogue manager is allowed to process a large number of sentences provided by the decoder . expectation is used as an effective criterion for selecting among competing hypotheses . this approach was tested with a simple flight reservation task and results show improvement in concept recognition without adding significant computation .",0
188,"This paper presents a bidirectional approach for non-rigid point set registration using a robust Gaussian mixture model. The proposed algorithm aims to improve the accuracy and robustness of registration techniques in the presence of outliers and noise components. The bidirectional EM process is employed to estimate the parameters of the model and register the point sets. The algorithm is evaluated on medical images and synthetic data, and its performance is compared to other registration techniques. The results show that the proposed approach achieves higher accuracy and robustness compared to other methods.",1
189,"in this paper we present a novel point set registration algorithm based on the robust gaussian mixture model . we take advantage of a robust estimation to weigh the noise component in robust gaussian mixture model . moreover , a bidirectional em process is introduced to model outliers in both point sets in contrast to traditional methods . the performance of the point set registration algorithm is demonstrated and validated in carefully designed synthetic data and point sets extracted from medical images . results show that the proposed point set registration algorithm can improve the robustness and accuracy as compared to the traditional registration techniques .",0
190,"This paper proposes a novel approach called ""Latent Pyramidal Regions"" for recognizing scenes. The approach leverages a discriminative characteristic of the scenes by representing them as spatial pyramids. It uses a latent support vector machine to learn the nonlinear feature coding of the spatial pyramid representation, which captures the global and local scene characteristics. The training procedure includes a non-linear locality constraint coding to improve the accuracy of the representation model. The paper evaluates the proposed method on the UIUC-Sports dataset and shows that it outperforms existing approaches. The results demonstrate the effectiveness of using the latent pyramidal regions approach for indoor and outdoor scene classification.",1
191,"in this paper we propose a simple but efficient image representation for solving the scene classification problem . our new image representation combines the benefits of spatial pyramid representation using nonlinear feature coding and latent support vector machine to train a set of latent pyramidal regions . each of our latent pyramidal regions captures a discriminative characteristic of the scenes and is trained by searching over all possible sub-windows of the images in a latent svm training procedure . each latent pyramidal regions is represented in a spatial pyramid and uses non-linear locality constraint coding for learning both shape and texture patterns of the scene . the final response of the latent pyramidal regions form a single feature vector which we call the image representation and can be used for the scene classification problem . we tested our proposed scene representation model in three datasets which contain a variety of scene categories -lrb- 15-scenes , uiuc-sports and mit-indoor -rrb- . our image representation obtains state-of-the-art results on all these datasets which shows that image representation can simultaneously model the global and local scene characteristics in a single framework and is general enough to be used for both indoor and outdoor scene classification .",0
192,"This paper presents a formalism called Tangent Prop, which enables the specification of invariances in an adaptive network. The proposed method allows the incorporation of priori knowledge of selected invariances, such as scale changes, rotations, and distortion operators, into the learning process of a machine learning application. Tangent Prop is demonstrated to be effective in a character recognition task where invariance to rotations and scale changes is required. The method is shown to significantly reduce learning time and improve the accuracy of the character recognition model.",1
193,"in many machine learning applications , one has access , not only to training data , but also to some high-level a priori knowledge about the desired behavior of the system . for example , it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images -lrb- translations , rotations , scale changes , etcetera -rrb- . we have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing . this not only reduces the learning time and the amount of training data , but also provides a powerful language for specifying what generalizations we wish the network to perform .",0
194,This paper addresses the problem of scalar ambiguity in blind channel estimation for Orthogonal Frequency Division Multiplexing (OFDM) systems. The ambiguity arises due to the blind identification of the channel and the source constellation. The paper discusses the various types of scalar ambiguity and the pilot overhead required to resolve the ambiguity. It proposes a multiple-constellation scheme to reduce the pilot overhead and increase the estimation accuracy. The paper also presents a semi-blind identification technique to overcome the ambiguity by exploiting prior information about the channel. The proposed methods are shown to effectively resolve the scalar ambiguity problem and improve the performance of blind channel estimation in OFDM systems.,1
195,"blind channel estimation is a promising technique to reduce the pilot overhead . unfortunately , most existing algorithms suffer from the scalar ambiguity problem , and hence only achieve semi-blind identification . in this paper , we show that with the information of source constellation , the phase of the ambiguous scalar can be divided into a fractional part and an integer part . then we propose a multiple-constellation scheme enabling totally blind identification regardless of constellation type for ofdm systems . the necessary and sufficient condition for eliminating the scalar ambiguity is given . an application example shows that our multiple-constellation scheme can help other algorithms circumvent the annoying ambiguity .",0
196,"This paper proposes a belief change method based on global minimization. The approach is built on a set-theoretic notion of minimization and involves cardinality-based and priority-based minimization techniques. The authors explore minimization-based belief change, which encompasses belief revision and belief merging. The method is applied to spatial locations and is tested using undirected graphs and their vertices. The paper also discusses semantic and syntactic characterizations of the approach, and shows that the minimization-based method outperforms other methods in terms of accuracy and efficiency.",1
197,"a general framework for minimisation-based belief change is presented . a problem instance is made up of an undirected graph , where a formula is associated with each vertex . for example , vertices may represent spatial locations , points in time , or some other notion of locality . information is shared between vertices via a process of vertices over the graph . we give equivalent semantic and syntactic characterisations of this vertices . we also show that this approach is general enough to capture existing minimisation-based approaches to belief merging , belief revision , and -lrb- temporal -rrb- extrap-olation operators . while we focus on a set-theoretic notion of minimisation , we also consider other approaches , such as cardinality-based and priority-based minimisation .",0
198,"This paper proposes a fast image super-resolution method based on in-place example regression. The method uses low-to-high resolution image patches and a fast regression model to learn a nonlinear mapping function. The proposed approach utilizes the self-similarity of image patches to estimate the upper scale image. The method is evaluated on benchmark and real-world images and achieves fast and robust estimation with low visual artifacts. The proposed method also includes a first-order approximation to further improve the accuracy of the nonlinear mapping function. The experimental results show that the proposed method outperforms other super-resolution approaches based on external databases, especially in the case of diverse textures and origin locations. The paper also discusses the influence of patch noise on the proposed method. Overall, the proposed fast in-place example regression-based method provides an effective and efficient solution for single image super-resolution.",1
199,"we propose a fast regression model for practical single image super-resolution based on in-place examples , by leveraging two fundamental super-resolution approaches -- learning from an external database and learning from self-examples . our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image . based on the in-place examples , a first-order approximation of the nonlinear mapping function from low-to high-resolution image patches is learned . extensive experiments on benchmark and real-world images demonstrate that our fast regression model can produce natural-looking results with sharp edges and preserved fine details , while the current state-of-the-art algorithms are prone to visual artifacts . furthermore , our fast regression model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation . the fast regression model runs fast and is particularly useful for practical applications , where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts .",0
200,"This paper presents the design of ESSENCE, a constraint language for specifying combinatorial problems. ESSENCE aims to provide a natural and rigorous language for expressing complex combinatorial objects and their decision variables, allowing for formal specifications of combinatorial problems of arbitrary depth. The language supports a variety of combinatorial objects, including multisets, partitions, tuples, and relations, which can be combined and abstracted to represent complex objects. ESSENCE also allows for the specification of constraints and the use of natural language descriptions. This paper discusses the motivation and design of ESSENCE, as well as its use in specifying and solving combinatorial problems.",1
201,"essence is a new formal language for specifying combinatorial problems in a manner similar to natural rigorous specifications that use a mixture of natural language and discrete mathematics . essence provides a high level of abstraction , much of which is the consequence of the provision of decision variables whose values can be combina-torial objects , such as tuples , sets , multisets , relations , partitions and functions . essence also allows these combinatorial problems to be nested to arbitrary depth , thus providing , for example , sets of partitions , sets of sets of partitions , and so forth . therefore , a problem that requires finding a complex combinatorial object can be directly specified by using a decision variable whose type is precisely that combinatorial object .",0
202,"This paper proposes a novel approach to unsupervised learning, which combines program synthesis and probabilistic modeling to learn symbolic compositional structures from noisy data. Specifically, the paper introduces a new unsupervised learning algorithm that uses solver-based techniques and program synthesis tools to learn visual concepts and English inflectional morphology. The approach is demonstrated to be effective in the visual learning domain and shows promise for language learning problems. The paper concludes that this new approach to unsupervised learning has the potential to enable new breakthroughs in artificial intelligence by allowing machines to learn complex, compositional structures without the need for explicit supervision.",1
203,"we introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis . we apply our techniques to both a visual learning domain and a language learning problem , showing that our unsupervised learning algorithm can learn many visual concepts from only a few examples and that unsupervised learning algorithm can recover some english inflectional morphology . taken together , these results give both a new approach to unsupervised learning of symbolic composi-tional structures , and a technique for applying program synthesis tools to noisy data .",0
204,"This paper presents a novel method for robust optimal pose estimation of a calibrated camera from an image of a known scene. The method uses classical geometry and branch and bound setting to find the optimal camera pose estimation in the presence of outliers. The optimization criterion is based on the l ∞ norm, which makes the algorithm more robust to outliers compared to classical l 2 norm-based methods. The proposed approach can handle real data and is suitable for use with image features. Additionally, the method is capable of finding the optimal solution using an efficient RANSAC-based technique. The paper shows that the proposed algorithm is effective in handling position outliers and can provide reliable and accurate camera pose estimation results.",1
205,"we study the problem of estimating the position and orientation of a calibrated camera from an image of a known scene . a common problem in camera pose estimation is the existence of false correspondences between image features and modeled 3d points . existing techniques such as ransac to handle outliers have no guarantee of opti-mality . in contrast , we work with a natural extension of the l ∞ norm to the outlier case . using a simple result from classical geometry , we derive necessary conditions for l ∞ optimality and show how to use l ∞ optimality in a branch and bound setting to find the optimum and to detect outliers . the algorithm has been evaluated on synthetic as well as real data showing good empirical performance . in addition , for cases with no outliers , we demonstrate shorter execution times than existing optimal algorithms .",0
206,"This paper presents a controlled query evaluation framework for Datalog and OWL 2 profile ontologies. The framework enables confidentiality enforcement by censoring the results of queries to prevent unauthorized access to sensitive data. The authors discuss the challenges of controlled query evaluation and propose a solution that leverages the expressive power of OWL 2 profiles to capture the semantics of queries and enforce confidentiality. They also demonstrate the effectiveness of the framework using several experiments on real-world datasets. Overall, this work provides a valuable contribution to the field of ontology languages and controlled query evaluation.",1
207,"we study confidentiality enforcement in ontologies under the controlled query evaluation framework , where a policy specifies the sensitive information and a censor ensures that query answers that may compromise the policy are not returned . we focus on censors that ensure confidentiality while max-imising information access , and consider both dat-alog and the owl 2 profiles as ontology languages .",0
208,"This paper proposes a privacy-preserving crowd monitoring system that counts people without using people models or tracking. The system uses a dynamic textures motion model and a Gaussian process regression to estimate crowd density. The system includes a crowd segmentation algorithm to segment the crowd into regions with homogeneous motion. The segmented regions are then processed to extract holistic features that are used to count people. The system does not rely on tracking individuals, making it privacy-preserving. The proposed system was tested on a large pedestrian dataset and showed promising results, demonstrating the effectiveness of the approach.",1
209,"we present a privacy-preserving system for estimating the size of inhomogeneous crowds , composed of pedestrians that travel in different directions , without using explicit object segmentation or tracking . first , the crowd is segmented into components of homogeneous motion , using the mixture of dynamic textures motion model . second , a set of simple holistic features is extracted from each segmented region , and the correspondence between features and the number of people per segment is learned with gaussian process regression . we validate both the crowd segmentation algorithm , and the crowd counting system , on a large pedestrian dataset -lrb- 2000 frames of video , containing 49,885 total pedestrian instances -rrb- . finally , we present results of the privacy-preserving system running on a full hour of video .",0
210,"This paper presents Replicated Softmax, an undirected topic model for learning low-dimensional latent semantic representations from an unstructured collection of documents. The proposed model is a two-layer undirected graphical model that uses learning and inference algorithms to improve retrieval accuracy. The model is compared to Latent Dirichlet Allocation (LDA) on held-out documents, and it is shown that Replicated Softmax outperforms LDA in terms of retrieval accuracy. The proposed model is a promising approach for learning latent representations from text data.",1
211,"we introduce a two-layer undirected graphical model , called a '' replicated soft-max '' , that can be used to two-layer undirected graphical model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents . we present efficient learning and inference algorithms for this two-layer undirected graphical model , and show how a two-layer undirected graphical model , two-layer undirected graphical model , can be used to produce an accurate estimate of the log-probability the two-layer undirected graphical model assigns to test data . this allows us to demonstrate that the proposed two-layer undirected graphical model is able to generalize much better compared to latent dirichlet allocation in terms of both the log-probability of held-out documents and the retrieval accuracy .",0
212,"This paper presents an analysis of forward pruning, a decision-making technique used in game playing computer programs. Forward pruning is used to reduce the size of game trees by pruning branches that are likely to result in poor outcomes for the player. This paper examines the effectiveness of forward pruning by analyzing its impact on minimax values of sibling nodes in game trees. The analysis is applied to several chess-playing programs and the results show that forward pruning can improve the efficiency of game-playing programs while maintaining high levels of playing strength. The paper concludes with a discussion of the potential benefits and limitations of forward pruning in game playing and other decision-making contexts.",1
213,"several early game-playing computer programs used forward pruning -lrb- i.e. , the practice of deliberately ignoring nodes that are believed unlikely to affect a game tree 's minimax value -rrb- , but this technique did not seem to result in good decision-making . the poor performance of forward pruning presents a major puzzle for ai research on game playing , because some version of forward pruning seems to be '' what people do , '' and the best chess-playing programs still do not play as well as the best humans . as a step toward deeper understanding of forward pruning , we have set up models of forward pruning on two different kinds of game trees , and used these models to investigate how forward pruning affects the probability of choosing the correct move . in our studies , forward pruning did better than minimaxing when there was a high correlation among the minimax values of sibling nodes in a game tree . this result suggests that forward pruning may possibly be a useful decision-making technique in certain kinds of games . in particular , we believe that bridge may be such a game .",0
214,"This paper presents an efficient dimensionality reduction approach for high-dimensional network estimation. Specifically, the paper proposes a module graphical Lasso technique to estimate the graph structure of high-dimensional Gaussian graphical models. The approach is evaluated on real-world networks in cancer biology, including predicting the survival time of patients with ovarian cancer using gene expression data. The paper shows that the proposed technique achieves better scalability, robustness, and accuracy compared to other clustering algorithms and graphical Lasso. Furthermore, the paper demonstrates that the functionally coherent gene sets are more predictive of patient survival than individual genes. Overall, this study provides a useful tool for network estimation in high-dimensional settings, with potential applications in various fields, including biology and computer science.",1
215,"we propose module graphical lasso , an aggressive dimensionality reduction and network estimation technique for a high-dimensional gaussian graphical model . module graphical lasso achieves scalability , interpretability and ro-bustness by exploiting the modularity property of many real-world networks . variables are organized into tightly coupled modules and a graph structure is estimated to determine the conditional independencies among modules . module graphical lasso iteratively learns the module assignment of variables , the latent variables , each corresponding to a module , and the parameters of the ggm of the latent variables . in synthetic data experiments , module graphical lasso outperforms the standard graphical lasso and three other methods that incorporate latent variables into ggm . when applied to gene expression data from ovarian cancer , module graphical lasso out-performs standard clustering algorithms in identifying functionally coherent gene sets and predicting survival time of patients . the learned modules and their dependencies provide novel insights into cancer biology as well as identifying possible novel drug targets .",0
216,"This paper presents an algorithm for computing the Earth Mover's Distance (EMD) in linear time. EMD is a metric for measuring the difference between two histograms, where each histogram represents the distribution of a certain feature in a given dataset. The proposed algorithm uses a weighted wavelet transform to compute the EMD metric and has a computational complexity that is linear in the number of histogram bins. The algorithm is based on the Kantorovich-Rubinstein transshipment problem, and the optimization problem is formulated in the wavelet domain. The paper shows that the EMD metric can be computed efficiently using the proposed algorithm, and that the algorithm is particularly effective for low-dimensional histograms. The paper also discusses the Hölder continuity constraint and presents the dual form of the optimization problem. Overall, the proposed algorithm provides a fast and efficient way to compute the EMD metric for a wide range of applications.",1
217,"the earth mover 's distance -lsb- 19 -rsb- is an important perceptually meaningful metric for comparing histograms , but earth mover 's distance suffers from high -lrb- o -lrb- n 3 log n -rrb- -rrb- computational complexity . we present a novel linear time algorithm for approximating the earth mover 's distance for low dimensional his-tograms using the sum of absolute values of the weighted wavelet coefficients of the difference histogram . emd computation is a special case of the kantorovich-rubinstein transshipment problem , and we exploit the hölder continuity constraint in its dual form to convert earth mover 's distance into a simple optimization problem with an explicit solution in the wavelet domain . we prove that the resulting wavelet emd metric is equivalent to earth mover 's distance , i.e. the ratio of the two is bounded . we also provide estimates for the bounds . the weighted wavelet transform can be computed in time linear in the number of histogram bins , while the comparison is about as fast as for normal euclidean distance or χ 2 statistic . we experimentally show that wavelet emd metric is a good approximation to earth mover 's distance , has similar performance , but requires much less computation .",0
218,This paper proposes a blind equalization method for MIMO channels using deterministic precoding. The MIMO channel is assumed to be unknown and characterized by an instantaneous mixture of multiple channels. The proposed method involves iterative demodulation and parallel transmission of data signals. The iterative demodulation algorithm uses deterministic precoding to estimate the channel state information and then removes the effects of the channel from the received signals. The algorithm iteratively improves the estimate of the channel until convergence. The proposed method is able to achieve high equalization performance without requiring explicit channel estimation. The effectiveness of the proposed method is demonstrated through simulations.,1
219,we present a novel precoding or modulation scheme -lrb- matrix modulation -rrb- that allows parallel transmission of several data signals over an unknown multiple-input multiple-output -lrb- mimo -rrb- channel . we first present a theorem on unique signal demodulation and an efficient iterative demodulation algorithm for transmission over an unknown instantaneous-mixture channel . we then generalize our results to an unknown mimo channel with memory .,0
220,"This paper presents an MMSE-based method for noise PSD tracking with low complexity. The proposed method is designed for speech enhancement algorithms and non-stationary noise sources. It uses minimum statistics based noise tracking and minimum mean-squared error estimator to estimate the noise power spectral density. The method computes noise magnitude-squared DFT coefficients from noisy data, and it also considers segmental SNR and PESQ measures to track non-stationary noise sources. The low computational complexity of the proposed method makes it suitable for real-time applications.",1
221,"most speech enhancement algorithms heavily depend on the noise power spectral density . because this quantity is unknown in practice , estimation from the noisy data is necessary . we present a low complexity method for noise psd estimation . the low complexity method is based on a minimum mean-squared error estima-tor of the noise magnitude-squared dft coefficients . compared to minimum statistics based noise tracking , segmental snr and pesq are improved for non-stationary noise sources with 1 db and 0.25 mos points , respectively . compared to recently published algorithms , similar good noise tracking performance is obtained , but at a computational complexity that is in the order of a factor 40 lower .",0
222,"This paper proposes a speech enhancement system based on wavelet analysis and adaptive threshold estimation. The system uses time-frequency adaptive wavelet soft thresholding and a modified Weiner filtering technique to suppress noise in stationary and non-stationary noise cases. The system employs a bark-scaled wavelet packet decomposition and a magnitude decision-directed approach for threshold estimation, and an adaptive noise level-tracking algorithm to estimate the noise power spectral density. The proposed method achieves noise suppression through the computed threshold of the wavelet band coefficients. The system's performance is evaluated using the PESQ and SNR metrics, and the results show that the proposed method outperforms other state-of-the-art speech enhancement techniques while having low computational complexity.",1
223,"a new speech enhancement system , which is based on a time-frequency adaptive wavelet soft thresholding , is presented in this paper . the speech enhancement system utilises a bark-scaled wavelet packet decomposition integrated into a modified weiner filtering technique using a novel threshold estimation method based on a magnitude decision-directed approach . first , a bark-scaled wavelet packet transform is used to decompose the speech signal into critical bands . threshold estimation is then performed for each wavelet band according to an adaptive noise level-tracking algorithm . finally , the speech is estimated by incorporating the computed threshold into a wiener filtering process , using the magnitude decision-directed approach . the proposed speech enhancement technique has been tested with various stationary and non-stationary noise cases . reported results show that the speech enhancement system is capable of a high-level of noise suppression while preserving the intelligibility and naturalness of the speech .",0
224,"This paper presents a robust collaborative state estimation method for smart grid monitoring. The proposed method employs a gossip-based Gauss-Newton algorithm for decentralized state estimation scheme, which can handle bad data and noise variances, as well as network reconfigurations and random failures. The method utilizes power flow equations and distributed control areas to obtain a global state estimate. Additionally, the paper compares the proposed method with existing distributed techniques and shows its effectiveness on the IEEE-118 system network. The results indicate that the proposed method outperforms existing methods in terms of accuracy and scalability. Overall, this work provides a promising approach for smart grid monitoring and state estimation.",1
225,"this paper proposes a decentralized state estimation scheme via network gossiping with applications in smart grid wide-area monitoring . the proposed decentralized state estimation scheme allows distributed control areas to solve for an accurate global state estimate collaboratively using the proposed gossip-based gauss-newton algorithm . furthermore , the proposed decentralized state estimation scheme mitigates the influence of bad data by adap-tively updating the noise variances and re-weighting the contributions of the most recent measurements for state estimation . compared with other distributed techniques , our decentralized state estimation scheme via gossiping is more flexible and resilient in case of network reconfigurations and failures . we further prove that the power flow equations satisfy the sufficient condition for the gossip-based gauss-newton algorithm to converge to the desired solution . simulations of the ieee-118 system show that the proposed decentralized state estimation scheme estimates and tracks the global state robustly , and degrades gracefully when there are random failures and bad data .",0
226,"This paper proposes a new approach to tackle the ""Curse of Dimensionality"" problem in discrete combinatorial optimization problems by integrating hashing and optimization techniques. The method is based on using randomly generated parity constraints and a hash function to discretize the model. It allows for efficient computation of marginal probabilities and MAP queries by utilizing randomized algorithms to approximate the partition function. Additionally, the approach enables model selection by optimizing the trade-off between computational efficiency and accuracy. Experimental results demonstrate the effectiveness of the proposed method in reducing the computational complexity while maintaining high accuracy.",1
227,"integration is affected by the curse of dimen-sionality and quickly becomes intractable as the dimensionality of the problem grows . we propose a randomized algorithm that , with high probability , gives a constant-factor approximation of a general discrete integral defined over an exponentially large set . this randomized algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function . as an discrete combinatorial optimization problem , we demonstrate that with a small number of map queries we can efficiently approximate the partition function of discrete graphical models , which can in turn be used , for instance , for marginal computation or model selection .",0
228,"This paper presents a method for spectral estimation of voiced speech using a family of minimum variance distortionless response (MVDR) estimates. The MVDR estimates are applied in a fixed order MVDR-MFCC approach to model the sinusoidal and noise power of the voiced speech. A least squares approach is used to estimate the MVDR spectra with fixed model order coefficients. Synthetic vowels and sinusoidal signals are used to model the voiced speech, and the method is evaluated on the TIMIT database for recognition of voiced speech. The results show that the proposed method provides a good estimate of the spectral envelope of the voiced speech.",1
229,"we present a robust approach to modeling voiced speech using a family of minimum variance distortionless response -lrb- mvdr -rrb- spectral estimates . the method exploits the fact that for a ſxed model order , for a sinusoidal signal in noise , the mvdr estimate at the sinusoidal frequency is approximately related to the sinusoidal and noise power in a simple linear manner with the coefſcients being dependent on the model order . modeling voiced speech as a sum of harmonic signals , we then use the aforementioned relationship along with a least squares approach to combine a family of mvdr estimates -lrb- mvdr estimates of different orders -rrb- and develop a robust approach for modeling voiced speech . experimental results of spectral estimation of sinusoids , synthetic vowels , and actual speech signals at mvdr estimates -lrb- mvdr estimates of 0 db and 5 db using this approach indicate an increased resolution in the estimated mvdr spectra . the mvdr estimates -lrb- mvdr estimates computed from the mvdr estimate using this approach are also used for speaker identiſcation experiments on the timit database at various mvdr estimates -lrb- mvdr estimates . the results indicate a reasonable improvement in recognition performance when compared to the mvdr estimates -lrb- mvdr estimates and the ſxed order mvdr-mfcc .",0
230,"This paper presents a novel approach to the problem of signal declipping in high-dimensional audio data processing. The proposed method, called Consistent Iterative Hard Thresholding, aims to reconstruct a clipped audio signal from its observations while maintaining signal quality. The algorithm iteratively applies a hard thresholding procedure to the observed signal until a consistent estimate of the original signal is obtained. The reconstructed signal is evaluated through subjective user listening evaluations, and its quality is quantified by the signal-to-noise ratio (SNR). The proposed approach outperforms existing declipping algorithms in terms of both objective and subjective metrics, demonstrating its efficacy in practical audio processing applications.",1
231,"clipping or saturation in audio signals is a very common problem in signal processing , for which , in the severe case , there is still no satisfactory solution . in such case , there is a tremendous loss of information , and traditional methods fail to appropriately recover the signal . we propose a novel approach for this signal restoration problem based on the framework of iterative hard thresholding . this approach , which enforces the consistency of the reconstructed signal with the clipped observations , shows superior performance in comparison to the state-of-the-art declipping algorithms . this is confirmed on synthetic and on actual high-dimensional audio data processing , both on snr and on subjective user listening evaluations .",0
232,This paper proposes an optimal joint base station assignment and downlink beamforming scheme for heterogeneous networks. The goal is to optimize system-wide utility by maximizing the sum rate while maintaining α-fairness for all users. The proposed scheme uses linear transmit beamformers and addresses the problem of joint base station assignment and beamforming design. The objective function is formulated as a sum rate maximization problem subject to α-fairness constraints. The linear beamformer design is optimized using a novel iterative algorithm. Simulation results demonstrate the effectiveness of the proposed scheme in terms of sum rate and α-fairness.,1
233,"consider a mimo heterogeneous network with multiple transmitters -lrb- including macro , pico and femto base stations -rrb- and many receivers -lrb- mobile users -rrb- . the users are to be assigned to the base stations which then optimize their linear transmit beamformers accordingly . in this work , we consider the problem of joint base station assignment and linear beam-former design to maximize a system wide utility . we first establish the np-hardness of the resulting optimization problem for a large family of α-fairness utility functions . then , we propose an efficient algorithm to approximately solve this problem for the special case of sum rate maximization . the simulation results show that the algorithm improves the sum rate .",0
234,"This paper proposes new algorithms for blind block synchronization in zero-padding systems. Blind block synchronization is a challenging problem in wireless communication systems due to the unknown channel characteristics and synchronization parameters. The proposed algorithms use linear redundant filterbank precoders and zero-padding precoders to address the blind block synchronization problem. The blind block synchronization algorithm is evaluated based on the block synchronization error rate, and the results show improved performance over existing methods. Furthermore, the proposed algorithms can be generalized to handle different repetition indices and block synchronization parameters. The paper provides a comprehensive analysis of blind block synchronization and presents new techniques to overcome this problem in wireless communication systems.",1
235,"blind channel identification using linear redundant filterbank precoders has been studied extensively in the literature . most methods are proposed based on the assumption that block synchronization is perfect . in practice , a blind block synchronization algorithm must be used to justify this assumption . this paper studies the blind block synchronization problem in systems using a zero-padding precoder . a previously reported method is reviewed and a new approach for the blind block synchronization problem is proposed . generalized versions of both approaches are then developed using a parameter called repetition index . simulation results show that when the repetition index is chosen to be greater than unity , the block synchronization error rate performance of the proposed blind block synchronization algorithm has a significant improvement over the previously reported method . '",0
236,"This paper presents grammars that can handle local and long dependencies in a sentence. Specifically, it focuses on polarized dependency (PD) grammars, which capture the negative and positive valencies of words and allow for discontinuous constructions. The paper introduces bounded PD-grammars for handling derived structures and unbounded raising, as well as non-saturated valencies and rules for extraposition dependencies. Additionally, the paper discusses the use of CF-grammars for local extraction. The proposed grammars provide a promising approach for natural language processing tasks such as parsing and machine translation.",1
237,"polarized dependency -lrb- pd - -rrb- grammars are proposed as a means of efficient treatment of discontinuous constructions . pd-grammars describe two kinds of dependencies : local , explicitly derived by the rules , and long , implicitly specified by negative and positive va-lencies of words . if in a pd-grammar the number of non-saturated valencies in derived structures is bounded by a constant , then it is weakly equivalent to a cf-grammar and has a cents ¡ $ # ¦ ¥ ¨ § - time parsing algorithm . it happens that such bounded pd-grammars are strong enough to express such phenomena as unbounded raising , extraction and ex-traposition .",0
238,"This paper proposes a novel vision model for handwritten digit recognition. The model extracts linearly separable features that can be used with a linear classifier to achieve high recognition accuracy. The approach is inspired by biological vision, and the model is designed to mimic the process of feature extraction in the human visual system. Experimental results show that the proposed model outperforms existing approaches on several benchmark datasets. The study concludes that the proposed vision model could be a promising approach for achieving high accuracy in handwritten digit recognition tasks.",1
239,"we use well-established results in biological vision to construct a novel vision model for handwritten digit recognition . we show empirically that the features extracted by our vision model are linearly separable over a large training set -lrb- mnist -rrb- . using only a linear classifier on these features , our vision model is relatively simple yet outperforms other models on the same data set .",0
240,"This paper presents a method for fast modelling of pinna spectral notches from Head-Related Transfer Functions (HRTFs) using the linear prediction residual cepstrum. The authors focus on the virtualization of sound and aim to improve the accuracy of the HRTF model. The method is based on the Batteau's reflection model and uses the linear prediction residual cepstrum to extract spectral notches from pinna images. The authors demonstrate that their approach results in a significant improvement in the accuracy of the HRTF model, and can be applied to a wide range of applications in virtual acoustics.",1
241,"developing individualized head related transfer functions -lrb- hrtf -rrb- is an essential requirement for accurate virtualization of sound . however it is time consuming and complicated for both the subject and the developer . obtaining the spectral notches which are the most prominent features of hrtf is very important to reconstruct the head related impulse response -lrb- hrtf -rrb- accurately . in this paper , a method suitable for fast computation of the frequencies of spectral notches is proposed . the linear prediction residual cepstrum is used to compute the spectral notches with a high degree of accuracy in this work . subsequent use of batteaus reflection model to overlay the spectral notches on the pinna images indicate that the proposed method is able to provide finer contours . experiments on reconstruction of the hrtf indicates that the method performs better than other methods .",0
242,"This paper proposes a new method for partial matchmaking using approximate subsumption, which can be used to facilitate service discovery and information integration in structured objects. The method relies on approximate logical reasoning and matching tasks to identify partial matches between objects, and is based on the subsumption relation in description logics. The proposed approach aims to address the problem of mismatches between objects and the subsumption criterion by using approximate subsumption. The paper presents a sound and complete algorithm for computing approximate subsumption, and shows that the approach can be used effectively for web ontology language (OWL) matching tasks. The results indicate that the proposed method can achieve high accuracy while significantly reducing the computational cost compared to exact subsumption.",1
243,"description logics , and in particular the web ontology language owl has been proposed as an appropriate basis for computing matches between structured objects for the sake of information integration and service discovery . a drawback of the direct use of subsumption as a matching criterion is the inability to compute partial matches and qualify the degree of mismatch . in this paper , we describe a method for overcoming these problems that is based on approximate logical reasoning . in particular , we approximate the subsumption relation by defining the notion of subsumption with respect to a certain subset of the concept and relation names . we present the formal semantics of this relation , describe a sound and complete algorithm for computing approximate subsumption and discuss its application to matching tasks .",0
244,"This paper proposes a side-informed data hiding scheme based on Weber's law, which takes into account the limitations of the human perceptual system. The proposed scheme uses a logarithmic quantization algorithm to embed data in the host signal, and a generalized version of Weber's law to determine the embedding power. The scheme is designed to be robust against attacks while minimizing the distortion introduced to the host signal. Experimental results show that the proposed scheme outperforms existing state-of-the-art methods in terms of both capacity and perceptual quality.",1
245,"in this work weber 's law is followed for designing a perceptually-shaped side-informed data hiding scheme . the resulting perceptually-shaped side-informed data hiding scheme is a generalized version of a logarithmic quantization algorithm previously proposed by the authors . closed formulas for analyzing the embedding power and decoding error probability of this new perceptually-shaped side-informed data hiding scheme are provided , and experimental results showing its good behavior against severe attacks are reported .",0
246,"This paper proposes a novel method for multiscale manifold representation and modeling, which is applicable to real-world datasets consisting of noisy point cloud samples. The proposed method utilizes the lower-dimensional manifold structure of the data in a higher-dimensional space by performing multiscale representation and wavelet thresholding. The point cloud manifold approximations are obtained using a lifting approach. The proposed method can effectively model and represent complex datasets with unknown smooth manifold structure. The experimental results demonstrate the effectiveness of the proposed method for point cloud denoising and shape reconstruction.",1
247,many real world data sets can be viewed as points in a higher-dimensional space that lie concentrated around a lower-dimensional manifold structure . we propose a new multiscale representation for such point clouds based on lifting and perfect matching . the result is an adaptive wavelet transform that decomposes a point cloud into manifold approximations and details at multiple scales . we illustrate with several examples that the transform can extract an unknown smooth manifold from noisy point cloud samples using simple wavelet thresholding ideas .,0
248,"This paper proposes an improved error-resilient scheme based on Distributed Source Coding (DSC) principles for video transmission. The scheme uses an auxiliary stream to provide redundancy and improve robustness to errors, and employs a decoder side channel to transmit feedback information. The main contribution of the paper is an improved bit allocation algorithm for the auxiliary stream that optimizes the use of available bits and improves the overall performance of the system. The algorithm is based on a novel approach that takes into account the impact of the auxiliary stream on the main video stream, and is shown to outperform existing methods in terms of video quality and error resilience. The proposed scheme and bit allocation algorithm are evaluated using simulations and experimental results demonstrate the effectiveness of the approach.",1
249,"in this work we propose an error-resilient scheme that allows enhancing the robustness of a video stream . based on distributed source coding principles , an auxiliary stream is sent in parallel to the main stream as a redundant representation of the sequence that is used to correct errors at the decoder , thus reducing the impact of drift . in order to perform an optimal bit allocation in the auxiliary stream , the encoder needs to compute a reliable estimate of the expected video distortion observed at the decoder side due to channel loss . this paper proposes an algorithm to calculate the expected distortion of decoded dct-coefficients -lrb- dubbed eddd -rrb- and its application to the bit allocation problem in a dsc based auxiliary stream .",0
250,"In this paper, we present a comparative study of different evidence combination strategies for content-based image retrieval. We evaluate the performance of the bordafuse and combmin combination strategies on a set of Corel images using support vector machines. Our goal is to determine which strategy yields the most accurate and efficient results. We compare the performance of the different strategies in terms of precision and recall, and analyze the strengths and weaknesses of each. Our results show that the choice of combination strategy can have a significant impact on the performance of the retrieval system, and that the best strategy depends on the specific application and dataset being used.",1
251,"this paper reports on experimental results obtained from a performance comparison of feature combinations strategies in content based image retrieval . the use of support vector machines is compared to combmin , combmax , combsum and bordafuse combination strategies , all of which are evaluated on a carefully compiled set of corel images and the trecvid 2003 search task collection .",0
252,This paper presents a time-domain implementation of sound zoning method for low-frequency input signals based on acoustic contrast control. The proposed method aims to achieve a flat frequency response in the controlled zones using an average squared sound pressure criterion. The sound zoning method is applicable to both anechoic and reverberant environments and can be used in personal audio applications. The method uses a bandlimited low-frequency scenario and an acoustically damped room with a loudspeaker layout that is optimized to achieve the desired sound pressure levels in the listening space. The paper compares the proposed method with other sound zoning methods such as the BACC method and shows that the proposed method is more effective in controlling the sound zones.,1
253,"sound zones are two or more regions within a listening space where listeners are provided with personal audio . acoustic contrast control is a sound zoning method that maximizes the average squared sound pressure in one zone constrained to constant pressure in other zones . state-of-the-art time domain broadband acoustic contrast control -lrb- bacc -rrb- sound zoning method are designed for anechoic environments . these sound zoning method are not able to realize a flat frequency response in a limited frequency range within a reverberant environment . sound field control in a limited frequency range is a requirement to accommodate the effective working range of the loudspeakers . in this paper , a new bacc method is proposed which results in an implementation realizing a flat frequency response in the target zone . this bacc method is applied in a bandlimited low-frequency scenario where the loudspeaker layout surrounds two controlled zones . the performance is verified with experimental results in an acousti-cally damped room .",0
254,"This paper explores the structure in dichotomous preferences and its implications for computationally hard approval-based multi-winner rules in social choice theory. Dichotomous profiles, where each voter divides candidates into two categories, have been shown to be computationally tractable under certain conditions. The paper examines restricted domains of preferences, such as single-peaked and single-crossing preferences, and investigates the computational complexity of preference aggregation under these conditions. The findings suggest that the structure in dichotomous preferences can be leveraged to address computationally hard problems in social choice theory.",1
255,"many hard computational social choice problems are known to become tractable when voters ' preferences belong to a restricted domain , such as those of single-peaked or single-crossing preferences . however , to date , all algorithmic results of this type have been obtained for the setting where each voter 's preference list is a total order of candidates . the goal of this paper is to extend this line of research to the setting where voters ' preferences are dichotomous , i.e. , each voter approves a subset of candidates and disapproves the remaining candidates . we propose several analogues of the notions of single-peaked and single-crossing preferences for dichotomous profiles and investigate the relationships among them . we then demonstrate that for some of these notions the respective restricted domains admit efficient algorithms for computationally hard approval-based multi-winner rules .",0
256,"This paper presents a novel approach for joint color and depth completion from stereo images, referred to as stereoscopic inpainting. The method utilizes a segmentation-based approach to identify missing color occlusion regions and uses a depth-assisted texture synthesis technique to fill in the missing information. Specifically, 3D warping is applied to the texture patches from the corresponding stereo image to synthesize the missing texture information in the occlusion regions. The proposed method is evaluated on various data sets and is shown to outperform existing approaches for simultaneous color and depth inpainting.",1
257,"we present a novel algorithm for simultaneous color and depth inpainting . the algorithm takes stereo images and estimated disparity maps as input and fills in missing color and depth information introduced by occlusions or object removal . we first complete the disparities for the occlusion regions using a segmentation-based approach . the completed disparities can be used to facilitate the user in labeling objects to be removed . since part of the removed regions in one image is visible in the other , we mutually complete the two images through 3d warping . finally , we complete the remaining unknown regions using a depth-assisted texture synthesis technique , which simultaneously fills in both color and depth . we demonstrate the effectiveness of the proposed algorithm on several challenging data sets .",0
258,"Abstract: This paper proposes a method for dense scene reconstruction using commodity handheld cameras. The method reconstructs detailed scene geometry by leveraging the locally smooth scene fragments, which are prone to high-frequency errors and low-frequency distortion. To overcome this challenge, the proposed method uses elastic fragments that balance the trade-off between high-frequency and low-frequency errors. The method optimizes the elastic fragments to reduce the distortion in the range data and generate a more accurate reconstruction of the scene geometry. The experimental results demonstrate that the proposed method achieves better performance than existing methods in terms of reconstruction accuracy and visual quality.",1
259,we present an approach to reconstruction of detailed scene geometry from range video . range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion . our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other . we develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes . experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras .,0
260,"This paper proposes a novel image segmentation method called Unlevel-Sets, which combines geometry and prior-based information to achieve accurate and efficient segmentation results. The proposed method is based on the zero-crossing of an evolving level set function, which is guided by both a shape term and a prior term that incorporates knowledge about the image content. The shape term is derived from the Chan-Vese energy functional, while the prior term is based on a top-down segmentation approach that recovers transformation parameters from a reference object. The method is able to handle perspective distortion and contour scaling, and is validated on real-world data sets using ground truth annotations and point correspondences. The results show that the proposed method outperforms existing approaches in terms of segmentation accuracy and robustness.",1
261,"we present a novel variational approach to top-down image segmentation , which accounts for significant projective transformations between a single prior image and the image to be segmented . the proposed variational approach is coupled with reliable estimation of the transformation parameters , without using point correspondences . the prior shape is represented by a generalized cone that is based on the contour of the reference object . its unlevel sections correspond to possible instances of the visible contour under perspective distortion and scaling . we extend the chan-vese energy functional by adding a shape term . this term measures the distance between the currently estimated section of the generalized cone and the region bounded by the zero-crossing of the evolving level set function . promising segmentation results are obtained for images of rotated , translated , corrupted and partly occluded objects . the recovered transformation parameters are compatible with the ground truth .",0
262,"In this paper, we propose a co-regression algorithm for cross-language review rating prediction. We aim to predict the rating of reviews in one language given the ratings of reviews in another language, without the need for parallel corpora or machine translation. Our approach uses human ratings as a bridge between languages and trains two regression models jointly. We show that our co-regression algorithm outperforms several state-of-the-art regression algorithms on a dataset of German and English movie reviews. Our method can be easily extended to other language pairs, making it a promising approach for cross-lingual sentiment analysis.",1
263,"the task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings . in this paper , we aim to investigate a more challenging task of cross-language review rating prediction , which makes use of only rated reviews in a source language -lrb- e.g. english -rrb- to predict the rating scores of unrated reviews in a target language -lrb- e.g. german -rrb- . we propose a new co-regression algorithm to address this task by leveraging unlabeled reviews . evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results .",0
264,"This paper presents a sparse probabilistic state mapping approach and its application to speech bandwidth expansion. The proposed algorithm employs sparsity constraints and entropic priors to map the narrowband speech signal to a higher bandwidth representation with missing frequency components. The state map is learned using a probabilistic model, which enables efficient inference and natural incorporation of prior knowledge. The approach is evaluated on speech signals and compared with a subspace mapping technique, demonstrating improved performance in terms of both subjective and objective measures. The proposed algorithm provides a promising framework for artificial bandwidth expansion in speech and other related applications.",1
265,in this paper we present a probabilistic algorithm that extracts a mapping between two subspaces by representing each subspace as a collection of states . an arbitrary increase in number of states results in over-fitting the training data without exploring the underlying structure of the map . this paper suggests a method to impose sparsity constraints on the state map by using entropic priors . this probabilistic algorithm is applied to the problem of artificial bandwidth expansion that involves estimating the missing frequency components -lrb- 3.7 -- 8 khz and 0 -- 0.3 khz -rrb- of speech given the nar-rowband speech signal -lrb- 0.3 -- 3.7 khz -rrb- .,0
266,"This paper introduces a novel method for image segmentation called ""Corrected Laplacians"". The method utilizes a parameterized family of shapes and statistical shape models to incorporate prior information into the segmentation process. The approach uses a weighted graph and a cut cost to find a combinatorial solution that is optimized using an optimization procedure. Spectral relaxation approaches are used for comparison. The paper presents a quantitative comparison of the proposed method with existing methods using ground truth data and demonstrates its effectiveness in segmenting foreground regions in MRI medical images. The proposed method uses a new formulation of the Laplacian matrix, called the ""Corrected Laplacian"", that improves the performance of the average case normalized cut. The approach also incorporates sparsity constraints and edge-relaxation SDPs to improve the perceptual relevance of the segmentation results. Overall, the paper presents a new approach to image segmentation that combines prior information with spectral relaxation methods and combinatorial optimization to achieve state-of-the-art performance.",1
267,"we optimize over the set of corrected laplacians associated with a weighted graph to improve the average case normalized cut of a graph partitioning . unlike edge-relaxation sdps , optimizing over the set cl naturally exploits the matrix sparsity by operating solely on the diagonal . this structure is critical to image segmentation applications because the number of vertices is generally proportional to the number of pixels in the image . cl optimization provides a guiding principle for improving the combinatorial solution over the spectral relaxation , which is important because small improvements in the cut cost often result in significant improvements in the perceptual relevance of the segmenta-tion . we develop an optimization procedure to accommodate prior information in the form of statistical shape models , resulting in a segmentation method that produces foreground regions which are consistent with a parameterized family of shapes . we validate our optimization procedure with ground truth on mri medical images , providing a quantitative comparison against results produced by current spectral relaxation approaches to graph partitioning .",0
268,"This paper proposes a method for optimizing expensive functions with stochastic binary outcomes using Bayesian optimization. The goal is to address optimization problems that arise in real-world systems, where the outcome of each function evaluation is a binary stochastic variable. The proposed approach uses a metric for experiment selection and an offline training phase to build a model of the system. The method is evaluated on synthetic problems as well as physical systems, such as bandit problems and parameterized controllers. Bayesian optimization methods, specifically Gaussian processes, are shown to be effective in addressing the stochastic binary optimization problem. The results demonstrate the potential of the proposed approach for optimizing expensive functions with stochastic binary outcomes.",1
269,"real world systems often have parameterized controllers which can be tuned to improve performance . bayesian optimization methods provide for efficient optimization of these real world systems , so as to reduce the number of required experiments on the expensive physical system . in this paper we address bayesian optimization in the setting where performance is only observed through a stochastic binary outcome -- success or failure of the experiment . unlike bandit problems , the goal is to maximize the real world systems performance after this offline training phase rather than minimize regret during training . in this work we define the stochastic binary optimization problem and propose an approach using an adaptation of gaussian processes for stochastic binary optimization problem that presents a bayesian optimization framework for this stochastic binary optimization problem . we propose an experiment selection metric for this setting based on expected improvement . we demonstrate the algorithm 's performance on synthetic problems and on a real snake robot learning to move over an obstacle .",0
270,"This paper proposes a new approach for solving the maximum a posteriori (MAP) hypothesis problem on Bayesian networks. The approach is based on compiling the network into an arithmetic circuit and using branch-and-bound search with treewidth-imposed limits to search for the optimal solution. The paper shows that the compiled arithmetic circuit can be constructed in linear time using variable elimination and join-tree algorithms, which exploit the local structure of the network. The paper also derives treewidth bounds on the compiled arithmetic circuit and shows that the approach can solve the MAP problem exactly within these bounds. Experimental results demonstrate that the approach outperforms existing methods that use constrained treewidth or structure-based inference methods. Overall, this paper provides a promising new direction for exact MAP inference on Bayesian networks.",1
271,"the map -lrb- maximum a posteriori hypothesis -rrb- problem in bayesian networks is to find the most likely states of a set of variables given partial evidence on the complement of that set . standard structure-based inference methods for finding exact solutions to map , such as variable elimination and join-tree algorithms , have complexities that are exponential in the constrained treewidth of the network . a more recent algorithm , proposed by park and darwiche , is exponential only in the treewidth and has been shown to handle networks whose constrained treewidth is quite high . in this paper we present a new algorithm for exact map that is not necessarily limited in scalability even by the treewidth . this is achieved by leverag-ing recent advances in compilation of bayesian networks into arithmetic circuits , which can circumvent treewidth-imposed limits by exploiting the local structure present in the bayesian networks . specifically , we implement a branch-and-bound search where the bounds are computed using linear-time operations on the compiled arithmetic circuit . on networks with local structure , we observe orders-of-magnitude improvements over the algorithm of park and darwiche . in particular , we are able to efficiently solve many problems where the latter algorithm runs out of memory because of high treewidth .",0
272,"This paper presents a non-parametric Bayesian approach to segmenting Japanese noun phrases, which is a challenging task due to the lack of explicit word boundaries in the language. The proposed approach integrates both language models and external lexical resources to improve segmentation accuracy. The approach utilizes a hybrid type-based sampling method to efficiently explore the space of possible segmentations, and incorporates a block sampling procedure to enhance global optimization. To improve local optimization, the approach uses a morphological analyzer and a high-coverage dictionary. The paper also proposes segmentation criteria that balance segmentation accuracy and efficiency. Experimental results demonstrate that the proposed approach outperforms existing methods in terms of segmentation accuracy on a benchmark dataset. Overall, this paper provides a promising solution to the problem of Japanese noun phrase segmentation, with potential applications in natural language processing and information retrieval.",1
273,"a key factor of high quality word segmenta-tion for japanese is a high-coverage dictionary , but it is costly to manually build such a lexical resource . although external lexical resources for human readers are potentially good knowledge sources , external lexical resources have not been utilized due to differences in segmentation criteria . to supplement a morphological dictionary with these resources , we propose a new task of japanese noun phrase segmentation . we apply non-parametric bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text . for inference , we propose a novel block sampling procedure named hybrid type-based sampling , which has the ability to directly escape a local optimum that is not too distant from the global optimum . experiments show that the proposed block sampling procedure efficiently corrects the initial segmentation given by a morphological ana-lyzer .",0
274,"This paper proposes a novel approach for delaying commitment in plan recognition using Combinatory Categorial Grammars (CCGs). The proposed approach is based on lexicalized intent recognition, which allows the system to recognize the goals of a plan without committing to a specific interpretation of the plan until later stages of the recognition process. CCGs are used as the grammatical formalism to capture the structural dependencies between words in a sentence, and to guide the recognition process. The paper presents a detailed description of the approach and its implementation, and provides experimental results demonstrating the effectiveness of the approach in delaying commitment and improving plan recognition accuracy. The paper also discusses potential applications of the approach in various domains, including natural language processing, intelligent systems, and robotics. Overall, this paper presents a promising approach for delaying commitment in plan recognition using CCGs, which has the potential to improve the accuracy and efficiency of plan recognition systems in various domains.",1
275,this paper presents a new algorithm for plan recognition called elexir -lrb- engine for lexicalized intent recognition -rrb- . elexir represents the plans to be recognized with a grammatical formalism called combinatory categorial grammar . we show that representing plans with combinatory categorial grammar can allow us to prevent early commitment to plan goals and thereby reduce runtime .,0
276,"This paper presents an entity-centric approach to coreference resolution using model stacking. The proposed approach utilizes mention pair scores to train an entity-centric coreference system, which incorporates entity-level information to resolve coreferences at the entity level. The system is trained on the CoNLL shared task dataset, and utilizes a loss function that considers both mention-level and entity-level features. The paper describes the architecture of the system, which uses multiple mention pair models to generate features for the entity-level system. The system also incorporates a policy that guides the search space for possible coreference chains, which is informed by the mention pair models and the entity-level features. Experimental results demonstrate that the proposed approach outperforms existing state-of-the-art methods on the CoNLL dataset. Overall, this paper presents a promising approach for entity-centric coreference resolution using model stacking, which has the potential to improve the accuracy and efficiency of coreference resolution systems in English and other languages.",1
277,"mention pair mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coref-erence resolution , but do not make use of entity-level information . however , we show that the scores produced by such mention pair models can be aggregated to define powerful entity-level features between clusters of mentions . using these features , we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally . the mention pair scores are also used to prune the search space the entity-centric coreference system works in , allowing for efficient training with an exact loss function . we evaluate our entity-centric coreference system on the english portion of the 2012 conll shared task dataset and show that entity-centric coreference system improves over the current state of the art .",0
278,"This paper investigates the role of discourse cues in facilitating recall in information presentation messages. The study examines whether the presence of discourse cues, such as transition words and phrases, can improve recall of information presented in written messages. A total of 100 participants were recruited and presented with two versions of the same message: one with discourse cues and one without. Participants were then asked to recall as much information from the message as possible. The study found that the presence of discourse cues led to significantly better recall of information. The paper discusses the implications of these findings for the design of effective information presentation messages, and provides recommendations for incorporating discourse cues to improve recall. Overall, this paper provides important insights into the role of discourse cues in facilitating recall in information presentation messages, with potential applications in various domains, including education, communication, and marketing.",1
279,this paper describes an experiment comparing the effect of two different approaches to information presentation on item recall . the results show that using discourse cues facilitates recalling the presented information .,0
280,"This paper compares the performance of two widely used algorithms for solving correspondence problems in computer vision field: RANSAC and CS-RANSAC. The study focuses on the problem of estimating homography matrices from feature points in stereo images, which is a crucial step in many computer vision applications. The paper discusses the limitations of RANSAC algorithm in dealing with degenerate features and proposes CS-RANSAC as an alternative approach. CS-RANSAC uses a two-layer grid sampling strategy to efficiently explore the search space and solve the constraint satisfaction problem. The study evaluates the performance of both algorithms on a range of datasets, comparing their accuracy and execution time. The results show that CS-RANSAC outperforms RANSAC in terms of both accuracy and efficiency, particularly in cases where degenerate features are present. The paper concludes that CS-RANSAC is a promising approach for solving correspondence problems in computer vision applications, with potential applications in various domains, including robotics, autonomous vehicles, and augmented reality.",1
281,"a homography matrix is used in computer vision field to solve the correspondence problem between a pair of stereo images . ransac algorithm is often used to calculate the homography matrix by randomly selecting a set of features iteratively . ransac algorithm in this paper converts ransac algorithm into two-layers . the first layer is addressing sampling problem which we can describe our knowledge about degenerate features by mean of constraint satisfaction problems . by dividing the input image into a grid and making feature points into discrete domains , we can model the image into the constraint satisfaction problems to efficiently filter out degenerate features . by expressing the knowledge about degenerate feature samples using constraint satisfaction problems in the first layer , so that computer has knowledge about how to skip computing the homography matrix in the model estimation step for the second layer . the experimental results show that the proposed ransac algorithm can outperform the most of variants of ransac without sacrificing its execution time .",0
282,"This paper presents tensor-based spatiotemporal image processing techniques for studying dynamical processes in various fields such as environmental physics, biology, and computer vision. The techniques are applied to analyze ocean surface microturbulence in IR image sequences and sediment transport, as well as plant growth using calibrated image sequence analysis. The paper discusses the optimization of low-level motion estimators for accurate estimation of the motion field and the use of derivative filters for spatial non-uniformity. The tensor method is evaluated for computer-generated sequences and real-world images using CCD sensors, with a focus on accuracy and responsivity. The paper highlights the importance of first-order derivatives of the motion field and dynamical changes of the moving objects for understanding the exchange, growth, and transport processes in various dynamical systems.",1
283,"image sequence processing image sequence processing techniques are used to study exchange , growth , and transport processes and to tackle key questions in environmental physics and biology . these applications require high accuracy for the estimation of the motion field since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects . therefore the performance and optimization of low-level motion estimators is discussed . a tensor method tuned with carefully optimized derivative filters yields reliable and dense displacement vector fields -lrb- dvf -rrb- with an accuracy of up to a few hundredth pixels/frame for real-world images . the accuracy of the tensor method is verified with computer-generated sequences and a calibrated image sequence . with the improvements in accuracy the motion estimation is now rather limited by imperfections in the ccd sensors , especially the spatial nonuni-formity in the responsivity . with a simple two-point calibration , these effects can efficiently be suppressed . the application of the image sequence processing techniques to the analysis of plant growth , to ocean surface microturbulence in ir image sequences , and to sediment transport is demonstrated .",0
284,"This paper explores bilinear signal synthesis in array processing. The focus is on improving the quality of multiple source signals by mitigating smearing of the signal terms. Blind source separation methods and spatial signatures of sources are examined, as well as techniques for source separation using spatial time-frequency distributions and quadratic T-F distributions. The paper also considers weighing functions and non-integer values in the time-frequency domain. To optimize the synthesis process, the authors propose several methods, including antenna array averaging, crossterms whitening, and the use of angular separations and source positions. The paper concludes that the proposed techniques improve the accuracy of the synthesis process, even in the presence of communication channel noise levels.",1
285,"| multiple source signals impinging on an antenna array can be separated by time-frequency synthesis techniques . averaging of the time-frequency distributions of the data across the array permits the spatial signatures of sources to play a fundamental role in improving the synthesis performance . array a veraging introduces a weighing function in the time-frequency domain that decreases the noise levels , reduces the interactions of the source signals , and mitigates the crossterms . this is achieved independent of the temporal characteristics of the source signals and without causing any smearing of the signal terms . the weighing function may take non-integer values , which are determined by the communication channel , the source positions and their angular separations . unlike the recently devised blind source separation methods using spatial time-frequency distributions , the proposed method does not require whitening or retrieval of the source directional matrix . the paper evaluates the proposed method in terms of performance and computations relative t o the existing source separation techniques based on quadratic t-f distributions .",0
286,"This paper discusses the use of probabilistic latent variable models for distinguishing between cause and effect in real-world data. Specifically, the paper considers a hypothetical cause variable and a hypothetical effect variable, with an unobserved noise variable affecting the relationship between the two. The paper proposes a Bayesian model selection approach to determine the causal direction between the two variables, using both synthetic and real-world data. The authors demonstrate the effectiveness of their approach in a variety of model classes, highlighting the importance of considering the underlying structure of the data when attempting to distinguish between cause and effect. Overall, this work provides a useful framework for using probabilistic latent variable models in causal inference tasks.",1
287,"we propose a novel method for inferring whether x causes y or vice versa from joint observations of x and y . the basic idea is to model the observed data using probabilistic latent variable models , which incorporate the effects of unobserved noise . to this end , we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term -lrb- not necessarily additive -rrb- . an important novel aspect of our work is that we do not restrict the model class , but instead put general non-parametric priors on this function and on the distribution of the cause . the causal direction can then be inferred by using standard bayesian model selection . we evaluate our approach on synthetic data and real-world data and report encouraging results .",0
288,"This paper presents a novel approach for automatic speech recognition without any audio information. The proposed method utilizes the electromagnetic articulography device to capture tongue and lip parameters during speech production. These parameters are then used to extract phonetic features, which are modeled using hidden Markov models. The accuracy of the proposed method is evaluated using both synthetic and real-world data, and compared with lip parameter-based models. The results show that the proposed method achieves higher accuracy in speech recognition, demonstrating the potential of using tongue parameters in speech recognition systems.",1
289,"this article introduces automatic recognition of speech without any audio information . movements of the tongue , lips , and jaw are tracked by an electromagnetic articulogra-phy device and are used as features to create hidden markov models and conduct automatic speech recognition in a conventional way . the results obtained are promising , which confirm that phonetic features characterizing articulation are as discriminating as those characterizing acoustics -lrb- except for voicing -rrb- . the results also show that using tongue parameters result in a higher accuracy compared with the lip parameters .",0
290,"This paper explores the potential of progressive randomization (PR) as a technique to improve the accuracy of broad image categorization. PR is a perturbation method that modifies images by adding or removing information while preserving their visual content. The study shows that PR can effectively increase the accuracy of image categorization compared to traditional approaches. This is achieved by training the classification model on PR perturbed images and using statistical descriptors of least significant bit (LSB) occurrences as image descriptors. The paper also discusses the importance of using a diverse set of training examples and image databases for effective PR. Overall, the results suggest that PR has more potential than what meets the eye, and it can be a valuable tool for image classification tasks.",1
291,"in this paper , we introduce a new image descriptor for broad image categorization , the progressive randomiza-tion , that uses perturbations on the values of the least significant bits -lrb- lsb -rrb- of images . we show that different classes of images have a distinct behavior under our image descriptor , and that using statistical descriptors of lsb occurrences and enough training examples , the image descriptor already performs as well or better than comparable existing techniques in the literature . with few training examples , pr still has good separability , and its accuracy increases with the size of the training set . we validate our image descriptor using four image databases with different categories .",0
292,"This paper presents a tuned eigenspace technique for recognizing articulated motion in real-world and synthetic environments. The technique employs sequential postures and tuned eigenspaces to recognize human motion and poses. The method uses background subtraction and clothing texture to segment the human body, and statistical descriptors to analyze the motion. The proposed technique is tested on both synthetic and real-world data and achieves high accuracy for recognition tasks in outdoor environments. Results show that the tuned eigenspace technique outperforms other methods in recognizing human motion and poses, making it a promising approach for articulated motion recognition.",1
293,"in this paper , we introduce a tuned eigenspace technique so as to classify human motion . the tuned eigenspace technique presented here overcomes those problems related to articulated motion and dress texture effects by learning various human motions in terms of their sequential postures in an eigenspace . in order to cope with the variability inherent to articulated motion , we propose a tuned eigenspace technique to tune the set of sequential eigenspaces . once the learnt tuned eigenspaces are at hand , the recognition task then becomes a nearest-neighbor search over the eigenspaces . we show how our tuned eigenspace technique can be used for purposes of real-world and synthetic pose recognition . we also discuss and overcome the problem related to clothing texture that occurs in real-world data , and propose a background subtraction method to employ the tuned eigenspace technique in outdoor environment . we provide results on synthetic imagery for a number of human poses and illustrate the utility of the tuned eigenspace technique for the purposes of human motion recognition .",0
294,"This paper presents a novel approach for interactive tracking of 2D generic objects using spacetime optimization. The proposed framework enables simultaneous tracking of multiple objects in the presence of sudden movement, visual measurements, tracking ambiguity, and illumination changes. The system is based on a continuous optimization framework that uses user constraints to improve accuracy and robustness of the tracking process. The key contribution of the paper is the spacetime optimization framework, which allows for efficient tracking of generic objects by exploiting spatiotemporal correlations in the data. The proposed method is evaluated on several datasets and demonstrates superior performance compared to state-of-the-art methods in terms of tracking accuracy and robustness to sudden movements and scale changes.",1
295,"we present a continuous optimization framework for interactive tracking of 2d generic objects in a single video stream . the user begins with specifying the locations of a target object in a small set of keyframes ; the continuous optimization framework then automatically tracks locations of the objects by combining user constraints with visual measurements across the entire sequence . we formulate the problem in a spacetime optimization framework that optimizes over the whole sequence simultaneously . the resulting continuous optimization framework is consistent with visual measurements across the entire sequence while satisfying user constraints . we also introduce prior terms to reduce tracking ambiguity . we demonstrate the power of our continuous optimization framework on tracking ambiguity with significant occlusions , scale and orientation changes , illumination changes , sudden movement of objects , and also simultaneous tracking of multiple objects . we compare the performance of our continuous optimization framework with alternative methods .",0
296,"This paper proposes a parameter estimation technique that employs sparse reconstruction with dynamic dictionaries. The paper explores the use of dynamic dictionary subset selection approach and compares it with fixed dictionary approaches. The proposed technique addresses the issue of parameter bias in existing parameter estimation techniques. The paper demonstrates that the dynamic dictionary approach provides better results compared to fixed dictionary approaches, especially when dealing with parameterized functions and dictionary quantization. The study shows that the selection of dictionary elements and the sparsity of the reconstruction significantly affect the accuracy of the estimation. Overall, the proposed approach shows promise in improving parameter estimation in various applications.",1
297,"we consider the problem of parameter estimation for signals characterized by sums of parameterized functions . we present a dynamic dictionary subset selection approach to parameter estimation where we iteratively select a small number of dictionary elements and then alter the parameters of these dictionary elements to achieve better signal model fit . the proposed dynamic dictionary subset selection approach avoids the use of highly oversampled -lrb- and highly correlated -rrb- dictionary elements , which are needed in fixed dictionary approaches to reduce parameter bias associated with dictionary quantization . we demonstrate estimation performance on a sinusoidal signal estimation example .",0
298,"This paper proposes a cluster identification method for speaker-environment tracking. The authors introduce a cluster-labelling scheme selection approach that utilizes broadcast news (BBN) metrics to identify clusters of audio signals corresponding to specific speakers and their environments. The BBN metrics are used to measure the similarity between pairs of audio signals and to form clusters based on their similarity. The proposed approach is evaluated using a speaker-environment tracking system, and the results demonstrate its effectiveness in accurately identifying the speaker and their environment. Overall, the proposed method provides a useful tool for audio signal processing in a variety of applications.",1
299,cluster identification is introduced as the process of jointly evaluating clustering and labelling schemes for cluster-labelling scheme selection . normalized rand and bbn metrics for comparing clustering performances across varied clustering and labelling schemes are presented . the merits of the bbn metrics are evaluated and applied for speaker-environment tracking in broadcast news .,0
300,"This paper explores the analysis of acoustic features in the estimation of speech emotion primitives. Robust linear and nonlinear estimation techniques are used to estimate emotion primitives such as activation, valence, and dominance from speech. Local and global speech duration, energy, and MFCC are used as acoustic features. Feature selection is performed to determine the relative importance of each feature. The paper also examines the average fusion of estimators and evaluates their performance using mean absolute error. The results demonstrate that the proposed approach can accurately estimate emotion primitives from speech with high accuracy.",1
301,"we recently proposed a family of robust linear and nonlin-ear estimation techniques for recognizing the three emotion primitives -- valence , activation , and dominance -- from speech . these were based on both local and global speech duration , energy , mfcc and pitch features . this paper aims to study the relative importance of these four categories of acoustic features in this emotion estimation context . three measures are considered : the number of features from each category when all features are used in selection , the mean absolute error when each category is used separately , and the mean absolute error when a category is excluded from feature selection . we find that the relative importance is in the order of mfcc > energy ≈ pitch > duration . additionally , estimator fusion almost always improves performance , and locally weighted fusion always outperforms average fusion regardless of the number of features used .",0
302,"This paper proposes a new algorithm called FBEM for the joint optimization of features and acoustic model parameters in bird call classification. The algorithm combines a filter bank with an Expectation-Maximization (EM) algorithm to jointly optimize the filter bank and acoustic model parameters. The mel-scaled filter bank is used for feature extraction, and the EM algorithm is used to optimize the acoustic model parameters. The classification error rate is evaluated to measure the effectiveness of the algorithm. The results show that the FBEM algorithm outperforms the traditional gradient ascent method in terms of classification accuracy. Overall, this study provides a new approach to improve the accuracy of bird call classification.",1
303,"this paper extends the expectation-maximization algorithm to estimate not only optimal acoustic model parameters , but also optimal center frequencies and bandwidths of the filter bank used in cepstral feature extraction for bird call classification . the search is done using the gradient ascent method . filter bank and model parameters are optimized iteratively . experiments are conducted on a large noisy corpus containing antbird calls from 5 species . it is shown that features extracted using the optimized filter bank result in a lower classification error rate than those extracted using a mel-scaled filter bank .",0
304,"This paper proposes a new approach for reinforcement learning with hidden state, called Instance-Based Utile Distinctions (IBUD). The IBUD algorithm utilizes a tree-structured representation of the state space, which allows for the detection of state aliasing, a common problem in reinforcement learning with hidden state. IBUD uses short-term memory to store and retrieve state-action pairs, and utile suffix memory to extract and store useful features for decision making. The paper demonstrates the effectiveness of the IBUD algorithm through experiments on several tasks, including grid-world and maze navigation problems, and compares it with other state-of-the-art algorithms. The results show that IBUD outperforms other methods in terms of learning efficiency and accuracy. The paper also discusses statistical tests that can be used to evaluate the significance of the differences in performance between algorithms.",1
305,"we present utile suffix memory , a reinforcement learning algorithm that uses short-term memory to overcome the state aliasing that results from hidden state . by combining the advantages of previous work in instance-based -lrb- or '' memory-based '' -rrb- learning and previous work with statistical tests for separating noise from task structure , the reinforcement learning algorithm learns quickly , creates only as much memory as needed for the task at hand , and handles noise well . utile suffix memory uses a tree-structured representation , and is related to work on predic",0
306,"This paper proposes a data-driven approach for continuous speech recognition using pure example-based recognition methodology. In contrast to the dominant acoustic modeling approach using hidden Markov models (HMMs), the proposed approach utilizes a large set of speech examples for recognition. The Dynamic Time Warping (DTW) algorithm is used to perform alignment between the speech examples and the input speech signal. The approach also takes into account speaker information and time dependencies to improve recognition accuracy. The search space for recognition is reduced by selecting the best matching speech examples using statistical tests. The paper also discusses HMM adaptation using the proposed approach. Experimental results demonstrate that the proposed approach outperforms traditional HMM-based approaches in certain scenarios.",1
307,"the dominant acoustic modeling methodology based on hidden markov models is known to have certain weaknesses . partial solutions to these flaws have been presented , but the fundamental problem remains : compression of the data to a compact hmm discards useful information such as time dependencies and speaker information . in this paper , we look at pure example based recognition as a solution to this problem . by replacing the hmm with the underlying examples , all information in the training data is retained . we show how information about speaker and environment can be used , introducing a new interpretation of adaptation . the basis for the pure example based recognition is the well-known dtw algorithm , which has often been used for small tasks . however , large vocabulary speech recognition introduces new demands , resulting in an explosion of the search space . we show how this problem can be tackled using a data driven approach which selects appropriate speech examples as candidates for dtw-alignment .",0
308,"Abstract: This paper investigates the problem of rate-optimal MIMO transmission with mean and covariance feedback in a wireless communication scenario with low SNR. A spatially-correlated complex Gaussian distribution is assumed for the channel. The ergodic achievable rate from an ergodic rate perspective is considered, and a beamforming strategy is proposed to achieve the rate-optimal performance. The impact of non-zero mean on the system performance is also studied. Simulation results show that the proposed approach can achieve rate optimality even at low SNRs.",1
309,"we consider a multiple-input multiple-output wireless communication scenario in which the channel follows a general spatially-correlated complex gaussian distribution with non-zero mean . we derive an explicit characterization of the optimal input covariance from an ergodic rate perspective for systems that operate at low snrs . this characterization is in terms of the eigen decomposition of a matrix that depends on the mean and the covariance of the channel , and typically results in a beamforming strategy along the principal eigenvector of that matrix . simulation results show the potential impact of -lrb- jointly -rrb- exploiting the mean and the covariance of the channel on the ergodic achievable rate at both low and moderate-to-high snrs .",0
310,"This paper proposes a two-phase kernel estimation method for robust motion deblurring. The proposed method employs a variable substitution scheme and a spatial prior to refine the estimated kernel. The first phase involves an iterative support detection process to identify the edges in the image. In the second phase, a TV-1 deconvolution model is used to estimate the kernel. The proposed method also incorporates a gradient selection process and a hard threshold to improve the accuracy of kernel estimation. The experiments demonstrate that the proposed method achieves better results in terms of image quality and PSNR than other state-of-the-art methods.",1
311,"we discuss a few new motion deblurring problems that are significant to kernel estimation and non-blind deconvolution . we found that strong edges do not always profit kernel estimation , but instead under certain circumstance degrade it . this finding leads to a new metric to measure the usefulness of image edges in motion deblurring and a gradient selection process to mitigate their possible adverse effect . we also propose an efficient and high-quality kernel estimation method based on using the spatial prior and the iterative support detection kernel refinement , which avoids hard threshold of the kernel elements to enforce sparsity . we employ the tv-1 deconvolution model , solved with a new variable substitution scheme to robustly suppress noise .",0
312,"This paper investigates what color changes can reveal about an outdoor scene. The analysis of outdoor scenes is conducted using temporal color changes captured in time-lapse video data. The spectral composition of daylight and radiometric and geometric information are used to infer the ambient skylight and direct sunlight. Visual tasks such as scene reconstruction, shadow detection, and background subtraction are explored. The camera geo-location is also considered for scene reconstruction and visual tasks. The paper proposes using image sequence analysis techniques to extract information about the scene. The results show that color changes can provide useful information for various visual tasks in outdoor scenes.",1
313,"in an extended image sequence of an outdoor scene , one observes changes in color induced by variations in the spectral composition of daylight . this paper proposes a model for these temporal color changes and explores its use for the analysis of outdoor scenes from time-lapse video data . we show that the time-varying changes in direct sunlight and ambient skylight can be recovered with this model , and that an image sequence can be decomposed into two corresponding components . the decomposition provides access to both radiometric and geometric information about a scene , and we demonstrate how this can be exploited for a variety of visual tasks , including color-constancy , background subtraction , shadow detection , scene reconstruction , and camera geo-location .",0
314,"This paper presents a method for automatically aligning clauses in parallel texts using a probabilistic model and statistical techniques. The method is evaluated using a small English-Greek corpus and incorporates shallow linguistic processing, simulated annealing, and regular grammars. The system aligns clauses at the sentence and clause level, and takes into account part-of-speech categories, word occurrence, and character lengths. Co-occurrence probabilities are used to improve the accuracy of the alignments. The software system developed for this purpose can be used with any parallel bilingual corpus, and has potential applications in machine translation and natural language processing.",1
315,"this paper describes a method for the automatic alignment of parallel texts at clause level . the method features statistical techniques coupled with shallow linguistic processing . it presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus . parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories . regular grammars functioning on tags , recognize clauses on both sides of the parallel text . a probabilistic model is applied next , operating on the basis of word occurrence and co-occurrence probabilities and character lengths . depending on sentence size , possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim ~ te the best alignment . 1he method has been tested on a small eng ~ lish-greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments .",0
316,"This paper presents a method for learning to take concurrent actions within the framework of a semi-Markov decision process. The approach involves utilizing temporally extended actions and parallel termination schemes to enable concurrent decision-making. The proposed method enables more efficient and effective decision-making in settings where concurrent actions are necessary. By incorporating the concept of concurrency into the decision-making process, the method provides a more realistic representation of many real-world scenarios, such as those encountered in robotics and other areas of artificial intelligence. The paper describes the methodology in detail and presents experimental results demonstrating the effectiveness of the approach. Overall, this work contributes to the development of more advanced decision-making systems that are better suited to handle complex, real-world scenarios.",1
317,"we investigate a general semi-markov decision process framework for modeling concurrent decision making , where agents learn optimal plans over concurrent temporally extended actions . we introduce three types of parallel termination schemes -- all , any and continue -- and theoretically and experimentally compare them .",0
318,"This paper proposes a method for clustering unknown speech utterances based on maximizing the within-cluster homogeneity of speaker voice characteristics. The proposed method uses a divergence-based model to measure the similarity between speech utterances and within-cluster homogeneity is defined as the similarity between the speaker voice characteristics of the utterances in a cluster. To ensure computational efficiency, a likelihood probability is introduced and a genetic algorithm is used to optimize the clustering process. The performance of the proposed method is evaluated using a set of experiments and the results show that it outperforms other clustering methods in terms of cluster homogeneity.",1
319,"this paper investigates the problem of how to partition unknown speech utterances into clusters , such that the overall within-cluster homogeneity of speakers ' voice characteristics can be maximized . the within-cluster homogeneity is characterized by the likelihood probability that a cluster model , trained using all the utterances within a cluster , matches each of the within-cluster utterances . such probability is then maximized by using a genetic algorithm , which determines the best cluster where each utterance should be located . for greater computational efficiency , also proposed is an alternative solution that approximates the likelihood probability with a divergence-based model similarity . the method is further designed to estimate the optimal number of clusters automatically .",0
320,"This paper presents a multibaseline stereo system with active illumination and real-time image acquisition for dense stereo depth data. The system uses a four-camera multibaseline stereo configuration to capture images of camera view areas, and a parallel depth recovery scheme is employed to recover the stereo depth data. The system is capable of capturing images at video rate, and the convergence configuration allows for three-dimensional tracking. Local discriminability is achieved through the use of active illumination. The system is evaluated for image capture and the parallel depth recovery scheme is found to be effective, providing high-quality stereo depth data in real time.",1
321,"we describe our implementation of a parallel depth recovery scheme for a four-camera multibaseline stereo in a conver-gent configuration . our parallel depth recovery scheme is capable of image capture at video rate . this is critical in applications that require three-dimensional tracking . we obtain dense stereo depth data by projecting a light pattern of frequency modulated sinusoidally varying intensity onto the scene , thus increasing the local discriminability at each pixel and facilitating matches . in addition , we make most of the camera view areas by converging them at a volume of interest . results show that we are able to extract stereo depth data that are , on the average , less than 1 mm in error at distances between 1.5 to 3.5 m away from the cameras .",0
322,"This paper presents a study on source coding with intermittent and degraded side information at the decoder, where Gaussian-erasure models and non-standard correlation models are used to characterize non-stationarities in the correlation between the source and side information. Lower and upper bounds on rate-distortion functions are derived, along with practical schemes for distributed video coding. The paper also explores the use of Gaussian sources with impulse noise and discusses the challenges and opportunities of using correlation models in this context. The results suggest that correlation models can improve the performance of practical coding schemes, particularly when the correlation between the source and side information is Gaussian.",1
323,"practical schemes for distributed video coding with side information at the decoder need to consider non-standard correlation models in order to take non-stationarities into account . in this paper we introduce two correlation models for gaussian sources , the gaussian-bernoulli-gaussian -lrb- gbg -rrb- and the gaussian-erasure models , and evaluate lower and upper bounds on their rate-distortion functions . provided that the probability of impulse noise or of erasures remains small , these bounds remain close to the rate-distortion function for gaussian correlation . two practical schemes for the ge correlation model are also presented , with performance about 1.5 db away from the upper bound .",0
324,"This paper characterizes n-player strongly monotone scheduling mechanisms. The paper focuses on additive valuations and combinatorial auctions, as well as affine minimizers. The authors present a task-independent approach that groups players and minimizes their costs. The paper provides insights into the design and implementation of scheduling mechanisms that can be used to achieve strong monotonicity in multi-agent systems. The proposed approach can be applied to a variety of scheduling problems, and the results of this paper can inform future research in this area.",1
325,"our work deals with the important problem of globally characterizing truthful mechanisms where players have multi-parameter , additive valuations , like scheduling unrelated machines or additive combinatorial auctions . very few mechanisms are known for these settings and the question is : can we prove that no other truthful mechanisms exist ? we characterize truthful mechanisms for n players and 2 tasks or items , as either task-independent , or a player-grouping minimizer , a new class of mechanisms we discover , which generalizes affine min-imizers . we assume decisiveness , strong mono-tonicity and that the truthful payments 1 are continuous functions of players ' bids .",0
326,This paper proposes a random time-frequency subdictionary design for sparse representations with greedy algorithms for low bit rate compression of audio signals. The authors aim to achieve a sparse signal approximation using controlled computational complexity in a probabilistic fashion. They use parameter estimation to design the subdictionaries and employ dictionary space decomposition algorithms for sparse decompositions. The main focus of the paper is on achieving sparsity with the use of greedy decomposition processes. The proposed approach is expected to improve the performance of low bit-rate coding schemes.,1
327,"sparse signal approximation can be used to design efficient low bit-rate coding schemes . sparse signal approximation heavily relies on the ability to design appropriate dictionaries and corresponding decomposition algorithms . the size of the dictionary , and therefore its resolution , is a key parameter that handles the tradeoff between sparsity and tractability . this work proposes the use of a non adaptive random sequence of subdictionaries in a greedy decomposition process , thus browsing a larger dictionary space in a probabilistic fashion with no additional projection cost nor parameter estimation . this sparse signal approximation leads to very sparse decompositions , at a controlled computational complexity . experimental evaluation is provided as proof of concept for low bit rate compression of audio signals .",0
328,"This paper proposes a new approach for emotion recognition from speech using amplitude modulation features. The proposed method utilizes a smoothed nonlinear energy operator and an energy separation algorithm to obtain the amplitude and frequency components of speech signals. The C-channel Gammatone filterbank is then applied to extract the amplitude modulation power spectrum, which is further processed by Discrete Cosine Transform and Root Compressed AM Power Spectrum to obtain the AMCC features. The proposed approach is evaluated on the FAU Aibo Spontaneous Emotion Corpus and achieves promising results in recognizing emotions. The paper also compares the proposed approach with the conventional MFCC features and demonstrates the superiority of the proposed approach.",1
329,"the goal of speech emotion recognition is to identify the emotional or physical state of a human being from his or her voice . one of the most important things in a speech emotion recognition is to extract and select relevant speech features with which most emotions could be recognized . in this paper , we present a smoothed nonlinear energy operator - based amplitude modulation cepstral coefficients -lrb- amcc -rrb- feature for recognizing emotions from speech signals . smoothed nonlinear energy operator estimates the energy required to produce the am-fm signal , and then the estimated energy is separated into its amplitude and frequency components using an energy separation algorithm . amcc features are obtained by first decomposing a speech signal using a c-channel gammatone filterbank , computing the am power spectrum , and taking a discrete cosine transform of the root compressed am power spectrum . conventional mfcc -lrb- mel-frequency cepstral coefficients -rrb- and mel-warped dft -lrb- discrete fourier transform -rrb- spectrum based cepstral coefficients -lrb- mwdcc -rrb- features are used for comparing the recognition performances of the proposed features . emotion recognition experiments are conducted on the fau aibo spontaneous emotion corpus . it is observed from the experimental results that the amcc features provide a relative improvement of approximately 3.5 % over the baseline mfcc .",0
330,This paper presents a robust hidden Markov model (HMM) training algorithm for a unified Dutch and German speech recognition system. The system includes Dutch and German subword recognition tasks and uses a common set of phoneme models. The proposed algorithm uses a minimum classification error (MCE)-based training approach to reduce the overall string error rate of the system. The acoustic component of the system is evaluated in the SpeechDat domain. The results show that the proposed algorithm improves the performance of the unified Dutch and German speech recognition system and outperforms a baseline system trained with language-dependent phoneme models.,1
331,"this paper describes our recent work in developing an unified dutch and german speech recognition system in the speechdat domain . the acoustic component of the unified dutch and german speech recognition system is accomplished through sharing common phonemes without preserving any information about the languages . we propose a more robust mce-based training algorithm , where only the language dependent phoneme models are allowed to be adjusted , according to the type of training data . experimental results on dutch and german subword recognition tasks clearly show an overall string error rate reduction of about 7 % and 13 % obtained by the newly trained unified dutch and german speech recognition system in comparison with the conventional mce-trained multilingual system .",0
332,"This paper introduces a low-complexity compressive spectral embedding algorithm as an alternative to the Singular Value Decomposition (SVD) for spectral embedding. The proposed algorithm constructs a matrix of pairwise similarity metrics and computes a partial SVD that extracts the dominant singular vectors. Instead of computing the full SVD, the algorithm uses finite order polynomial expansions to approximate the embedding coordinates. The algorithm is shown to have superior time complexity compared to the SVD-based embeddings while producing comparable results. Furthermore, the compressive spectral embedding algorithm is evaluated on various network datasets, demonstrating its effectiveness in downstream inference tasks such as matrix embedding and clustering. The paper concludes by highlighting the potential of the proposed algorithm in learning tasks that involve arbitrary vectors, problem size, and problem structure.",1
333,"spectral embedding based on the singular value decomposition is a widely used '' preprocessing '' step in many learning tasks , typically leading to di-mensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes -lrb- by a predefined function of the singular value -rrb- . however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck . in this paper , we propose a low-complexity compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding . for an m × n matrix with t non-zeros , its time complexity is o -lrb- -lrb- t + m + n -rrb- log -lrb- m + n -rrb- -rrb- , and the embedding dimension is o -lrb- log -lrb- m + n -rrb- -rrb- , both of which are independent of the number of singular vectors whose effect we wish to capture . to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings . the key to sidestepping the singular value decomposition is the observation that , for downstream inference tasks such as clustering and classification , we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the ℓ 2-norm , rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial singular value decomposition tries to do . our numerical results on network datasets demonstrate the efficacy of the proposed low-complexity compressive spectral embedding algorithm , and motivate further exploration of its application to downstream inference tasks .",0
334,"This paper proposes a data-driven technique for optimizing acoustic models in commercial speech recognition systems. The technique involves using foreground scores and data weighting to improve the modeling of poorly represented data and reduce recognition errors. The authors show that score-based optimization using frame-averaged foreground log-likelihoods can lead to significant improvements in semantic error rates. Additionally, they demonstrate that data weighting can further improve recognition accuracy by assigning higher weights to more important data. The proposed approach is evaluated on a live evaluation set and is shown to be effective for optimizing acoustic models in commercial applications. Overall, the study highlights the importance of priors and data-driven techniques in improving the accuracy of speech recognition systems.",1
335,this paper describes a data-driven technique for optimizing the acoustic models for speech recognition systems that target commercial applications over telephones . frame-averaged foreground log-likelihoods -lrb- foreground scores -rrb- correlate to recognition errors . these scores are used together with gender to optimize data weighting for the acoustic models . this process is interpreted as increasing the priors and associated parameters for poorly modeled data . the score-based optimization leads to about 7 % fewer semantic errors on a live evaluation set collected after the last data used to estimate the acoustic models .,0
336,"This paper proposes an approach for HMM-based singing voice synthesis by integrating speaker and pitch adaptive training. Singing voice synthesis systems based on statistical parametric approaches, such as HMMs, often suffer from the data sparseness problem. This work addresses the issue by incorporating contextual factors, such as singer-dependent and pitch-dependent information, and applying context-dependent HMMs and vibrato HMMs. The proposed approach involves singer-adaptive training and pitch-adaptive training, which effectively integrates the information from multiple singers and pitch ranges, respectively. The experimental results show that the integration of speaker and pitch adaptive training significantly improves the quality of the synthesized singing voices, as compared to the conventional methods that only consider one type of adaptation. The proposed approach has potential applications in various areas, such as speech and music synthesis.",1
337,"a statistical parametric approach to singing voice synthesis based on hidden markov models has been growing in popularity over the last few years . the spectrum , exci-tation , vibrato , and duration of the singing voice in this statistical parametric approach are simultaneously modeled with context-dependent hmms and waveforms are generated from the hmms themselves . since hmm-based singing voice synthesis systems are '' corpus-based , '' the hmms corresponding to contextual factors that rarely appear in the training data can not be well-trained . however , it may be difficult to prepare a large enough quantity of singing voice data sung by one singer . furthermore , the pitch included in each song is imbalanced , and there is the vocal range of the singer . in this paper , we propose '' singer adaptive training '' which can solve the data sparse-ness problem . experimental results demonstrated that the proposed statistical parametric approach improved the quality of the synthesized singing voices .",0
338,"This paper proposes a machine learning approach for inducing deterministic Prolog parsers from treebanks. The authors utilize inductive logic programming and statistical methods to generate deterministic Prolog rules from corpora of parsed sentences. The ATIS corpus is used as a case study to evaluate the effectiveness of this approach. The results show that the induced Prolog parsers achieve high accuracy and outperform existing statistical parsers. The approach can also be extended to other languages and domains with treebanks available. Overall, the study demonstrates the potential of machine learning methods in inducing Prolog rules for natural language parsing.",1
339,this paper presents a method for constructing de-terministic prolog parsers from corpora of parsed sentences . our approach uses recent machine learning methods for inducing prolog rules from examples -lrb- inductive logic programming -rrb- . we discuss several advantages of this method compared to recent statistical methods and present results on learning complete parsers from portions of the atis corpus .,0
340,"This paper proposes a method for inferring unseen views of people using a probabilistic tensor completion approach. The problem of unseen view synthesis is addressed by considering the 3D appearance of people as a low-dimensional latent factor tensor, and estimating the camera viewpoint using rough viewpoint information. Synthetic views of people are then inferred based on the estimated viewpoint, resulting in virtual views of the people from unseen viewpoints. The proposed method exhibits robustness in handling incomplete or missing data, and demonstrates promising results in inferring unseen views of people. The paper presents the approach in detail and provides experimental results to validate the proposed method.",1
341,"we pose unseen view synthesis as a probabilistic tensor completion problem . given images of people organized by their rough viewpoint , we form a 3d appearance tensor indexed by images -lrb- pose examples -rrb- , viewpoints , and image positions . after discovering the low-dimensional latent factors that approximate that tensor , we can impute its missing entries . in this way , we generate novel synthetic views of people -- even when they are observed from just one camera viewpoint . we show that the inferred views are both visually and quantitatively accurate . furthermore , we demonstrate their value for recognizing actions in unseen views and estimating viewpoint in novel images . while existing methods are often forced to choose between data that is either realistic or multi-view , our virtual views offer both , thereby allowing greater robustness to viewpoint in novel images .",0
342,"This paper presents a novel approach for the classification of tensors and fiber tracts using a kernel-based approach with Mercer kernels. The proposed method encodes both soft probabilistic spatial and diffusion information to classify healthy and diseased subjects. The approach involves using nonlinear kernel support vector machines for classification and atlas-based registration of diffusion-free images for clustering of diffusion tensors. The method also includes a landmark-isomap embedding for k-means clustering, and a multi-instance kernel for tensor kernel and tensor space representation. The proposed approach is evaluated on synthetic data and real-world diffusion tensor magnetic resonance imaging datasets, demonstrating its effectiveness in the classification of fibers and tensor segmentation. Results show that the proposed approach outperforms other state-of-the-art methods in terms of accuracy and robustness.",1
343,"in this paper , we present a kernel-based approach to the clustering of diffusion tensors and fiber tracts . we propose to use a mercer kernel over the tensor space where both spatial and diffusion information are taken into account . this mercer kernel highlights implicitly the connectivity along fiber tracts . tensor segmentation is performed using kernel-pca compounded with a landmark-isomap embedding and k-means clustering . based on a soft fiber representation , we extend the tensor kernel to deal with fiber tracts using the multi-instance kernel that reflects not only interactions between points along fiber tracts , but also the interactions between diffusion tensors . this kernel-based approach is further extended by way of an atlas-based registration of diffusion-free images , followed by a classification of fibers based on nonlinear kernel support vector machines . promising experimental results of tensor and fiber classification of the human skeletal muscle over a significant set of healthy and diseased subjects demonstrate the potential of our kernel-based approach .",0
344,This paper proposes a method for query spelling correction in web search engines using distributional similarity based models. The web query spelling correction task is addressed by exploring various models based on distributional similarity that can capture the similarity between a query term and its context. The models are trained on web search relevance and query logs data to learn the distributional similarity between the query terms. The distributional similarity models are then used to develop a query speller to suggest corrections for misspelled queries. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods in terms of query spelling correction accuracy. The study concludes that distributional similarity based models can be effective for improving the performance of search engines in handling query spelling correction.,1
345,"a query speller is crucial to search engine in improving web search relevance . this paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models . the key to our methods is the property of dis-tributional similarity between two terms : it is high between a frequently occurring misspelling and its correction , and low between two irrelevant terms only with similar spellings . we present two models that are able to take advantage of this property . experimental results demonstrate that the distributional similarity based models can significantly outper-form their baseline systems in the web query spelling correction task .",0
346,"This paper proposes a two-stage training algorithm with a heuristic line search strategy for L1-regularized Conditional Random Fields (CRFs), which is a sparse learning framework used in natural language processing tasks such as Chinese word segmentation and named entity recognition. L1 regularization is used to improve convergence and generalizability of the algorithm. The paper compares the proposed algorithm to a batch training algorithm and demonstrates that it achieves faster training times while maintaining accuracy. Additionally, the stochastic gradient descent method is used with a heuristic line search strategy for parameter tuning. The results show that the proposed algorithm outperforms other methods for L1-regularized CRFs, making it a promising approach for various natural language processing tasks.",1
347,"sparse learning framework , which is very popular in the field of nature language processing recently due to the advantages of efficiency and generalizability , can be applied to conditional random fields with l1 regularization method . stochastic gradient descent method has been used in training l1-regularized crfs , because stochastic gradient descent method often requires much less training time than the batch training algorithm like quasi-newton method in practice . nevertheless , stochastic gradient descent method sometimes fails to converge to the optimum , and stochastic gradient descent method can be very sensitive to the learning rate parameter settings . we present a two-stage training algorithm which guarantees the convergence , and use heuris-tic line search strategy to make the first stage of stochastic gradient descent method more robust and stable . experimental evaluations on chinese word segmentation and name entity recognition tasks demonstrate that our two-stage training algorithm can produce more accurate and compact model with less training time for l1 regularization .",0
348,"In this paper, we present a new approach for channel tracking in MIMO-OFDM communication systems over Gauss-Markov channels. The proposed approach utilizes pruning techniques to reduce the complexity of the channel tracking algorithm while maintaining the accuracy of the estimates. The performance of the proposed algorithm is evaluated in terms of bit error probability and reduced spectral efficiency, and compared to existing methods such as the Kalman filter and zero-forcing soft detector. Our results show that the proposed algorithm is able to effectively track the channel matrix taps even in the presence of mismodeling and error propagation effects. Additionally, the proposed algorithm is able to significantly reduce the computational complexity and improve the overall performance of the system in terms of detection of both correctly detected symbols and mis-detected symbols.",1
349,"in this paper we investigate the problem of channel tracking and detection for mimo-ofdm systems over fast varying channels obeying a gauss-markov model . we consider time domain tracking of the channel matrix taps with kalman ¿ l-ter , whereas symbols detection is carried out by a zero-forcing soft detector . a key assumption of the theory of kalman ¿ lter is that the gauss-markov model is perfectly known , while communication systems make use of the detected symbols as an input to the kalman ¿ lter in order to form a suitable gauss-markov model . this gives rise to error propagation due to mis-detected symbols -lrb- model mismatch -rrb- and is usually solved by using frequently inserted pilot symbols , resulting in a reduced spectral ef ¿ ciency . to overcome this problem , we suggest a novel approach to mitigate the error propagation due to mis-detections without using frequent pilot symbols . in particular , we consider the reliability of the detections based on the soft detector and use only those outputs that have robust reliability to track the channel matrix taps , minimizing the effect of kalman ¿ lter mismodeling . this method can signi ¿ cantly reduce the error propagation effect , leading to an improved bit error probability .",0
350,"This paper proposes a method for object category recognition based on scale-invariant shape features. The method uses convex local arrangements of contours and a class of distinguished regions to avoid spurious edge detections and to capture local interest points. A tangential-gradient energy term is used to enhance the local convexity of the detected regions, and a local support cost function is defined to measure the similarity between the detected regions and the class of distinguished regions. An entropy term is also included to take into account the angular positions of the tangential edges. The proposed method is designed to handle scale changes, rotations, occlusions, and clutter. Experimental results on gray-level images demonstrate the effectiveness of the proposed method in recognizing object categories. The method outperforms existing methods in terms of recognition accuracy and computational efficiency.",1
351,"we introduce a new class of distinguished regions based on detecting the most salient convex local arrangements of contours in the image . the regions are used in a similar way to the local interest points extracted from gray-level images , but they capture shape rather than texture . local convexity is characterized by measuring the extent to which the detected image contours support circle or arc-like local structures at each position and scale in the image . our class of distinguished regions combines two cost functions defined on the tangential edges near the circle : a tangential-gradient energy term , and an entropy term that ensures local support from a wide range of angular positions around the circle . the detected regions are invariant to scale changes and rotations , and robust against clutter , occlusions and spurious edge detections . experimental results show very good performance for both shape matching and recognition of object categories .",0
352,"This paper proposes a feature clustering method to accelerate parallel coordinate descent algorithms for solving large-scale 1-regularized loss minimization problems. Such problems arise in high-dimensional supervised learning, classification, and compressed sensing applications. The authors analyze the convergence of coordinate descent algorithms, including block-greedy and thread-greedy coordinate descent, and provide a unified convergence analysis. They show that feature clustering can speed up block-greedy algorithms without sacrificing convergence guarantees. The proposed method is evaluated on both synthetic and real-world datasets, including regression and classification problems, and achieves significant speedups without compromising accuracy. The authors also discuss the potential for applying their approach to other high-dimensional applications.",1
353,"large-scale 1-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learning , including classification and regression problems . high-performance algorithms and implementations are critical to efficiently solving these problems . building upon previous work on coordinate descent algorithms for 1-regularized problems , we introduce a novel family of algorithms called block-greedy coordinate descent that includes , as special cases , several existing algorithms such as scd , greedy cd , shotgun , and thread-greedy . we give a unified convergence analysis for the family of block-greedy algorithms . the analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small . our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications . we hope that algorithmic approaches and convergence analysis we provide will not only advance the field , but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale 1-regularization problems .",0
354,"This paper examines the acoustics of overlapping laughter in conversational speech, focusing on how it differs from non-overlapping laughter. The study explores the contagious nature of laughter and joint vocal action in spoken dialogue systems. The research measures the maximum intensity of laughter events and compares overlapping ones to non-overlapping ones. The findings suggest that the acoustics of overlapping laughter are different from those of non-overlapping laughs and are affected by group size and entrainment. Understanding the acoustics of overlapping laughter is an important step towards improving the design of spoken dialogue systems and enhancing our understanding of social dynamics in conversation.",1
355,"the social nature of laughter invites people to laugh together . this joint vocal action often results in overlapping laughter . in this paper , we show that the acoustics of overlapping laughs are different from non-overlapping laughs . we found that overlapping laughs are stronger prosodically marked than non-overlapping ones , in terms of higher values for duration , mean f0 , mean and maximum intensity , and the amount of voicing . this effect is intensified by the number of people joining in the laughter event , which suggests that entrainment is at work . we also found that group size affects the number of overlapping laughs which illustrates the contagious nature of laughter . finally , people appear to join laughter simultaneously at a delay of approximately 500 ms ; a delay that must be considered when developing spoken dialogue systems that are able to respond to users ' laughs .",0
356,"This paper proposes a filter-and-forward distributed beamforming strategy for relay networks operating in frequency selective fading channels. The objective is to maximize the destination quality-of-service constraint by optimizing the transmit relay power, given the half-duplex distributed beamforming technique and the relay transmitted power. The proposed approach utilizes finite impulse response filters and distributed beamforming techniques to overcome the limitations of frequency selective fading channels. Closed-form solutions are obtained for the filter-and-forward strategy, providing significant gains in network performance. The results show that the proposed approach outperforms the conventional amplify-and-forward approach, making it a promising solution for relay networks in practical scenarios.",1
357,"a half-duplex distributed beamforming technique for relay networks with frequency selective fading channels is developed . the network relays use the filter-and-forward strategy to compensate for the transmitter-to-relay and relay-to-destination channels using finite impulse response filters . with the channel state information -lrb- csi -rrb- being available at the receiver , the transmit relay power is minimized subject to the destination quality-of-service constraint . this distributed beamforming problem is shown to have a closed-form solution . simulation results demonstrate substantial improvements in terms of the relay transmitted power and feasibility of the destination qos constraint as compared to amplify-and-forward distributed beamforming techniques .",0
358,"This paper presents a method for automatic laughter segmentation in meetings. The proposed approach uses short-term features, including Mel-frequency cepstral coefficients (MFCCs), and high-level and long-term features. A hybrid MLP/HMM system is employed for frame-level detection of laughter, with Viterbi decoding used to determine the most likely sequence of laughter segments. The system is evaluated on the ICSI Meeting Recorder Corpus, achieving an equal error rate (EER) of 9.9%. The system also uses speaker recognition to improve detection accuracy. Median filtering is used to improve precision and recall rates, and the system achieves a median segmentation error rate of 1.7 seconds. The results demonstrate the effectiveness of the proposed approach for automatic laughter segmentation in meetings.",1
359,"our goal in this work was to develop an accurate method to identify laughter segments , ultimately for the purpose of speaker recognition . our previous work used mlps to perform frame level detection of laughter using short-term features , including mfccs and pitch , and achieved a 7.9 % eer on our test set . we improved upon our previous results by including high-level and long-term features , median filtering , and performing segmentation via a hybrid mlp/hmm system with viterbi decoding . upon including the long-term features and median filtering , our results improved to 5.4 % eer on our test set and 2.7 % eer on an equal-prior test set used by others . after attaining segmentation results by incorporating the hybrid mlp/hmm system and viterbi decoding , we had a 78.5 % precision rate and 85.3 % recall rate on our test set . to our knowledge these are the best known laughter detection results on the icsi meeting recorder corpus to date .",0
360,"This paper proposes a decomposable attention model for natural language inference, using the Stanford Natural Language Inference dataset. The model takes into account word-order information and utilizes an intra-sentence attention mechanism. The neural architecture of the proposed model is designed to be easily decomposable, allowing for better interpretability of the attention mechanism. Experimental results show that the proposed model outperforms previous state-of-the-art methods on the Stanford Natural Language Inference dataset, achieving an accuracy of 86.1%. This suggests that the proposed attention model can effectively capture important features for natural language inference tasks.",1
361,"we propose a simple neural architecture for natural language inference . our neural architecture uses attention to decompose the problem into subprob-lems that can be solved separately , thus making it trivially parallelizable . on the stanford natural language inference dataset , we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information . adding intra-sentence attention that takes a minimum amount of order into account yields further improvements .",0
362,"This paper discusses the use of accidental pinhole and pinspeck cameras to reveal the scene outside the picture. These cameras, created unintentionally through apertures in walls and other objects, capture images that can reveal information about the lighting conditions and shadows present in the scene. The authors use a reference image of the indoor scene and a video sequence of accidental '' images to extract information about the natural light in the scene. They show that this technique can be used to improve the quality of images captured in challenging lighting conditions. The study concludes that accidental pinhole and pinspeck cameras can be useful tools for revealing information about the scene outside the picture.",1
363,"we identify and study two types of '' accidental '' images that can be formed in scenes . the first is an accidental pin-hole camera image . these images are often mistaken for shadows , but can reveal structures outside a room , or the unseen shape of the light aperture into the room . the second class of accidental '' images are '' inverse '' pinhole camera images , formed by subtracting an image with a small occluder present from a reference image without the oc-cluder . the reference image can be an earlier frame of a video sequence . both types of accidental '' images happen in a variety of different situations -lrb- an indoor scene illuminated by natural light , a street with a person walking under the shadow of a building , etc. -rrb- . accidental cameras can reveal information about the scene outside the image , the lighting conditions , or the aperture by which light enters the scene .",0
364,"This paper presents a flow-based approach to detect vehicles and create a mosaic of the background in airborne video sequences. The method utilizes optical flow algorithms to compute dense residual flow between consecutive frames and estimates the gross affine background motion. An EM-based motion segmentation algorithm is then used to distinguish between the moving objects and the background layer. Finally, a background appearance model is created to stabilize the frames and detect the vehicles in the video stream. The proposed approach is demonstrated on several airborne video sequences, and the results show that it is capable of accurately detecting and tracking vehicles while creating a stable and coherent background mosaic.",1
365,"we address the detection of vehicles in a video stream obtained from a moving airborne platform . our approach is based on robust optical flow algorithm applied on stabilized frames . stabilization of the frames compensates for gross affine background motion prior to running robust optical flow to compute dense residual flow . based on the flow and the previous background appearance model , the new frame is separated into background and foreground oc-clusion layers using an em-based motion segmentation . the proposed framework shows that ground vehicles can be detected and segmented from airborne video sequences while building a mosaic of the background layer .",0
366,"This paper proposes a new method for improving the performance of model-order selection criteria by incorporating a partial-model selection searching method. Traditional model-order selection criteria use a nested full-parameters-set searching procedure, which can be inefficient for low signal-to-noise ratios. The proposed method involves selecting a subset of candidate models based on partial-model selection searches before conducting a full-model order selection. The method is evaluated using linear regression and bootstrap-based criteria and demonstrates improved accuracies compared to traditional methods. Overall, this approach provides an efficient and effective way to improve the performance of model-order selection criteria.",1
367,"the traditional partial-model selection searching method for model-order selection in linear regression is a nested full-parameters-set searching procedure over the desired orders , which we call full-model order selection . on the other hand , a method for model-selection searches for the best sub-model within each order . in this paper , we propose using the partial-model selection searching method for model-order selection , which we call full-model order selection . we show by simulations that the proposed partial-model selection searching method gives better accuracies than the traditional one , especially for low signal-to-noise ratios over a wide range of model-order selection criteria -lrb- both information theoretic-based and bootstrap-based -rrb- . also , we show that for some models the performance of the bootstrap-based criterion improves significantly by using the proposed partial-model selection searching method .",0
368,This paper presents a method for solving continuous state and action Markov Decision Processes (MDPs) using Symbolic Dynamic Programming (SDP). The proposed SDP solution works for multivariate continuous state and action spaces and is able to handle restricted piecewise quadratic rewards. The method is demonstrated on a didactic nonlinear planning example and shown to be effective for real-world decision-theoretic planning problems. The SDP approach uses a piecewise linear dynamics model and closed-form value function approximation to automate the exact solution. The paper also discusses the dynamic programming backup process and presents a policy for continuous action maximization step. The approach is evaluated on a variety of test problems and shows promising results for improving policy accuracies in continuous state and action MDPs.,1
369,"many real-world decision-theoretic planning problems are naturally modeled using both continuous state and action spaces , yet little work has provided exact solutions for the case of continuous actions . in this work , we propose a symbolic dynamic programming solution to obtain the optimal closed-form value function and policy for csa-mdps with mul-tivariate continuous state and actions , discrete noise , piecewise linear dynamics , and piecewise linear -lrb- or restricted piecewise quadratic -rrb- reward . our key contribution over previous sdp work is to show how the continuous action maximization step in the dynamic programming backup can be evaluated optimally and symbolically -- a task which amounts to symbolic constrained optimization subject to unknown state parameters ; we further integrate this technique to work with an efficient and compact data structure for sdp -- the extended algebraic decision diagram -lrb- xadd -rrb- . we demonstrate empirical results on a didactic nonlinear planning example and two domains from operations research to show the first automated exact solution to these problems .",0
370,"This paper presents a method for commonsense knowledge base completion using neural network models. The approach uses a bilinear model and additive architecture to predict the missing relations in a knowledge base. The method is evaluated on two knowledge bases, Freebase and ConceptNet, using true held-out tuples and medium-confidence tuples. The results show that the proposed method outperforms existing methods on both datasets, demonstrating its effectiveness in completing missing knowledge. The paper concludes that this approach provides a promising direction for future research in the field of knowledge base completion.",1
371,"we enrich a curated resource of common-sense knowledge by formulating the problem as one of knowledge base completion . most work in knowledge base completion focuses on knowledge bases like freebase that relate entities drawn from a fixed set . however , the tuples in conceptnet -lrb- speer and havasi , 2012 -rrb- define relations between an unbounded set of phrases . we develop neural network models for scoring tuples on arbitrary phrases and evaluate neural network models by their ability to distinguish true held-out tuples from false ones . we find strong performance from a bilinear model using a simple additive architecture to bilinear model phrases . we manually evaluate our trained bilinear model 's ability to assign quality scores to novel tuples , finding that bilinear model can propose tu-ples at the same quality level as medium-confidence tuples from conceptnet .",0
372,"Here's an abstract based on the given information:

This paper presents a novel approach to imaging concert hall acoustics using visual and audio cameras. By combining panoramic mosaiced visual images of the space with audio and video images captured by a real-time audio camera, we are able to create detailed acoustic intensity images that can be used to analyze the acoustics of rooms and halls. We use a spherical microphone array beamformer to capture audio and apply computer vision techniques to extract acoustical features from both the visual and audio data. Our method also includes central projection registration to ensure accurate alignment between the two data streams. Experimental results demonstrate the effectiveness of our approach in imaging concert hall acoustics.",1
373,"using a recently developed real time audio camera , that uses the output of a spherical microphone array beamformer steered in all directions to create central projection to create acoustic intensity images , we present a technique to measure the acoustics of rooms and halls . a panoramic mosaiced visual image of the space is also create . since both the visual and the audio camera images are central projection , registration of the acquired audio and video images can be performed using standard computer vision techniques . we describe the technique , and apply it to the examine the relation between acoustical features and architectural details of the dekelbaum concert hall at the clarice smith performing arts center in college park , md. .",0
374,"This paper presents non-minimum phase inverse filter methods for immersive audio rendering. The authors discuss signal processing considerations for spatial sound rendering in augmented and virtual reality, as well as for virtual sound sources in manufacturing and entertainment applications. The paper explores the use of synthetic head-related transfer functions and spectral characteristics in immersive audio systems, including their application in air traffic control and distance learning pilot warning systems. The authors also consider the use of teleconferencing and displays in conjunction with these technologies. Overall, this work provides insight into the challenges and opportunities associated with non-minimum phase inverse filter methods in immersive audio systems.",1
375,"immersive audio immersive audio systems are being envisioned for applications that include teleconferencing and telepresence ; augmented and virtual reality for manufacturing and entertainment ; air traffic control , pilot warning , and guidance systems ; displays for the visually-impaired ; distance learning ; and professional sound and picture editing for television and film . the principal function of such immersive audio systems is to synthesize , manipulate and render sound fields in real time . in this paper we examine several signal processing considerations in spatial sound rendering over loudspeakers . we propose two methods that can be used to implement the necessary filters for generating virtual sound sources based on synthetic head-related transfer functions with the same spectral characteristics as those of the real source .",0
376,"This paper proposes an iterative reinforcement approach for simultaneous document summarization and keyword extraction. The proposed approach leverages a knowledge-based approach to compute word semantics and generates both summary and keywords for a single document. The iterative reinforcement approach uses rewards and punishments to iteratively improve the quality of the output summary and keywords. The approach is compared with the knowledge-based approach, demonstrating its effectiveness in generating high-quality summaries and keywords. The proposed approach has the potential to enhance the efficiency and accuracy of text summarization systems in various applications.",1
377,"though both document summarization and keyword extraction aim to extract concise representations from documents , these two tasks have usually been investigated independently . this paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document under the assumption that the summary and keywords of a document can be mutually boosted . the iterative reinforcement approach can naturally make full use of the reinforcement between sentences and keywords by fusing three kinds of relationships between sentences and words , either homogeneous or heterogeneous . experimental results show the effectiveness of the proposed iterative reinforcement approach for both tasks . the iterative reinforcement approach is validated to work almost as well as the knowledge-based approach for computing word semantics .",0
378,"This paper investigates brain connectivity patterns during motor imagery for Brain-Computer Interfaces (BCIs). Motor imagery EEG data was analyzed to extract connectivity measures, including transfer entropy and beamforming, and bandpower features in high-frequency bands. Connectivity features were used to build a feature space for motor imagery classification. The study also explored how cognitive demands modulate the γ-band connectivity patterns during motor imagery. Results showed that connectivity-based BCIs using transfer entropy and beamforming outperformed BCIs using bandpower features. The study provides insights into the underlying brain mechanisms during motor imagery, which can be useful in developing effective BCIs.",1
379,"eeg connectivity measures could provide a new type of feature space for inferring a subject 's intention in brain-computer interfaces . however , very little is known on eeg connectivity patterns for brain-computer interfaces . in this study , eeg connectivity measures during motor imagery of the left and right is investigated in a broad frequency range across the whole scalp by combining beamforming with transfer entropy and taking into account possible volume conduction effects . observed connec-tivity patterns indicate that modulation intentionally induced by motor imagery is strongest in the γ-band , i.e. , above 35 hz . furthermore , modulation between motor imagery and rest is found to be more pronounced than between motor imagery of different hands . this is in contrast to results on motor imagery obtained with bandpower features , and might provide an explanation for the so far only moderate success of connectivity features in brain-computer interfaces . it is concluded that future studies on connectivity based bcis should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions .",0
380,"This paper proposes a novel approach for improving symbolic bidirectional search by introducing abstraction heuristics. The approach uses abstract state spaces and/or partial and perimeter abstractions to construct heuristic functions that guide the search process. Specifically, the authors introduce a symbolic bidirectional uniform-cost search called Symba*, which incorporates the abstraction heuristics. They show that the heuristics significantly improve the performance of Symba* compared to other state-of-the-art cost-optimal planning algorithms. The authors also discuss the use of the approach in different domains, such as the International Planning Competition (IPC), demonstrating its effectiveness in various scenarios. Overall, the results indicate that the abstraction heuristics are a promising direction for improving symbolic bidirectional search.",1
381,"symbolic bidirectional uniform-cost search is a prominent technique for cost-optimal planning . thus , the question whether it can be further improved by making use of heuristic functions raises naturally . however , the use of heuristics in bidi-rectional search does not always improve its performance . we propose a novel way to use abstraction heuristics in symbolic bidirectional search in which the search only resorts to heuristics when it becomes unfeasible . we adapt the definition of partial and perimeter abstractions to bidirectional search , where a ⇤ is used to traverse the abstract state spaces and/or generate the perimeter . the results show that abstraction heuristics can further improve symbolic bidirectional search in some domains . in fact , the resulting planner , symba ⇤ , was the winner of the optimal-track of the last ipc .",0
382,"This paper presents a computational model for unsupervised word discovery based on acoustic similarity and temporal sequence learning. The model aims to discover word-like fragments from raw speech signals without an upfront defined lexicon. The model incorporates acoustic phone models and clustering to discover patterns in the speech signal. Modelling constraints inspired by language acquisition are used to guide the unsupervised algorithm towards plausible word-like fragments. The model's performance is evaluated against mainstream ASR approaches, demonstrating promising results for unsupervised word discovery. This research contributes to the development of unsupervised approaches to ASR and provides insights into language acquisition mechanisms.",1
383,"we present an unsupervised algorithm for the discovery of words and word-like fragments from the speech signal , without using an upfront defined lexicon or acoustic phone models . the unsupervised algorithm is based on a combination of acoustic pattern discovery , clustering , and temporal sequence learning . unsupervised algorithm exploits the acoustic similarity between multiple acoustic tokens of the same words or word-like fragments . in its current form , the unsupervised algorithm is able to discover words in speech with low per-plexity -lrb- connected digits -rrb- . although its performance still falls off compared to mainstream asr approaches , the value of the unsupervised algorithm is its potential to serve as a computational model in two research directions . first , the unsupervised algorithm may lead to an approach for speech recognition that is fundamentally liberated from the modelling constraints in conventional asr . second , the proposed unsupervised algorithm can be interpreted as a computational model of language acquisition that takes actual speech as input and is able to find words as 'em ergent ' properties from raw input .",0
384,"The task of reordering phrases in machine translation is crucial to ensure high-quality translations. In this paper, we propose a novel approach for reordering grammar induction, which aims to predict the target order of words based on non-ITG reordering patterns. Our method utilizes word-aligned parallel corpora and linguistic annotation to learn a transduction function that maps source sentences to their reordered counterparts. We introduce a latent variable model that captures the hierarchical structure of reordering rules, represented as permutation trees. Our experiments on English-Japanese translation show that our approach outperforms preordering baselines in terms of accuracy and translation quality, highlighting the effectiveness of incorporating reordering grammar into phrase-based machine translation.",1
385,"we present a novel approach for unsu-pervised induction of a reordering grammar using a modified form of permutation trees -lrb- zhang and gildea , 2007 -rrb- , which we apply to preordering in phrase-based machine translation . unlike previous approaches , we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora . furthermore , our model -lrb- 1 -rrb- handles non-itg reordering patterns -lrb- up to 5-ary branching -rrb- , -lrb- 2 -rrb- is learned from all derivations by treating not only labeling but also bracketing as latent variable , -lrb- 3 -rrb- is entirely unlexicalized at the level of reordering rules , and -lrb- 4 -rrb- requires no linguistic annotation . our model is evaluated both for accuracy in predicting target order , and for its impact on translation quality . we report significant performance gains over phrase reordering , and over two known preordering baselines for english-japanese .",0
386,"This paper presents a study on speech recognition of Japanese news commentary using linguistic and acoustic features. The research focuses on developing efficient models for language model adaptation and improving word recognition accuracy through the use of crossword triphone models. The study analyzes various aspects of news commentaries, such as speech rate and rules, to develop a decoder that can accurately recognize word sequences. The results show that the proposed approach significantly reduces the word error rate and improves the accuracy of speech recognition of news commentary in comparison to read speech. The findings of this study can contribute to the development of more effective speech recognition systems for news programs.",1
387,"this paper describes some improvements in speech recognition of broadcast news commentary in japanese . since news commentary speech has different linguistic and acoustic features from read speech , it gives lower word recognition accuracy . in this paper we apply to news manuscripts some rules which represent the linguistic features of news commentaries , and generate word sequences for language model adaptation . we also use a large volume of transcriptions of news programs as training texts . acoustic models are speaker-adapted and their structures are changed so as to recognize relatively short phonemes , because we found the speech rate of news commentary is sometimes much faster than that of read speech . furthermore , by using a decoder that can handle crossword triphone models , we reduced the word error rate by 32 % .",0
388,"This paper proposes a method for the automatic extraction of speech lip features using a 2-D discrete cosine transform (DCT). The method involves transforming lip images into DCT coefficients, which are then subjected to principal component analysis (PCA) to extract geometric lip features. The resulting features are used to estimate natural lip aperture features, which can be used in visual speech processing. The accuracy of the proposed method is evaluated and compared to other lip feature extraction methods. The results demonstrate that the proposed method is effective in estimating lip features with high accuracy.",1
389,this study is a contribution to the field of visual speech processing . it focuses on the automatic extraction of speech lip features from natural lips . the method is based on the direct prediction of these features from predictors derived from an adequate transformation of the pixels of the lip region of interest . the transformation is made of a 2-d discrete cosine transform combined with a principal component analysis applied to a subset of the dct coefficients corresponding to about 1 % of the total dcts . the results show the possibility to estimate the geometric lip features with a good accuracy -lrb- a root mean square of 1 to 1.4 mm for the lip aperture and the lip width -rrb- using a reduce set of predictors derived from the pca .,0
390,"This paper proposes a supervised descriptor learning algorithm for multi-output regression tasks. The algorithm is based on a descriptor learning framework that provides a low-dimensional space representation of raw features, allowing for the estimation of multivariate targets. The framework incorporates supervised manifold regularization, generalized low-rank approximations of matrices, and discriminative and compact feature representation. The performance of the algorithm is evaluated on the benchmark pointing '04 dataset, a representative multi-output regression task, and head pose estimation. The results show that the proposed algorithm significantly reduces the ambiguity of the regression and classification tasks, leading to improved accuracy and error reduction. The proposed approach is applicable in computer vision applications where the supervised estimation of multivariate targets is required.",1
391,"descriptor learning has recently drawn increasing attention in computer vision , existing algorithms are mainly developed for classification rather than for regression which however has recently emerged as a powerful tool to solve a broad range of problems , e.g. , head pose estimation . in this paper , we propose a novel supervised descriptor learning algorithm to establish a discriminative and compact feature representation for multi-output regression . by formulating as generalized low-rank approximations of matrices with a supervised manifold regularization , the supervised descriptor learning algorithm removes irrelevant and redundant information from raw features by transforming into a low-dimensional space under the supervision of multivariate targets . the obtained discriminative while compact descriptor largely reduces the variability and ambiguity in multi-output regression , and therefore enables more accurate and efficient multivariate estimation . we demonstrate the effectiveness of the proposed supervised descriptor learning algorithm on a representative multi-output regression task : head pose estimation using the benchmark pointing '04 dataset . experimental results show that the supervised descriptor learning algorithm can achieve high pose estimation accuracy and significantly outperforms state-of-the-art algorithms by an error reduction up to 27.5 % . the proposed supervised descriptor learning algorithm provides a general descriptor learning framework in a supervised way for multi-output regression which can largely boost the performance of existing multi-output regression tasks .",0
392,This paper presents a novel approach for tracking multiple birdsongs using a combination of distribution derivative method and Markov renewal process clustering. The proposed method improves the accuracy of simultaneous bird sound tracking by incorporating rapid pitch modulations and automatic recognition of vocalization patterns. The approach utilizes a spectrogram representation of the birdsongs and a segregation algorithm to detect and track individual bird sounds. The Markov renewal process model is employed to cluster vocalizations and the distribution derivative method is used to estimate the rapid pitch modulations. The experimental results demonstrate that the proposed method outperforms existing methods in terms of tracking accuracy and efficiency.,1
393,"segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task . however , birdsong often contains rapid pitch modulations , and these rapid pitch modulations carry information which may be of use in automatic recognition . in this paper we demonstrate that an improved spec-trogram representation , based on the distribution derivative method , leads to improved performance of a segregation algorithm which uses a markov renewal process model to track vocalisation patterns consisting of singing and silences .",0
394,"This paper proposes spectral-envelope and group-delay models for analyzing transient signals, and applies them to the study of castanets and stop consonants. The authors use spectral-domain amplitude-modulated/frequency-modulated functions and group delay functions to represent the additive noise in the signals. They also analyze the modulation structure of the Fourier spectrum, spectral FM, spectral AM, and spectral zero-crossings envelope to study the transients. The paper presents the coherent demodulator and the Fourier spectrum as techniques to estimate the spectral envelope and group delay, respectively. The results suggest that the proposed models are effective in analyzing transient signals and can be applied to the study of other percussive instruments and speech sounds.",1
395,"we present a novel approach to represent transients using spectral-domain amplitude-modulated/frequency-modulated functions . the model is applied to the real and imaginary parts of the fourier transform -lrb- ft -rrb- of the transient . the suitability of the model lies in the observation that since transients are well-localized in time , the real and imaginary parts of the fourier spectrum have a modulation structure . the spectral am is the envelope and the spectral fm is the group delay function . the group delay is estimated using spectral zero-crossings and the spectral envelope is estimated using a coherent demodulator . we show that the proposed technique is robust to additive noise . we present applications of the proposed technique to castanets and stop-consonants in speech .",0
396,"This paper presents a new approach called GiSS, which combines Gibbs Sampling and SampleSearch techniques for inference in mixed probabilistic and deterministic graphical models. The GiSS method overcomes the limitations of existing methods by using both importance sampling and approximate weighting schemes. It uses hard deterministic spaces to generate unweighted samples and then applies MCMC techniques such as Gibbs Sampling and SampleSearch to improve the inference accuracy. Real-world applications of GiSS in SAT/CSP solvers demonstrate its effectiveness in handling mixed probabilistic and deterministic models. The paper also discusses the relationship between GiSS and other inference methods such as belief propagation and MC-SAT. The experimental results show that GiSS outperforms existing methods in terms of accuracy and determinism.",1
397,"mixed probabilistic and deterministic graphical models are ubiquitous in real-world applications . unfortunately , gibbs sampling , a popular mcmc technique , does not converge to the correct answers in presence of determinism and therefore can not be used for inference in such models . in this paper , we propose to remedy this problem by combining gibbs sampling with samplesearch , an advanced importance sampling technique which leverages complete sat/csp solvers to generate high quality samples from hard deterministic spaces . we call the resulting algorithm , samplesearch . unlike gibbs sampling which yields unweighted samples , samplesearch yields weighted samples . computing these weights exactly can be computationally expensive and therefore we propose several approximations . we show that our approximate weighting schemes yield consistent estimates and demonstrate experimentally that samplesearch is competitive in terms of accuracy with state-of-the-art algorithms such as samplesearch , mc-sat and belief propagation .",0
398,"This paper proposes a novel scene representation framework called ""Shape Recipes,"" which refers to the input image and provides a high-dimensional representation of the scene. The approach combines low-dimensional shape estimates with high-dimensional regression coefficients to represent complex scene configurations. The recipe is created by bandpass filtering the input image and using observed image data to derive a low-level vision representation. Stereo shape estimates are used to provide an accurate representation of the scene geometry, while material segmentation is used to classify materials in the scene. The paper demonstrates that shape recipes provide an effective way to represent real-world scenes and outperform existing approaches in terms of accuracy.",1
399,"the goal of low-level vision is to estimate an underlying scene , given an observed image . real-world scenes -lrb- eg , albedos or shapes -rrb- can be very complex , conventionally requiring high dimensional representations which are hard to estimate and store . we propose a low-dimensional representation , called a scene recipe , that relies on the image itself to describe the complex scene configurations . shape recipes are an example : these are the regression coefficients that predict the bandpassed shape from image data . we describe the benefits of this low-dimensional representation , and show two uses illustrating their properties : -lrb- 1 -rrb- we improve stereo shape estimates by learning shape recipes at low resolution and applying stereo shape estimates at full resolution ; -lrb- 2 -rrb- shape recipes implicitly contain information about lighting and materials and we use stereo shape estimates for material segmentation .",0
400,"This paper presents a novel nonlinear second-order digital oscillator designed for simulating virtual acoustic feedback. The oscillator is used to model the effect of feedback in musical instruments, particularly in guitar feedback effect. The proposed technique utilizes computational techniques to create a virtual instrument that can generate a sound palette with pitched real-time input. The aim of this study is to improve the realism of virtual acoustics and to minimize howling, a common problem in acoustic feedback. The proposed oscillator is evaluated on its effectiveness in generating realistic feedback sounds, and the results demonstrate its potential as a powerful tool for musicians and sound engineers.",1
401,"the guitar feedback effect , or howling , is well known to the general public and identified with many rock music genres and it is the only case of acoustic feedback employed for musical purposes . virtual acoustic feedback , is regarded as the extension of this phenomenon to any instrument or sound source by means of virtual acoustics and is meant to enrich the sound palette of a musician . the study of the acoustic feedback as a musical tool and computational techniques for its emulation have been scarcely addressed in literature . in this paper a nonlinear feedback oscillator is proposed and its properties derived . the nonlinear feedback oscillator does not necessarily need to be connected to a virtual instrument , thus enables to process any kind of pitched real-time input .",0
402,"This paper presents a method for domain adaptation of dependency parsing in natural language processing applications. The focus is on learning the reliability of parses in a new domain, using the Conll 2007 shared task dataset. Paraphrase acquisition and relation extraction are also explored. The accuracy of the parsing is evaluated to demonstrate the effectiveness of the proposed method. The results show that the method improves the parsing accuracy and can be useful for natural language processing applications.",1
403,"the accuracy of parsing has exceeded 90 % recently , but this is not high enough to use parsing results practically in natural language processing applications such as paraphrase acquisition and relation extraction . we present a method for detecting reliable parses out of the outputs of a single dependency parser . this technique is also applied to domain adaptation of dependency parsing . our goal was to improve the performance of a state-of-the-art dependency parser on the data set of the domain adaptation track of the conll 2007 shared task , a formidable challenge .",0
404,"This paper proposes a novel approach to compressing convolutional neural networks (CNNs) called CNNpack, which uses discrete cosine transform (DCT) bases to pack CNNs in the frequency domain. The goal of this approach is to reduce the storage and computational requirements of CNNs, making them more practical for deployment on mobile devices. The proposed approach achieves high compression and speed-up ratios while maintaining high accuracy on benchmark image datasets. The paper describes the CNNpack compression approach, including the use of cluster centers for frequency coefficients, and frequency domain convolution operations. The results demonstrate that CNNpack can achieve high compression rates with only a modest loss in accuracy, making it a promising approach for compressing and speeding up CNNs for use on low-energy mobile devices.",1
405,"deep convolutional neural networks -lrb- cnns -rrb- are successfully used in a number of applications . however , their storage and computational requirements have largely prevented their widespread use on mobile devices . here we present an effective cnn compression approach in the frequency domain , which focuses not only on smaller weights but on all the weights and their underlying connections . by treating convolutional filters as images , we decompose their representations in the frequency domain as common parts -lrb- i.e. , cluster centers -rrb- shared by other similar filters and their individual private parts -lrb- i.e. , individual residuals -rrb- . a large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy . we relax the computational burden of convolution operations in cnns by linearly combining the convolution responses of discrete cosine transform bases . the compression and speed-up ratios of the proposed cnn compression approach are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods .",0
406,"This paper proposes a method to resolve inter-frame interference in a transmit-reference ultra-wideband communication system. Transmit-reference ultra-wideband systems suffer from inter-frame interference due to overlapping of frames. The proposed method utilizes signal processing and a data model to remove the interference. The method addresses the issue of channel length and receiver algorithms, and reduces the complexity of the transmitter. The study highlights the importance of frame length and its impact on the performance of the system, while avoiding restrictive assumptions. The proposed method shows improved performance compared to existing techniques, demonstrating its potential for practical implementation in real-world applications.",1
407,"-- transmit-reference ultra-wideband systems are attractive due to their relatively low complexity at both the transmitter and the receiver . partly , this is achieved by making restrictive assumptions such as a frame length which should be much larger than the channel length . this limits their use to low data rate applications . in this paper , we lift this restriction and allow inter-frame interference to occur . we propose a suitable signal processing data model and corresponding receiver algorithms which take the inter-frame interference into account . the performance of the signal processing data model are verified using simulations .",0
408,"This paper proposes an assessment tool for capturing difficulty expressions in online Q&A discussions among students. The tool is based on a speech act framework that enables the identification of patterns in online dialogues. Specifically, the framework allows for the detection of high certainty expressions and emotional expressions in the discussions. The authors develop a classification system to categorize the difficulty expressions, which can help teachers identify areas where students are struggling and provide targeted support. The proposed tool was tested on student discussions from a pedagogical assessment platform and the results showed that it was effective in capturing difficulty expressions.",1
409,"we introduce a new application of online dialogue analysis : supporting pedagogical assessment of online q&a discussions . extending the existing speech act framework , we capture common emotional expressions that often appear in student discussions , such as frustration and degree of certainty , and present a viable approach for the classification . we demonstrate how such dialogue information can be used in analyzing student discussions and identifying difficulties . in particular , the difficulty expressions are aligned to discussion patterns and student performance . we found that frustration occurs more frequently in longer discussions . the students who frequently express frustration tend to get lower grades than others . on the other hand , frequency of high certainty expressions is positively correlated with the performance . we expect such dialogue analyses can become a powerful assessment tool for instructors and education researchers .",0
410,"This paper presents a new approach to translation using both source constituency and dependency trees. The method utilizes the phrasal nodes of constituency trees and the head-dependents relations of dependency trees to generate translation rules. The rules are evaluated using Chinese-English NIST test sets and the BLEU metric. Results show that the translation model using both types of trees outperforms a hierarchical phrase-based model, demonstrating the effectiveness of this approach in improving translation accuracy.",1
411,"we present a novel translation model , which simultaneously exploits the constituency and dependency trees on the source side , to combine the advantages of two types of trees . we take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules , and the target side as strings . our rules hold the property of long distance reorderings and the compatibility with phrases . large-scale experimental results show that our translation model achieves significantly improvements over the constituency-to-string -lrb- +2.45 bleu on average -rrb- and dependency-to-string -lrb- +0.91 bleu on average -rrb- models , which only employ single type of trees , and significantly outperforms the state-of-the-art hierarchical phrase-based model -lrb- +1.12 bleu on average -rrb- , on three chinese-english nist test sets .",0
412,"This paper presents an automatic classification approach for environmental noise events based on hidden Markov models (HMMs). The proposed approach uses the average spectrum of noise events as features and employs time-frequency analysis to generate observations for HMMs. The system is evaluated on a dataset of noise events from various sources such as trucks, aircraft, and mopeds. The results show that the HMM-based approach outperforms classifiers based on acoustic signatures and achieves comparable results to human listeners. The paper also discusses the potential applications of the proposed system in noise monitoring systems.",1
413,"the automatic classification of environmental noise sources from their acoustic signatures recorded at the microphone of a noise monitoring system -lrb- nms -rrb- is an active subject of research nowadays . this paper shows how hidden markov models -lrb- hmm 's -rrb- can be used to build an environmental noise recognition system based on a time-frequency analysis of the noise signal . the performance of the proposed hmm-based approach is evaluated experimentally for the classification of five types of noise events -lrb- car , truck , moped , aircraft , train -rrb- . the hmm-based approach is found to outperform previously proposed classifiers based on the average spectrum of noise event with more than 95 % of correct classifications . for comparison , a classification test is performed with human listeners for the same data which shows that the best hmm-based approach outper-forms the '' average '' human listener who achieves only 91.8 % of correct classification for the same task .",0
414,"This paper proposes an acoustic model for phoneme recognition that integrates articulatory features using Kullback-Leibler divergence based Hidden Markov Model (HMM) system. The model utilizes posterior probabilities of phonemes and observation features, which are extracted from the TIMIT phoneme recognition task. The HMM-based ASR system is used to estimate phoneme probabilities, and the observation features are evaluated using multilayer perceptrons. The articulatory features are shown to improve phoneme recognition accuracy, as evaluated on the TIMIT phoneme recognition task. The posterior probabilities of the articulatory features are used in conjunction with the phoneme probabilities to improve the recognition accuracy. The results demonstrate the effectiveness of using Kullback-Leibler divergence based HMM system and articulatory features for phoneme recognition.",1
415,"in this paper , we propose a novel framework to integrate artic-ulatory features into hmm-based asr system . this is achieved by using posterior probabilities of different afs -lrb- estimated by multilayer perceptrons -rrb- directly as observation features in kullback-leibler divergence based hmm system . on the timit phoneme recognition task , the proposed framework yields a phoneme recognition accuracy of 72.4 % which is comparable to kullback-leibler divergence based hmm system using posterior probabilities of phonemes as features -lrb- 72.7 % -rrb- . furthermore , a best performance of 73.5 % phoneme recognition accuracy is achieved by jointly modeling af probabilities and phoneme probabilities as features . this shows the efficacy and flexibility of the proposed approach .",0
416,"This paper presents a vector encoding scheme, called tree quantization, for large-scale similarity search and classification. Tree quantization involves coding high-dimensional vectors using a tree structure, and the resulting lossy compact codes can be optimized using integer programming-based optimization. The paper also introduces fast encoding techniques for tree quantization, which reduce the compression error while maintaining accuracy. The proposed method is evaluated on image classification tasks using Fisher vectors and neural codes, and the results show that tree quantization improves retrieval performance compared to other encoding schemes. Additionally, tree-based dynamic programming is used to optimize the codeword numbers for each level of the tree, further improving the performance of the encoding scheme. Overall, the paper demonstrates the effectiveness of tree quantization for large-scale similarity search and classification, and highlights the potential for further research in this area.",1
417,"we propose a new vector encoding scheme -lrb- tree quan-tization -rrb- that obtains lossy compact codes for high-dimensional vectors via tree-based dynamic programming . similarly to several previous schemes such as product quantization , these product quantization correspond to codeword numbers within multiple codebooks . we propose an integer programming-based optimization that jointly recovers the coding tree structure and the codebooks by minimizing the compression error on a training dataset . in the experiments with diverse visual descriptors -lrb- sift , neural codes , fisher vectors -rrb- , tree quantization is shown to combine fast encoding and state-of-the-art accuracy in terms of the compression error , the retrieval performance , and the image classification error .",0
418,"This paper proposes a speaker verification system that combines factor-analysis-based inter-session variability modeling and continuous progressive model adaptation. The system employs Gaussian Mixture Model (GMM) and Support Vector Machine (SVM) configurations and is evaluated on the NIST 2005 SRE corpus. The proposed method incorporates adaptive score normalization techniques to improve the performance of the system. The continuous progressive speaker model adaptation involves updating the GMM mean supervectors and the SVM-based classification in an iterative manner. The inter-session variability (ISV) modeling process is based on factor analysis, which is used to estimate the underlying speaker factors and their variations across sessions. Confidence measures are employed to determine the effectiveness of the model adaptation process. The results demonstrate that the proposed method outperforms other speaker verification systems in terms of accuracy, and the continuous model adaptation and score shift adaptation processes contribute to the improved performance. Overall, the paper highlights the potential of combining factor analysis and continuous progressive model adaptation for speaker verification.",1
419,this paper proposes a novel technique of incorporating factor-analysis-based inter-session variability modelling in speaker verification systems that employ continuous progressive speaker model adaptation . continuous model adaptation involves the use of all encountered trials in the adaptation process through the assignment of confidence measures . the proposed approach incorporates these confidence measures in the general statistics used in the isv modelling process . progressive svm-based classification was integrated into the system through the utilisation of gmm mean supervectors . the proposed system demonstrated a gain of 50 % over baseline results when trialled on the nist 2005 sre corpus . adaptative score normalisation techniques were found to be beneficial to both gmm and svm configurations alleviating the detrimental effects of score shift in progressive systems .,0
420,"This paper presents a robust speech representation method for the voiced parts of speech signal under high noise level conditions. The proposed method is based on synchrony determination with Phase-Locked Loops (PLLs), which allows for a spectral-like representation of the speech signal that is robust to noise. The method employs a band-pass filter bank and Mel cepstrum features to extract the frequency distribution of the speech signal. The synchrony-based spectrum is then used for noisy speech recognition, where the synchrony effects are modeled as a part of the auditory system. The proposed method is evaluated under different Signal-to-Noise Ratio (SNR) conditions, and the results demonstrate that the synchrony-based spectrum provides a more robust representation of the speech signal than other methods. The spectral-like representation is shown to improve recognition accuracy, and the synchrony effects are found to be an essential factor in achieving robustness to noise. Overall, the proposed method provides a promising approach to addressing the challenges of speech recognition under high noise level conditions.",1
421,"we propose to include synchrony effects , known to exist in the auditory system , to represent voiced parts of the speech signal in a robust way . the system decomposes the input signal by means of a band-pass filter bank , and utilizes a bank of phase locked loops -lrb- plls -rrb- to obtain information on the frequencies present at a specific time . this information about the frequency distribution is transformed into a spectral-like representation based on synchrony effects . noisy speech recognition experiments are performed using this synchrony-based spectrum , which is transformed into a small set of coefficients by using a transformation similar to that utilized for mel cepstrum features . we show that recognition performance compared to mel cepstrum features is advantageous , when measured over a range of snr conditions , especially in the high noise level case .",0
422,"This paper presents a Bayesian approach to the problem of localizing multiple targets. The proposed method provides a constant factor approximation guarantee for the problem, which makes it suitable for use in a noisy setting. The approach uses a dyadic policy to efficiently explore the posterior distribution of the targets' locations. The method is evaluated on both simulated data and real images of known distribution, where it is applied to the task of localizing multiple faces. The results show that the proposed method outperforms state-of-the-art methods in terms of accuracy and computational efficiency. The constant factor approximation guarantee ensures that the method is robust to noise and can provide reliable results even in challenging environments. Overall, the proposed Bayesian multiple target localization method provides a promising approach to addressing the problem of localizing multiple targets in a noisy setting.",1
423,"we consider the problem of quickly localizing multiple targets by asking questions of the form '' how many targets are within this set '' while obtaining noisy answers . this setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried . we assume that the targets are points on the real line , or in a two dimensional plane for the experiments , drawn independently from a known distribution . we evaluate the performance of a policy using the expected entropy of the posterior distribution after a fixed number of questions with noisy answers . we derive a lower bound for the value of this problem and study a specific policy , named the dyadic policy . we show that this policy achieves a value which is no more than twice this lower bound when answers are noise-free , and show a more general constant factor approximation guarantee for the noisy setting . we present an empirical evaluation of this policy on simulated data for the problem of detecting multiple instances of the same object in an image . finally , we present experiments on localizing multiple faces simultaneously on real images .",0
424,"This paper proposes a method for reconstructing the pose of a 3D shape using an uncalibrated computed tomography (CT) imaging device. The proposed method relies on the use of fiducials and CT scan intrinsic parameters, and it employs closed-form and numerical algorithms for estimation of error bounds. The method takes into account the scaling transformation and the freedom of rotation of the 3D shape, which is achieved through a geometric transformation and anisotropic scaling. The reconstruction is performed on cross-sectional images obtained from the CT scan. The proposed method is evaluated on both noise-free and noisy CT images, and the results demonstrate its effectiveness in accurately reconstructing the 3D pose of the shape.",1
425,"in this paper , we address the problem of precisely recovering the 3-d pose of 3-d shape fiducials from images obtained by means of an uncalibrated computed tomogra-phy imaging device . the main goal in this work is to model and estimate the geometric transformation relating line fiducials to their projections in cross-sectional images . to do so , we propose techniques which solve the points to lines correspondence using closed-form and numerical algorithms . a geometric transformation with eight degrees of freedom -lrb- rotation , translation and anisotropic scaling -rrb- is used to model both a rigid-body transformation and a scaling transformation accounting for ct scan intrinsic parameters . furthermore , an estimation of error bounds in space is given when image data are affected by noise . real experiments show that the proposed method provides good results on a set of ct images from many viewpoints .",0
426,"This paper addresses the challenge of feature adaptation for lip shape classification in the context of Cued Speech (CS) for the deaf. Specifically, the study focuses on the vowel case and proposes a lip flow modeling approach to adapt lip shape features from professional normal-hearing CS speakers to deaf CS speakers. The paper presents classification models for French vowels using both the original lip shape features and the adapted features. The results show that the lip flow modeling approach significantly improves the performance of the classification models for the deaf CS speakers, demonstrating the effectiveness of feature adaptation for improving speech perception for the hearing-impaired.",1
427,"the phonetic translation of cued speech -lrb- cs -rrb- gestures needs to mix the manual cs information together with the lips , taking into account the desynchronization delay -lrb- attina et al. -lsb- 2 -rsb- , aboutabit et al. -lsb- 4 -rsb- -rrb- between these two flows of information . this contribution focuses on the lip flow modeling in the case of french vowels . previously , classification models have been developed for a professional normal-hearing cs speaker -lrb- aboutabit et al. , -lsb- 7 -rsb- -rrb- . these classification models are used as a reference . in this study , we process the case of a deaf cs speaker and discuss the possibilities of classification . the best performance -lrb- 92.8 % -rrb- is obtained with the adaptation of the deaf data to the reference models .",0
428,"This paper proposes a possibilistic logic handling of preferences by introducing a logical representation of preferences in a weighted logical setting. The paper discusses the use of utility or value functions for aggregating preferences and the encoding of preferences in logical propositions. It highlights the importance of prioritized goals and reasoning purposes in decision-making processes. The paper emphasizes the expressivity of possibilistic logic in handling preferences and constraints. It presents different expression modes for encoding preferences and discusses their applicability. Overall, this paper provides a comprehensive framework for handling preferences in a logical setting, which could be useful in various decision-making applications.",1
429,"the classical way of encoding preferences in decision theory is by means of utility or value functions . however agents are not always able to deliver such a function directly . in this paper , we relate three different ways of specifying preferences , namely by means of a set of particular types of constraints on the utility function , by means of an ordered set of prioritized goals expressed by logical propositions , and by means of an ordered set of subsets of possible candidates reaching the same level of satisfaction . these different expression modes can be handled in a weighted logical setting , here the one of possibilistic logic . the aggregation of preferences pertaining to different criteria can then be handled by fusing sets of prioritized goals . apart from a better expressivity , the benefits of a logical representation of preferences are to put them in a suitable format for reasoning purposes , or for modifying or revising them .",0
430,"This paper presents a minimum Bayes error (MBE) feature selection method for continuous speech recognition. The proposed method aims to improve recognition accuracy by selecting a subset of features that maximally discriminates between speech classes. The method uses class densities estimated from training data and a linear transformation to project the features into a new space. The new feature set is then evaluated using the MBE criterion to select the optimal subset. Experimental results on a speech recognition task demonstrate that the MBE feature selection method outperforms other feature selection methods, and achieves a lower word error rate. This work shows that the use of MBE feature selection can lead to improved performance in continuous speech recognition.",1
431,"we consider the problem of designing a linear transformation such as to achieve minimum bayes error -lrb- or probability of misclassification -rrb- . two avenues will be explored : the first is to maximize the cents - average divergence between the class densities and the second is to minimize the union bhattacharyya bound in the range of cents . while both approaches yield similar performance in practice , they out-perform standard lda features and show a 10 % relative improvement in the word error rate over state-of-the-art cepstral features on a large vocabulary telephony speech recognition task .",0
432,"This paper presents a method for solving constrained estimation problems using a minimum mean-squared error (MMSE) estimator. The approach involves finding a restricted estimator that satisfies the constraints imposed on the problem, and then deriving a closed-form formula for the MMSE estimator based on this restricted estimator. The proposed method is applicable to a variety of fields in signal processing, where constrained estimation problems arise frequently. Results demonstrate that the proposed approach can achieve high accuracy and efficiency in solving such problems.",1
433,we address the problem of minimum mean-squared error estimation where the estimator is constrained to belong to a pre-deened set of functions . we derive a simple closed form formula that reveals the structure of the restricted estimator for a wide class of constraints . using this closed form formula we study various types of constrained estimation problems that arise commonly in the elds of signal processing and communication .,0
434,"This paper proposes a method for learning latent topic random fields using a taxonomy of labels. The approach involves a topic-dependent probabilistic classifier and a latent topic model, which jointly model both image labeling and context representation. The method is able to handle missing labels, coarsely labeled images, and real-world datasets with varying levels of specificity. The proposed approach is applied to images with roughly-specified object boundaries, where co-occurring patterns of image features are used to infer latent topics. Experimental results show that the proposed method outperforms several state-of-the-art methods in terms of accuracy, and can be used for a variety of computer vision applications.",1
435,"an important problem in image labeling concerns learning with images labeled at varying levels of specificity . we propose an approach that can incorporate images with labels drawn from a semantic hierarchy , and can also readily cope with missing labels , and roughly-specified object boundaries . we introduce a new form of latent topic model , learning a novel context representation in the joint label-and-image space by capturing co-occurring patterns within and between image features and object labels . given a topic , the latent topic model generates the input data , as well as a topic-dependent probabilistic classifier to predict labels for image regions . we present results on two real-world datasets , demonstrating significant improvements gained by including the coarsely labeled images .",0
436,"This paper discusses strategies to reduce the design time in multimodal/multilingual dialog applications. The paper proposes the use of semiautomatic generation of human-machine dialog systems, which reduces the need for speech and web designer's intervention. The proposed approach also includes a flow model to guide the interaction and avoid overanswering in dialogs, as well as a confirmation handling mechanism. The paper suggests that these strategies can improve the efficiency of the dialog applications and service data.",1
437,"in this paper , we present a complete platform for the semiautomatic generation of human-machine dialog systems , that using as input a description of the database of the service , a flow model with the different states of the final application and a guided interaction step by step with the designer 's intervention , generates dialogs to access the service data in different languages and two modalities , speech and web , simultaneously . we describe in detail several strategies that have been followed to reduce the time needed to do the design using the mentioned information . we also address important issues in dialog applications as mixed initiative and overanswering dialogs , confirmation handling and how to provide the user long lists of information .",0
438,"This paper presents a Bayesian model for simultaneous image clustering, annotation, and object segmentation. The model uses a non-parametric Bayesian approach with a logistic stick-breaking process to model the heterogeneous mix of components in the image. The model can handle spatially contiguous objects and various image classes and features. The model is inferred using variational Bayesian analysis, allowing for efficient computation and scalability to large image databases. Object-feature mixture models are used to learn the relationship between image features and object types. The proposed model offers a unified framework for image analysis, facilitating the clustering, annotation, and segmentation of objects in images. The model's performance is evaluated on several benchmark datasets, and the results demonstrate the effectiveness of the proposed approach.",1
439,"a non-parametric bayesian model is proposed for processing multiple images . the analysis employs image features and , when present , the words associated with accompanying annotations . the non-parametric bayesian model clusters the images into classes , and each image is segmented into a set of objects , also allowing the opportunity to assign a word to each object -lrb- localized labeling -rrb- . each object is assumed to be represented as a heterogeneous mix of components , with this realized via mixture models linking image features to object types . the number of image classes , number of object types , and the characteristics of the object-feature mixture models are inferred nonparametrically . to constitute spatially contiguous objects , a new logistic stick-breaking process is developed . inference is performed efficiently via variational bayesian analysis , with example results presented on two image databases .",0
440,"This paper presents an investigation into the Cramér-Rao bound for the estimation of noisy phase signals, which is an important problem in radar and communications. Specifically, the authors focus on noisy phase monocomponent signals and examine the maximum likelihood solution in the presence of additive noise. They derive the Cramér-Rao bound, which places a theoretical lower bound on the variance of unbiased estimators, and show that it is achievable under certain conditions. The paper discusses the practical implications of these results and their relevance to real-world applications.",1
441,"this paper deals with noisy phase monocomponent signals in additive noise . this model is more appropriate for real world applications in particular for radar and communications . the problem is introduced and a maximum likelihood solution is proposed . specifically , the cramèr-rao bound is explicitly derived and compared to the case of noise free phase .",0
442,"This paper proposes a method for joint motion estimation and clock synchronization in a wireless network of mobile nodes. The goal is to estimate the relative motion and radial velocities of the nodes, as well as their clock offsets and unknown clock skews. The method uses initial pairwise distances and a single clock reference to perform localization and synchronization. The Cramer-Rao bound is used to derive a maximum likelihood solution for the estimation problem. The proposed approach is evaluated using a static network of nodes and real-world data. Results demonstrate the effectiveness of the method in accurately estimating the motion and clock parameters of the network.",1
443,"localization and synchronization are critical challenges for a wireless network , which are conventionally solved independently . recently , various estimators have been proposed to jointly synchronize and localize a node in a static network based on two way communication . in this paper , we present a novel and generic model based on two way communication between nodes , which are in relative motion with respect to each other . furthermore , for the entire static network we propose a closed form extended global least squares -lrb- egls -rrb- solution to solve for all the unknown clock skews , clock offsets , initial pairwise distances and relative radial velocities using a single clock reference within the static network . a new cramer rao bound is derived and the proposed fusion center based extended global least squares -lrb- egls -rrb- solution is shown to be asymptotically optimal .",0
444,"This paper proposes a spatiotemporal motion model for video summarization. The model makes use of a compact description of a video sequence based on temporally localized motion estimates and a dominant motion assumption, as well as a generic temporal constraint. An image map is used to represent the content of the video, and mosaicing is employed to produce a visual summary. The model is evaluated for robustness and compression, and its effectiveness is demonstrated for video browsing and retrieval. The proposed approach offers a novel way to summarize video content while maintaining temporal consistency, and has potential applications in areas such as video surveillance and content-based video retrieval.",1
445,"the compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization . building such a compact description of a video sequence requires the capability to register all the frames with respect to the dominant object in the scene , a task which has been , in the past , addressed through temporally localized motion estimates . in this paper , we show how the lack of temporal consistency associated with such estimates can undermine the validity of the dominant motion assumption , leading to oscillation between different scene interpretations and poor registration . to avoid this oscillation , we augment the motion model with a generic temporal constraint which increases the robustness against competing interpretations , leading to more meaningful content summarization .",0
446,"This paper presents a framework for expert finding in community-based question answering sites. The proposed approach involves learning a ranking metric network embedding through a random-walk based learning method and utilizing recurrent neural networks to model user expertise. A relative quality rank is assigned to users based on their question-answering activities, and social relations are also considered in the model. A large-scale dataset is used to evaluate the effectiveness of the proposed method for question routing and expert finding. Results show that the proposed approach outperforms existing methods in terms of accuracy and efficiency.",1
447,"expert finding for question answering is a challenging question answering in community-based question answering site , arising in many applications such as question routing and the identification of best answers . in order to provide high-quality experts , many existing approaches learn the user model mainly from their past question-answering activities in cqa sites , which suffer from the spar-sity question answering of cqa data . in this paper , we consider the question answering of expert finding from the viewpoint of learning ranking metric embedding . we propose a novel ranking metric network learning framework for expert finding by exploiting both users ' relative quality rank to given questions and their social relations . we then develop a random-walk based learning method with recurrent neural networks for ranking metric network embedding . the extensive experiments on a large-scale dataset from a real world cqa site show that our random-walk based learning method achieves better performance than other state-of-the-art solutions to the question answering .",0
448,"This paper proposes a deletion-informed confidence estimation approach for detecting deletions in automatic speech recognition (ASR) output. The proposed approach employs conditional random field models to estimate deletion confidence scores, which are used for overall confidence estimation. The paper shows that using deletion confidence scores significantly improves the classification task of detecting deletions compared to a non-sequential model that does not take into account the sequence structure of the data. Overall, the proposed approach provides an effective way to improve the accuracy of ASR output by detecting deletions.",1
449,"in this work , the novel task of detecting deletions within automatic speech recognition -lrb- asr -rrb- system output is investigated . deletion-informed confidence estimation is proposed as an approach which simultaneously yields a confidence score in a word being correct , as well as a deletion confidence score which indicates whether a deletion is likely to occur in the output . the sequential nature of conditional random field models is exploited as a means through which this can be achieved . it is shown that this sequence structure is crucial in yielding useful deletion detection scores , with an equivalent non-sequential model proven to be unsuitable for the task . the deletion-informed confidence estimation approach is also shown to outperform one where deletion confidence scores are estimated as a classification task separate from that of overall confidence estimation .",0
450,"This paper presents a new hybrid reasoning system called ""Inference Graphs"". The system combines the logic of arbitrary and indefinite objects with inference graphs to create a new kind of hybrid reasoner. The system is based on natural deduction and subsumption reasoning, and allows for efficient and scalable reasoning in complex domains. The paper describes the architecture of the system and its implementation, and presents experimental results demonstrating its effectiveness in a variety of domains. The authors argue that Inference Graphs represent a promising approach to hybrid reasoning that combines the strengths of different reasoning paradigms.",1
451,"hybrid reasoners combine multiple types of reasoning , usually subsumption and prolog-style resolution . we outline a system which combines natural deduction and subsumption reasoning using inference graphs implementing a logic of arbitrary and indefinite objects .",0
452,"This paper presents a near-optimal batch mode active learning and adaptive submodular optimization framework for tackling batch mode active learning tasks. The proposed framework addresses the natural diminishing returns condition inherent in batch-mode active learning, which typically results in reduced performance as more data is added. The framework combines multi-stage influence maximization with adaptive submodularity and sequential strategy to develop an optimal batch-mode policy. The proposed approach improves on the traditional greedy strategy and crowdsourcing heuristics by incorporating a more efficient learning algorithm. The proposed framework is evaluated on social networks and achieves improved performance on multi-stage influence maximization tasks. Overall, the paper presents a novel approach to batch-mode active learning that is both efficient and effective, with promising results for a variety of applications.",1
453,"active learning can lead to a dramatic reduction in labeling effort . however , in many practical implementations -lrb- such as crowdsourcing , surveys , high-throughput experimental design -rrb- , it is preferable to query labels for batches of examples to be labelled in parallel . while several heuristics have been proposed for batch-mode active learning , little is known about their theoretical performance . we consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity , a natural diminishing returns condition . we prove that for such problems , a simple greedy strategy is competitive with the optimal batch-mode policy . in some cases , surprisingly , the use of batches incurs competitively low cost , even when compared to a fully sequential strategy . we demonstrate the effectiveness of our approach on batch-mode active learning tasks , where it outperforms the state of the art , as well as the novel problem of multi-stage influence maximization in social networks .",0
454,"This paper proposes a method for detecting and localizing general 3D object classes using multiview and viewpoint invariant representations, specifically through the use of viewpoint invariant reference frames. The proposed method is evaluated using the public color feret database and the CMU profile database, with local scale-invariant features used for detection. The paper shows that the proposed viewpoint invariant approach outperforms traditional multiview representations in terms of average precision, with consistent geometrical interpretation and modeling. False positive detections are reduced using an iterative learning algorithm. The paper presents a data-driven approach to detect 3D object classes in a multi-stage pipeline. The experimental results show the effectiveness of the proposed method in detecting and localizing 3D object classes.",1
455,"in this paper , we investigate detection and localization of general 3d object classes by relating local scale-invariant features to a viewpoint invariant reference frame . this can generally be achieved by either a multi-view representation , where features and reference frame are modeled as a collection of distinct views , or by a viewpoint invariant representation , where features and reference frame are mod-eled independently of viewpoint . we compare multi-view and viewpoint invariant representations trained and tested on the same data , where the viewpoint invariant approach results in fewer false positive detections and higher average precision . we present a new , iterative learning algorithm to determine an optimal viewpoint invariant reference frame from training images in a data-driven manner . the learned optimal reference frame is centrally located with respect to the 3d object class and to image features in a given view , thereby minimizing reference frame localization error as predicted by theory and maintaining a consistent geometrical interpretation with respect to the underlying object class . modeling and detection based on the optimal reference frame improves detection performance for both multiview and viewpoint invariant representations . experimentation is performed on the class of 3d faces , using the public color feret database for training , the cmu profile database for testing and sift image features .",0
456,"This paper presents a method for estimating a speaker's confusion matrix from sparse data. The confusion matrix is a measure of the accuracy of speech recognizers, and this paper proposes a non-negative matrix factorization approach to estimate the mean confusion matrix for a given speaker. The method takes into account the sparsity of the data and uses a maximum likelihood estimation approach to obtain the best estimate of the confusion matrix. The proposed method is evaluated on real-world datasets, and the results demonstrate its effectiveness in estimating the speaker's confusion matrix from sparse data. This work has significant implications for improving the accuracy of speech recognizers and enabling better performance in various speech-related applications.",1
457,"confusion matrices have been widely used to increase the accuracy of speech recognisers , but usually a mean confusion matrix , averaged over many speakers , is used . however , analysis shows that confusion matrices for individual speakers vary considerably , and so there is benefit in obtaining estimates of confusion matrices for individual speakers . unfortunately , there is rarely enough data to make reliable estimates . we present a technique for estimating the elements of a speaker 's confusion matrix given only sparse data from the speaker . it utilizes non-negative matrix factorisation to find structure within confusion matrices , and this structure is exploited to make improved estimates . results show that under certain conditions , this technique can give estimates that are as good as those obtained with twice the number of utterances available from the speaker .",0
458,"This paper proposes a method for learning concept embeddings by combining both human and machine expertise. The authors use the Caltech-UCSD Birds dataset as an example of an unstructured set of pictographic characters and explore the use of low-dimensional concept embedding algorithms, such as automatic machine similarity kernels. They demonstrate that subjective human taste can be combined with objective machine-based measures to produce concept embeddings that capture both human expertise and automatic feature extraction. The authors show that combining deep-learned features with human insight results in better embeddings than using either alone. Additionally, they explore the use of snack embeddings and demonstrate that training datasets with snack embeddings improve bird classifiers. Overall, the proposed approach shows promise for learning concept embeddings that combine both human and machine expertise.",1
459,"this paper presents our work on '' snack , '' a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels . both parts are complimentary : human insight can capture relationships that are not apparent from the object 's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints . we show that our snack embeddings are useful in several tasks : distinguishing prime and nonprime numbers on mnist , discovering labeling mistakes in the caltech ucsd birds dataset with the help of deep-learned features , creating training datasets for bird classifiers , capturing subjective human taste on a new dataset of 10,000 foods , and qualitatively exploring an unstructured set of pictographic characters . comparisons with the state-of-the-art in these tasks show that snack produces better concept embeddings that require less human supervision than the leading methods .",0
460,"This paper proposes a method for efficiently simulating biological neural networks on massively parallel supercomputers with hypercube architecture. The simulation of neural networks is a computation-ally demanding task, particularly when considering the temporal dynamics of spike interactions between biologically realistic neurons with nontrivial single-cell dynamics. The authors show that their method can efficiently simulate the primary visual system neural network and scale to larger networks with biological data structures. Their approach utilizes the hypercube architecture to distribute the simulation across thousands of processors, reducing simulation time and enabling the simulation of larger and more complex networks. The paper presents experimental results demonstrating the efficiency and scalability of their method.",1
461,"we present a neural network simulation which we implemented on the massively parallel connection machine 2 . in contrast to previous work , this neural network simulation is based on biologically realistic neu-rons with nontrivial single-cell dynamics , high connectivity with a structure modelled in agreement with biological data , and preservation of the temporal dynamics of spike interactions . we simulate neural networks of 16,384 neurons coupled by about 1000 synapses per neuron , and estimate the performance for much larger systems . communication between neurons is identified as the computation-ally most demanding task and we present a novel method to overcome this bottleneck . the neural network simulation has already been used to study the primary visual system of the cat .",0
462,"This paper presents a symbolic model-checking algorithm for the One-Resource RB+-ATL logic. RB+-ATL is a logic for reasoning about systems with a limited resource, such as time or energy, and ATL is a temporal logic for reasoning about the behavior of multi-agent systems. The algorithm is based on a forward search of the state space and a symbolic implementation of the model-checking problem. The paper shows that the algorithm can handle larger models than previous approaches and provides experimental results to demonstrate its efficiency. The paper concludes that the proposed algorithm is an effective method for checking One-Resource RB+-ATL models.",1
463,"rb ± rb ± atl is an extension of rb ± atl where it is possible to model consumption and production of several resources by a set of agents . the model-checking problem for rb ± atl is known to be decidable . however the only available model-checking algorithm for rb ± atl uses a forward search of the state space , and hence does not have an efficient symbolic implementation . in this paper , we consider a fragment of rb ± atl , 1rb ± rb ± atl , that allows only one resource type . we give a symbolic model-checking algorithm for this fragment of rb ± atl , and evaluate the performance of an mcmas-based implementation of the symbolic model-checking algorithm on an example problem that can be scaled to large state spaces .",0
464,"This paper presents a novel approach to improving the accuracy of text classification tasks by using error-correcting codes. Specifically, the authors propose the use of error-correcting output coding (ECOC) to enhance the performance of text classifiers on real-world datasets. ECOC is a technique that maps multiple binary classifiers to a single multiclass problem by using error-correcting codes to represent the class labels. The authors demonstrate the effectiveness of their approach on various text classification tasks, showing that using ECOC can improve classification accuracy compared to traditional methods. They also explore the use of different types of error-correcting codes and analyze their impact on classification performance. The results of their experiments suggest that error-correcting codes have great potential for improving the accuracy of text classifiers on real-world datasets.",1
465,"this paper explores in detail the use of error correcting output coding for learning text classifiers . we show that the accuracy of a naive bayes classifier over text classification tasks can be significantly improved by taking advantage of the error-correcting properties of the code . we also explore the use of different kinds of codes , namely error-correcting codes , random codes , and domain and data-specific codes and give experimental results for each of them . the error correcting output coding scales well to large data sets with a large number of classes . experiments on a real-world data set show a reduction in classification error by up to 66 % over the traditional naive bayes classifier . we also compare our empirical results to semi-theoretical results and find that the two closely agree .",0
466,"This paper proposes a particle filter beamforming method for localizing an acoustic source in a reverberant environment. The method involves a single-step approach to estimate the source location using intermediate time-delay estimates and particle filtering. The proposed approach is shown to outperform the traditional two-step procedure for acoustic source localization. The particle filter beamformer is applied to steered beamforming, and the resulting localization estimates are evaluated for speech enhancement applications. The method is shown to be effective in dealing with reverberation and achieving accurate source trajectory estimation. The paper demonstrates the usefulness of particle filtering for acoustic source localization and suggests potential for further research in this area.",1
467,"traditional acoustic source localization uses a two-step procedure requiring intermediate time-delay estimates from pairs of microphones . an alternative single-step approach is proposed in this paper in which particle filtering is used to estimate the source location through steered beamforming . this single-step approach is especially attractive in speech enhancement applications , where the localization estimates are typically used to steer a beamformer at a later stage . simulation results show that the single-step approach is robust to reverberation , and is able to accurately follow the source trajectory .",0
468,"This paper proposes a railway infrastructure system diagnosis scheme that utilizes Empirical Mode Decomposition (EMD) and Hilbert Transform. EMD is used to extract intrinsic mode functions from the measured signals, which are then analyzed using Hilbert Transform to obtain instantaneous frequency and physical meaning. The proposed scheme is tested on simulated and experimental signals of track/vehicle transmission and track circuit, and is found to be effective in detecting periodic patterns and identifying faults in the railway infrastructure components. The results demonstrate the potential of EMD and Hilbert Transform in railway system diagnosis and maintenance.",1
469,"this paper introduces a diagnosis scheme of a railway infrastructure component based on a combined use of empirical mode decomposition and hilbert transform . this railway infrastructure component is dedicated to track/vehicle transmission referred as track circuit . the aim is to detect its working state from one measurement signal which can be viewed as a superposition of several oscillations and periodic patterns called intrinsic mode functions . for this application , it will be shown that physical meaning can be assigned to each mode that emd tries to extract . furthermore , when the hilbert transform of the intrinsic mode functions is performed , we show that the changing of instantaneous frequency can be linked to the existence of defect . the performances are illustrated on both simulated and experimental signals .",0
470,This paper proposes a method for foreign accent conversion through voice morphing. The proposed voice morphing strategy utilizes a continuum of accent transformations achieved by spectral morphing. The spectral detail is modified using pulse density modulation and averaging pulses in a pair-wise fashion. Speaker identity is preserved while converting foreign accent to native accent. The proposed method is evaluated on parallel recordings of Arctic speakers and demonstrated to achieve high acoustic quality in accent conversions. The results show that the proposed approach is effective for foreign accent conversion using voice morphing.,1
471,"we present a voice morphing strategy that can be used to generate a continuum of accent transformations between a foreign speaker and a native speaker . the voice morphing strategy performs a cepstral decomposition of speech into spectral slope and spectral detail . accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker . spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair-wise fashion . the voice morphing strategy is validated on parallel recordings from two arctic speakers using both objective and subjective measures of acoustic quality , speaker identity and foreign accent .",0
472,"This paper addresses the problem of fast search in Hamming space using multi-index hashing. The Hamming distance is an important concept in this context, and the paper focuses on binary codes. The paper presents a novel approach using multi-index hashing to address this open problem, allowing for faster search in high-dimensional spaces. The results show that the proposed approach is efficient and effective, and outperforms existing methods in terms of both speed and accuracy. This paper is a valuable contribution to the field of search in Hamming space and has practical applications in areas such as computer vision, pattern recognition, and information retrieval.",1
473,problem context open problem : exact sub-linear nearest neighbor search in hamming distance on binary codes .,0
474,"This paper presents an efficient bit assignment strategy for perceptual audio coding in the context of MPEG-4 AAC. The goal is to allocate bits optimally for different parts of an audio signal, while keeping computational complexity low. Two frame-level bit assignment methods, max-BNLR scheme and TB-ANMR, are compared, and a new bit assignment algorithm is proposed that combines the advantages of both methods. The proposed algorithm achieves better performance than the existing methods, especially at low rates. The algorithm is based on a band-level bit allocation strategy that takes into account the perceptual importance of different frequency bands. Experimental results show that the proposed algorithm achieves significant improvements in audio quality compared to the existing methods, while maintaining low computational complexity.",1
475,"for the purpose of efficient audio coding at low rates , a new bit allocation strategy is proposed in this paper . the basic idea behind this bit allocation strategy is '' give bits to the band with the maximum nmr-gain/bit '' or '' retrieve bits from the band with the maximum bits/nmr-loss '' . the notion of '' bit-use efficiency '' is suggested and bit allocation strategy can be employed to construct a bit assignment algorithm operated at band-level as compared to the traditional frame-level bit assignment methods . based on this bit assignment algorithm a new bit allocation strategy , called max-bnlr scheme , is designed for the mpeg-4 aac . simulation results show that the performance of the max-bnlr scheme is significantly better than that of the mpeg-4 aac verification model and is close to that of tb-anmr -lsb- 3 -rsb- , which is the -lrb- nearly -rrb- optimal solution . moreover , the max-bnlr scheme has the advantages of low computational complexity comparing to tb-anmr .",0
476,"This paper proposes a deep quantization network architecture for efficient image retrieval. The goal is to develop a supervised hashing method that can generate binary codes with high hashing quality and low quantization error. The proposed method uses a sub-network with convolution-pooling layers to extract deep image representations, which are then mapped to a dimension-reduced representation using a pairwise cosine loss layer. A product quantization loss is used to learn similarity-preserving hash codes. The method is evaluated on large-scale multimedia retrieval datasets and compared to other hand-crafted or machine-learned features and supervised hashing methods, including max-margin hashing and spectral hashing. The experimental results show that the proposed method outperforms state-of-the-art methods in terms of retrieval accuracy and efficiency.",1
477,"hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval . supervised hashing improves the quality of hash coding by exploiting the semantic similarity on data pairs and has received increasing attention recently . for most existing supervised hashing methods for image retrieval , an image is first represented as a vector of hand-crafted or machine-learned features , then quantized by a separate quantization step that generates binary codes . however , suboptimal hash coding may be produced , since the quantization error is not statistically minimized and the quantization step is not optimally compatible with the hash coding . in this paper , we propose a novel deep quantization network architecture for supervised hashing , which learns image representation for hash coding and formally control the quantization error . the deep quantization network architecture constitutes four key components : -lrb- 1 -rrb- a sub-network with multiple convolution-pooling layers to capture deep image representations ; -lrb- 2 -rrb- a fully connected bottleneck layer to generate dimension-reduced representation optimal for hash coding ; -lrb- 3 -rrb- a pairwise cosine loss layer for similarity-preserving learning ; and -lrb- 4 -rrb- a product quantiza-tion loss for controlling hashing quality and the quantizability of bottleneck representation . extensive experiments on standard image retrieval datasets show the proposed deep quantization network architecture yields substantial boosts over latest state-of-the-art hashing methods .",0
478,"This paper proposes a semantic segmentation method using regions and parts. The approach is based on scanning-windows part models, which incorporate both global appearance cues and local part-based features. The method utilizes region-based object detectors to generate bottom-up regions and then performs pixel classification using class-specific scores. The proposed approach is evaluated on the PASCAL segmentation challenge and achieves competitive results in recognizing objects and articulated categories in real-world images. Additionally, the method is shown to be effective in recognizing articulated objects with high accuracy on the VOC2010 dataset. Overall, the proposed approach demonstrates the efficacy of incorporating part models and region-based object detectors in semantic segmentation tasks.",1
479,"we address the problem of segmenting and recognizing objects in real world images , focusing on challenging articulated categories such as humans and other animals . for this purpose , we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues . our detectors produce class-specific scores for bottom-up regions , and then aggregate the votes of multiple overlapping candidates through pixel classification . we evaluate our approach on the pascal segmentation challenge , and report competitive performance with respect to current leading techniques . on voc2010 , our method obtains the best results in 6/20 categories and the highest performance on articulated objects .",0
480,"This paper presents a low-power bit-serial Viterbi decoder chip for next-generation wide-band CDMA systems. The decoder employs add-compare-select (ACS) units and a power-efficient trace-back scheme for high-speed convolutional decoding. The chip is designed using three-layer metal CMOS technology and is capable of 2Mbps operation. The proposed decoder also includes a bit-serial arithmetic coding rate and application-specific memory for efficient trace-back operations. The power consumption of the decoder is significantly reduced compared to conventional designs, making it suitable for use in mobile systems such as wireless ATM LANs. The performance of the decoder is evaluated using the PACT10 benchmark and the results demonstrate that the proposed design achieves state-of-the-art performance with reduced power consumption.",1
481,"this paper presents a low-power bit-serial viterbi decoder chip with the coding rate r = 1 = 3 and the constraint length k = 9 -lrb- 256 states -rrb- . this low-power bit-serial viterbi decoder chip has been implemented using 0.5 m three-layer metal cmos technology and is targeted for high speed convolu-tional decoding for next generation wireless applications such as wide-band cdma mobile systems and wireless atm lans . the low-power bit-serial viterbi decoder chip is expected to operate at 20mbps under 3.3 v and at 2mbps under 1.8 v . the add-compare-select units have been designed using bit-serial arithmetic , which has made it feasible to execute 256 acs operations in parallel . for trace-back operations , we have developed a novel power-efficient trace-back scheme and an application-specific memory , which was designed considering that 256 bits should be written simultaneously for write operations but only one bit needs to be accessed for read operations . we have estimated that the low-power bit-serial viterbi decoder chip dissipates only 10mw at 2mbps operation under 1.8 v.",0
482,"This paper presents a novel approach for non-stationary feature extraction for automatic speech recognition. The proposed method utilizes short-time Fourier transform based features and a pitch-adaptive gammatone filter bank for acoustic feature extraction. The non-stationary signal analysis is carried out in the context of an ASR framework. The robustness of the proposed method against noise and voiced speech is evaluated and compared with traditional Mel frequency cepstral coefficients (MFCC). Results show that the proposed method outperforms MFCC for speech recognition systems. The study also presents a comparison between MFCC and the proposed approach, demonstrating that the proposed approach is a promising alternative to traditional methods for acoustic feature extraction.",1
483,"in current speech recognition systems mainly short-time fourier transform based features like mfcc are applied . dropping the short-time stationarity assumption of the voiced speech , this paper introduces the non-stationary signal analysis into the asr framework . we present new acoustic features extracted by a pitch-adaptive gammatone filter bank . the noise robustness was proved on aurora 2 and 4 tasks , where the proposed acoustic features outperform the standard mfcc . furthermore , successful combination experiments via mfcc indicate the differences between the new acoustic features and mfcc .",0
484,"This paper presents a learning-based approach for automatic face annotation from frontal images only, even for arbitrary poses and expressions. The proposed method uses active appearance models and virtual images of unseen faces to establish correspondences between landmarks in order to annotate manually selected training images. The annotated frontal image is then used to reconstruct images of faces in different poses and expressions using view-based AAMs. Statistical approaches are used to fit non-rigid deformable models to the reconstructed images. The approach achieves automatic annotation of visually deformable objects, including faces, by fitting tracking and texture to virtual images. The effectiveness of the proposed method is demonstrated through experiments on a database of face images.",1
485,"statistical approaches for building non-rigid deformable models , such as the active appearance model , have enjoyed great popularity in recent years , but typically require tedious manual annotation of training images . in this paper , a learning based approach for the automatic annotation of visually deformable objects from a single annotated frontal image is presented and demonstrated on the example of automatically annotating face images that can be used for building active appearance model for fitting and tracking . this learning based approach employs the idea of initially learning the correspondences between landmarks in a frontal image and a set of training images with a face in arbitrary poses . using this learner , virtual images of unseen faces at any arbitrary pose for which the learner was trained can be reconstructed by predicting the new landmark locations and warping the texture from the frontal image . view-based aams are then built from the virtual images and used for automatically annotating unseen images , including images of different facial expressions , at any random pose within the maximum range spanned by the virtually reconstructed images . the learning based approach is experimentally validated by automatically annotating face images from three different databases .",0
486,"Age estimation from facial images has been an important research area in computer vision. In this paper, we propose a novel approach for learning ordinal discriminative features for age estimation. Our method utilizes the local manifold structure of facial images and incorporates both ordinal and locality information for feature learning. We evaluate our approach on the public available images of groups dataset and the FG-NET dataset, and show that our method outperforms several state-of-the-art methods in terms of accuracy and robustness. Moreover, we conduct an ablation study to demonstrate the effectiveness of our feature selection method and show that the redundant information in the aging process can be removed by our approach. Finally, we explore the nonlinear correlation between facial features and age and compare it with the rank correlation.",1
487,"in this paper , we present a new method for facial age estimation based on ordinal discriminative feature learning . considering the temporally ordinal and continuous characteristic of aging process , the proposed method not only aims at preserving the local manifold structure of facial images , but also it wants to keep the ordinal information among aging faces . moreover , we try to remove redundant information from both the locality information and ordinal information as much as possible by minimizing nonlinear correlation and rank correlation . finally , we formulate these two issues into a unified optimization problem of feature selection and present an efficient solution . the experiments are conducted on the public available images of groups dataset and the fg-net dataset , and the experimental results demonstrate the power of the proposed method against the state-of-the-art methods .",0
488,"This paper introduces the Set Covering Machine with Data-Dependent Half-Spaces, a new model that extends the traditional Set Covering Machine by using data-dependent half-spaces instead of the traditional data-independent balls. The authors argue that this approach leads to better generalization performance, particularly on natural data sets. Model selection is also discussed, with the authors proposing a method to tune the parameters of the model. The paper concludes by presenting experimental results that support the effectiveness of the proposed approach compared to the traditional Set Covering Machine. Overall, this paper shows the potential of using data-dependent half-spaces in machine learning models, particularly in the context of the Set Covering Machine.",1
489,"we examine the set covering machine when set covering machine uses data-dependent half-spaces for its set of features and bound its generalization error in terms of the number of training errors and the number of half-spaces set covering machine achieves on the training data . we show that set covering machine provides a favorable alternative to data-dependent balls on some natural data sets . compared to the support vector machine , the set covering machine with data-dependent half-spaces produces substantially sparser clas-sifiers with comparable -lrb- and sometimes better -rrb- generalization . furthermore , we show that our bound on the generalization error provides an effective guide for model selection .",0
490,"This paper proposes a distributed genetic algorithm for discovering the best wavelet packet basis for speech recognition. The algorithm is based on a global learning scheme and a fitness value that is derived from a speech-modeling algorithm. The wavelet packet basis is decomposed using a mel-scaled subband decomposition, and the algorithm searches for the optimal basis through a priori reference to a simulated fitness space. The reference system is based on a connectionist system that uses a neural network to model speech recognition. The proposed algorithm outperforms other feature extraction modules and speech recognition models, demonstrating the effectiveness of the approach. The results indicate that the proposed algorithm has great potential in improving speech recognition accuracy.",1
491,"in the learning process of speech modeling , many choices or settings are defined '' a priori '' or are resulting from years of experimental work . in this paper , instead , a global learning scheme is proposed based on a distributed genetic algorithm combined with a standard speech-modeling algorithm . the speech recognition models are now created out of a pre-defined space of solutions . furthermore , this global learning scheme enables to learn the speech models as well as the best feature extraction module . experimental validation is performed on the task of discovering the wavelet packet best basis decomposition , knowing that the '' a priori '' reference is the mel-scaled subband decomposition . two experiments are presented , a reference system using a simulated fitness and a second one that uses the speech recognition performance as fitness value . in the latter , each element of the space is a connectionist system defined by a wavelet topology and its associated neural network .",0
492,"This paper presents a novel approach for designing Piecewise Volterra (PWV) filters based on the threshold decomposition operator. PWV filters are a class of filters with hard nonlinear structures that can be expressed in multilinear tensor forms. The proposed method uses partition boundaries to divide the input space into regions where the PWV filters can be approximated by multivariate polynomials. The parameter estimation problem is then formulated as a linear problem, which can be efficiently solved. The PWV's filter taps are obtained by a hyper-rectangular lattice, and the continuity of the filter is guaranteed at the partition boundaries. The effectiveness of the proposed method is demonstrated through experiments on synthetic data and real-world applications. The results show that the proposed PWV filters outperform other state-of-the-art filters in terms of performance and computational complexity.",1
493,"in this paper we report our results concerning the study of multivariate functions of threshold-decomposed signals . in particular we show that multilinear tensor forms of the decomposed signal yield a class of lters that we propose to call piecewise volterra filters . a lter can be viewed as a transformation of r n ! r , where n is the number of lter taps . pwv lters partition r n using a hy-per-rectangular lattice , and assign a volterra lter to each of the partition regions . at the partition boundaries continuity between the multivariate polynomials is preserved resulting in class c 0 piecewise polynomials . piecewise volterra filters constitute an eecient alternative for describing some systems rich in hard nonlinear structures , especially since parameter estimation remains a linear problem for pwv 's .",0
494,"This paper presents a high-order regularization approach for semi-supervised learning of structured output problems, with a focus on image segmentation tasks. The proposed method utilizes discrete optimization algorithms and high-order regularizers to incorporate unlabeled data into the learning process. Specifically, the method employs a graph regularizer and a cardinality regularizer to encourage smoothness and sparsity in the model predictions, respectively. The approach is formulated within a max-margin framework and is evaluated on various image segmentation tasks. The results demonstrate the effectiveness of the proposed method in utilizing unlabeled examples and improving the performance of structured output learning.",1
495,"semi-supervised learning , which uses unlabeled data to help learn a discriminative model , is especially important for structured output problems , as considerably more effort is needed to label its multi-dimensional outputs versus standard single output problems . we propose a new max-margin framework for semi-supervised structured output learning , that allows the use of powerful discrete optimization algorithms and high order regular-izers defined directly on model predictions for the unlabeled examples . we show that our max-margin framework is closely related to posterior regulariza-tion , and the two max-margin framework optimize special cases of the same objective . the new max-margin framework is instantiated on two image segmentation tasks , using both a graph regularizer and a cardinality regularizer . experiments also demonstrate that this max-margin framework can utilize unlabeled data from a different source than the labeled data to significantly improve performance while saving labeling effort .",0
496,"This paper proposes a new approach for multiview subspace clustering, called Low-Rank Tensor Constrained Multiview Subspace Clustering (LT-MSC). The method leverages the complementary information available across multiple views of data and imposes a low-rank tensor constraint on the subspace representation matrices. The proposed approach formulates a tensor nuclear norm minimization problem along with an ℓ2,1-norm regularizer and linear equalities to capture high-order correlations between the views. The method is evaluated on benchmark image datasets, and results show that LT-MSC outperforms other state-of-the-art methods in terms of clustering accuracy. The paper also discusses the effectiveness of the affinity matrix and its role in the clustering inference process.",1
497,"in this paper , we explore the problem of multiview sub-space clustering . we introduce a low-rank tensor constraint to explore the complementary information from multiple views and , accordingly , establish a novel method called low-rank tensor constrained multiview subspace clustering -lrb- lt-msc -rrb- . our method regards the subspace representation matrices of different views as a low-rank tensor constraint , which captures dexterously the high order correlations underlying multi-view data . then the low-rank tensor constraint is equipped with a low-rank constraint , which models elegantly the cross information among different views , reduces effectually the redundancy of the learned subspace representations , and improves the accuracy of clustering as well . the inference process of the affinity matrix for clustering is formulated as a ten-sor nuclear norm minimization problem , constrained with an additional ℓ 2,1-norm regularizer and some linear equalities . the minimization problem is convex and thus can be solved efficiently by an augmented lagrangian alternating direction minimization -lrb- al-adm -rrb- method . extensive experimental results on four benchmark image datasets show the effectiveness of the proposed lt-msc method .",0
498,"This paper presents an approach to cost-optimal planning using landmarks. Landmarks are states or propositions that are useful for guiding the search process towards a solution plan. The paper proposes a new landmark-based heuristic that is admissible, meaning it never overestimates the true cost of the optimal solution, and also multi-path dependent, meaning it can be computed separately for each path in the search space. This heuristic is used within a best-first search procedure to find the optimal solution plan. The experimental results show that the proposed approach outperforms previous landmark-based heuristics and achieves state-of-the-art performance on several benchmark domains.",1
499,"planning landmarks are facts that must be true at some point in every solution plan . previous work has very successfully exploited planning landmarks in satisficing planning . we propose a methodology for deriving admissible heuristic estimates for cost-optimal planning from a set of planning landmarks . the resulting heuristics fall into a novel class of multi-path dependent heuristics , and we present a simple best-first search procedure exploiting such heuristics . our empirical evaluation shows that this framework favorably competes with the state-of-the-art of cost-optimal heuristic search .",0
500,"This paper addresses the problem of underdetermined blind separation of audio sources from their convolutive mixtures in the time-frequency domain. Specifically, the focus is on separating nonstationary sources that are active in the same time-frequency points. The proposed method utilizes a subspace projection technique to estimate the sources' time-frequency distribution values, taking into account both the disjoint and non-disjoint nature of the sources in the time-frequency domain. The effectiveness of the method is demonstrated through simulations and comparisons with other methods. The results show that the proposed method can achieve high-quality source separation in the underdetermined convolutive mixture case.",1
501,"this paper considers the blind separation of nonstationary sources in the underdetermined convolutive mixture case . we introduce two methods based on the sparsity assumption of the sources in the time-frequency domain . the first one assumes that the sources are disjoint in the tf domain ; i.e. there is at most one source signal present at a given point in the tf domain . in the second method , we relax this assumption by allowing the sources to be tf-nondisjoint to a certain extent . in particular , the number of sources present -lrb- active -rrb- at a tf point should be strictly less than the number of sensors . in that case , the blind separation of nonstationary sources can be achieved thanks to subspace projection which allows us to identify the active sources and to estimate their corresponding time-frequency distribution values .",0
502,This paper proposes a region-of-interest (ROI) based rate control scheme for high efficiency video coding (HEVC). The scheme is designed to achieve target bit rates while maintaining high subjective quality for videoconferencing systems. The proposed rate control algorithm allocates bits to coding units based on the relative importance of each region of interest (ROI) in the frame. The bit allocation process is guided by both subjective quality evaluation and objective metrics. The proposed scheme outperforms the global bit rate control approach in terms of subjective quality while maintaining similar coding efficiency. The effectiveness of the proposed ROI-based rate control scheme is demonstrated through simulations and comparisons with other methods. The results show that the proposed scheme can achieve high subjective quality while meeting the target bit rates for videoconferencing systems.,1
503,"in this paper , we propose a new rate control scheme designed for the newest high efficiency video coding -lrb- hevc -rrb- standard , and aimed at enhancing the quality of regions of interest -lrb- roi -rrb- . our rate control scheme allocates a higher bit rate to the region of interest while keeping the global bit rate close to the assigned target value . this rate control scheme is developed for a videoconferencing system , where the rois -lrb- typically , faces -rrb- are automatically detected and each coding unit is classified in a region of the interest map . this map is given as input to the rate control algorithm and the bit allocation is made accordingly . experimental results show that the proposed rate control scheme achieves accurate target bit rates and provides an improvement in the region of interest quality , both in objective metrics and based on subjective quality evaluation .",0
504,This paper proposes a novel approach for image segmentation with a bounding box prior. The method utilizes a global energy minimization framework that incorporates user-provided object bounding boxes as hard constraints. The proposed approach is based on the graph cut algorithm and can be used with a fractional LP solution or optimization strategies that involve linear relaxation and thresholding-based rounding. The proposed method also employs a topological prior to improve the segmentation quality. The interaction paradigm used in the method involves pinpointing objects of interest within the bounding box. The effectiveness of the proposed approach is demonstrated through simulations and comparisons with other methods using a publicly available dataset. The results show that the proposed method outperforms standalone heuristic methods and achieves better segmentation accuracy compared to other interactive image segmentation frameworks. The proposed method is especially effective when the objects of interest are well-contained within the user-provided bounding box.,1
505,"user-provided object bounding box is a simple and popular interaction paradigm considered by many existing interactive image segmentation frameworks . however , these frameworks tend to exploit the provided bounding box merely to exclude its exterior from consideration and sometimes to initialize the energy minimization . in this paper , we discuss how the bounding box can be further used to impose a powerful topological prior , which prevents the solution from excessive shrinking and ensures that the user-provided box bounds the segmentation in a sufficiently tight way . the topological prior is expressed using hard constraints incorporated into the global energy minimization framework leading to an np-hard integer program . we then investigate the possible optimization strategies including linear relaxation as well as a new graph cut algorithm called pinpointing . the latter can be used either as a rounding method for the fractional lp solution , which is provably better than thresholding-based rounding , or as a fast standalone heuristic . we evaluate the proposed algorithms on a publicly available dataset , and demonstrate the practical benefits of the new topological prior both qualitatively and quantitatively .",0
506,"This paper presents a new approach for watermarking speech signals using the sinusoidal model and frequency modulation of the partials. The proposed method utilizes the sinusoidal model to analyze the speech signals and extract the sinusoidal partials. The watermarking task is then performed by modulating the frequency of the partials to embed the watermark. The proposed method is robust to common signal processing operations and attacks, while maintaining high fidelity and perceptual quality. The effectiveness of the proposed method is demonstrated through simulations and comparisons with other methods. The results show that the proposed method outperforms other watermarking methods in terms of robustness and quality, making it suitable for various audio/speech signals applications. The proposed approach can be useful in protecting copyright, ensuring authenticity, and preventing unauthorized access to speech signals. The proposed method is especially effective when the sinusoidal model is used for audio/speech signals analysis.",1
507,"in this paper , the application of the sinusoidal model for audio/speech signals to the watermarking task is proposed . the basic idea is that adequate modulation of medium rank partials -lrb- frequency -rrb- trajectories is not perceptible and thus this modulation may contain the data to be embedded in the signal . the modulation -lrb- encoding -rrb- and estimation -lrb- decoding -rrb- of the message are described in this paper and preliminary promising results are given in the case of speech signals .",0
508,"This paper proposes a novel approach for enhancing real blurred and noisy color images using both color and spatial information. The proposed method employs a filter that takes into account both color and spatial information to improve image quality. The filter is designed based on a weighted cost function that balances the trade-off between image enhancement capabilities and computational complexity. The effectiveness of the proposed method is demonstrated through simulations and comparisons with other methods. The results show that the proposed method outperforms other image enhancement methods in terms of both visual quality and objective measures. The proposed approach is effective for various types of real blurred and noisy color images, making it suitable for a wide range of applications. The proposed filter is especially effective in enhancing color images by taking into account both color and spatial information. The proposed approach can be useful in various domains, including surveillance, medical imaging, and multimedia applications.",1
509,"* 564 -rrb- +6 in this paper , a new filter that is performing color image enhancement is presented . the filter is achieving this through the minimization of a weighted cost function . the weights are determined using potential functions which are calculated in such a way as to convey spatial information . application of the proposed filter on a real blurred and noisy color image is performed to verify its enhancement capabilities .",0
510,"This paper focuses on the rates of convergence of performance gradient estimates in reinforcement learning using function approximation and bias. In particular, the study considers linear function approximation representations of the state-action value function in policy gradient reinforcement learning algorithms. The paper shows that the performance gradient estimates using function approximation representations may converge at suboptimal rates due to a non-zero bias term that arises from the use of basis functions. The study examines the impact of the bias term on the convergence rates and proposes a new method for reducing its effects. The proposed method involves adjusting the basis functions to eliminate the bias term, which improves the convergence rates of the performance gradient estimates. The effectiveness of the proposed method is demonstrated through simulations and comparisons with other methods. The results show that the proposed approach improves the convergence rates of the performance gradient estimates compared to existing methods. The study provides insights into the role of function approximation bias in reinforcement learning algorithms and suggests methods for reducing its impact on performance gradient estimates. The proposed approach can be useful in various applications of reinforcement learning, including robotics, gaming, and control systems.",1
511,"we address two open theoretical questions in policy gradient reinforcement learning . the first concerns the efficacy of using function approximation to represent the state action value function , q. theory is presented showing that linear function approximation representations of q can degrade the rate of convergence of performance gradient estimates by a factor of o -lrb- m l -rrb- relative to when no function approximation of q is used , where m is the number of possible actions and l is the number of basis functions in the function approximation representation . the second concerns the use of a bias term in estimating the state action value function . theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by o -lrb- 1 − -lrb- 1/m -rrb- -rrb- , where m is the number of possible actions . experimental evidence is presented showing that these theoretical results lead to significant improvement in the convergence properties of policy gradient reinforcement learning algorithms .",0
512,"This paper proposes a cluster-based user simulation technique for learning dialogue strategies in spoken dialogue systems. The technique is evaluated through user simulations and used for reinforcement learning of clarification strategies to improve mutual understanding in conversational interactions. The paper highlights the importance of dialogue strategies in achieving mutual understanding and the use of cluster-based techniques to improve the accuracy of user simulations. The results show that the proposed technique outperforms a random baseline and can be used to inform dialogue management in spoken dialogue systems. Overall, the study provides insights into the effectiveness of cluster-based user simulations for learning dialogue strategies and improving dialogue outcomes in spoken dialogue systems.",1
513,"good dialogue strategies in spoken dialogue systems help to ensure and maintain mutual understanding and thus play a crucial role in robust conversational interaction . we focus on clarification strategies and build user simulations which are critical for reinforcement learning , which is a cheap and principled way to automatically optimise dialogue management . in this paper we present a novel cluster-based technique for building user simulations which show varying , but complete and consistent behaviour with respect to real users . we use this cluster-based technique to build user simulations and we also introduce the cluster-based user simulation technique which allows us to evaluate user simulations with respect to these desiderata . we show that the cluster-based user simulation technique performs significantly better -lrb- at p < 0.01 -rrb- than decisions made using either the one most likely action or a random base-line . the cluster-based user simulations reduce the average error of these other cluster-based user simulation technique by 53 % and 34 % respectively .",0
514,"This paper investigates the effect of signal-to-noise ratio (SNR) and beamforming techniques on speaker diarisation in meetings. The study uses a digital MEMS microphone array in an instrumented meeting room to capture audio signals, and evaluates the diarisation error rate under different SNR conditions, with and without analogue noise reduction, and using delay-sum and superdirective beamforming schemes. The results show that the superdirective beamforming scheme outperforms the delay-sum beamformer in terms of diarisation error rate, and that the performance of both beamforming schemes is affected by the SNR level. The study concludes that beamforming techniques can significantly improve speaker diarisation in meetings, and that superdirective beamforming is particularly effective in the presence of analogue noise. This work provides insights into the use of beamforming techniques and SNR for diarisation systems, and can be useful for researchers and practitioners in the field of speech processing.",1
515,"this paper examines the effect of sensor performance on speaker diarisation in meetings and investigates the use of more advanced beamforming techniques , beyond the typically employed delay-sum beamformer , for mitigating the effects of poorer sensor performance . we present super-directive beamforming and investigate how different time difference of arrival smoothing and beamforming techniques influence the performance of state-of-the-art diar-isation systems . we produced and transcribed a new corpus of meetings recorded in the instrumented meeting room using a high snr analogue and a newly developed low snr digital mems microphone array -lrb- dmma .2 -rrb- . this research demonstrates that delay-sum beamformer has a significant effect on the diarisation error rate and that simple noise reduction and beamforming schemes suffice to overcome audio signal degradation due to the lower snr of modern mems microphones . index terms -- speaker diarisation in meetings , digital mems microphone array , time difference of arrival -lrb- tdoa -rrb- , superdirective beamforming",0
516,"This paper proposes an incremental low-rank least-squares temporal difference (LSTD) algorithm, called Incremental Truncated LSTD, to address the high computational and storage complexity of LSTD in high-dimensional domains. The proposed algorithm employs a truncated low-rank approximation to reduce the computational complexity of the LSTD update step. The rank parameter of the approximation is adaptively adjusted based on an energy allocation scheme, which balances the bias-variance trade-off. The proposed algorithm achieves linear time complexity and low memory requirements. Experimental results show that Incremental Truncated LSTD outperforms existing LSTD algorithms in terms of computational and sample efficiency, and it is suitable for large-scale reinforcement learning problems with high-dimensional state and action spaces.",1
517,"balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning . temporal difference learning algorithms stochastically update the value function , with a linear time complexity in the number of features , whereas least-squares temporal difference algorithms are sample efficient but can be quadratic in the number of features . in this work , we develop an efficient incremental low-rank lstd -lrb- -rrb- algorithm that progresses towards the goal of better balancing computation and sample efficiency . the incremental low-rank lstd -lrb- -rrb- algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample efficiency of lstd . we derive a simulation bound on the solution given by truncated low-rank approximation , illustrating a bias-variance trade-off dependent on the choice of rank . we demonstrate that the incremental low-rank lstd -lrb- -rrb- algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain .",0
518,"This paper presents a quantitative evaluation of Explanation-Based Learning (EBL) as an optimization tool for a large-scale natural language system, specifically the SRI Core Language Engine. The study uses the ATIS corpus and measures the total processing time of the system with and without EBL. The results show that EBL significantly reduces the processing time, demonstrating its effectiveness as a machine learning technique for optimizing natural language systems. This study provides insights into the benefits of EBL and its potential applications for improving the efficiency of large-scale natural language processing systems.",1
519,"this paper describes the application of explanation-based learning , a machine learning technique , to the sri core language engine , a large scale general purpose natural language analysis system . the idea is to bypass normal morphological , syntactic and -lrb- partly -rrb- semantic processing , for most input sentences , instead using a set of learned machine learning technique . explanation-based learning is used to extract the learned machine learning technique automatically from sample sentences submitted by a user and thus tune the system for that particular user . by indexing the learned machine learning technique efficiently , it is possible to achieve dramatic speed-ups . performance measurements were carried out using a training set of 1500 sentences and a separate test set of 100 sentences , all from the atis corpus . a set of 680 learned machine learning technique was derived from the training set . these machine learning technique covered 90 percent of the test sentences and reduced the total processing time to a third . an overall speed-up of 50 percent was accomplished using a set of only 250 learned machine learning technique .",0
520,"This paper proposes an integrated approach to speech enhancement and coding in the time-frequency domain. The method combines a wavelet packet transform algorithm with a subtractive-type enhancement algorithm and a rough masking model to improve the quality of speech in noisy environments. The proposed method incorporates masking threshold constraints to optimize speech enhancement performance. The time-frequency transform domain is used for processing speech coefficients, which are then quantized to reduce quantization noise. Additionally, auditory modeling is employed to better understand the auditory spectrum and improve the overall quality of the speech signal. Experimental results show that the proposed method outperforms traditional speech enhancement and coding methods in terms of both subjective and objective quality measures.",1
521,"this paper addresses the problem of merging speech enhancement and coding in the context of an auditory modeling . the noisy signal is rst processed by a fast wavelet packet transform algorithm to obtain an auditory spectrum , from which a rough masking model is estimated . then , this model is used to rene a subtractive-type enhancement algorithm . the enhanced speech coecients are then encoded in the same time-frequency transform domain using masking threshold constraints for quantization noise . the advantage of the proposed method is that both enhancement and coding are performed with the transform coecients , without making use of the additional fft processing .",0
522,"This paper presents the development of a bilingual automatic speech recognition (ASR) system for the MediaParl corpus, which contains accented speech and reverberant recordings. The ASR system is based on frequency domain linear prediction-based features and employs both tandem and hybrid acoustic modeling approaches, including bilingual deep neural networks. The proposed system is compared to language-specific ASR systems and shown to outperform them on accented speech. The bilingual deep neural networks are shown to be effective for both tandem and hybrid acoustic modeling approaches. Additionally, an entropy-based decoding-graph selection method is used to improve the ASR system's performance on the reverberant recordings. Experimental results show that the proposed bilingual ASR system achieves significant improvements in word error rates over the baseline ASR system.",1
523,"the development of an automatic speech recognition system for the bilingual mediaparl corpus is challenging for several reasons : -lrb- 1 -rrb- reverberant recordings , -lrb- 2 -rrb- accented speech , and -lrb- 3 -rrb- no prior information about the language . in that context , we employ frequency domain linear prediction-based features to reduce the effect of reverberation , exploit bilingual deep neural networks applied in tandem and hybrid acoustic modeling approaches to significantly improve automatic speech recognition system for ac-cented speech and develop a fully automatic speech recognition system using entropy-based decoding-graph selection . our experiments indicate that the proposed automatic speech recognition system performs similar to a language-specific asr system if approximately five seconds of speech are available .",0
524,"This paper introduces a new quality measure for topic segmentation of both text and speech. The proposed quality measure evaluates the effectiveness of topic segmentation algorithms by comparing their output to a baseline one-best topic model. The measure is based on relative error reduction and takes into account the topicality information of the content, which can be used for content navigation. The proposed measure is evaluated on large multimedia collections, including speech recognition lattices and spoken language lattices, and is shown to be effective in improving the transcription quality of speech segments. Experimental results demonstrate that the proposed measure outperforms existing measures in terms of relative error reduction and can effectively evaluate the quality of topic segmentation algorithms. The proposed measure has potential applications in various domains, such as content-based multimedia retrieval and automatic summarization.",1
525,"the recent proliferation of large multimedia collections has gathered immense attention from the speech research community , because speech recognition enables the transcription and indexing of such large multimedia collections . topicality information can be used to improve transcription quality and enable content navigation . in this paper , we give a novel quality measure for topic segmen-tation algorithms that improves over previously used measures . our quality measure takes into account not only the presence or absence of topic boundaries but also the content of the text or speech segments labeled as topic-coherent . additionally , we demonstrate that topic segmentation quality of spoken language can be improved using speech recognition lattices . using lattices , improvements over the baseline one-best topic model are observed when measured with the previously existing topic segmentation quality measure , as well as the new quality measure proposed in this paper -lrb- 9.4 % and 7.0 % relative error reduction , respectively -rrb- .",0
526,"This paper presents a method for region extraction from multiple images using a global minimization of an energy functional. The proposed approach employs a polynomial time graph algorithm to minimize the energy functional, which is defined based on the intensity values and high intensity gradients of the images. The energy functional is designed to identify regions based on their disparity map and optical flow discontinuities. The proposed method also utilizes dense optical flow to identify regions in the multi-dimensional space. The ratio form of the energy functional is used as a heuristic to guide the region identification process. The proposed approach is evaluated on multiple image datasets and is shown to be effective in extracting regions from complex scenes. The experimental results demonstrate that the proposed approach outperforms existing methods in terms of accuracy and computational efficiency. The proposed method has potential applications in various domains, such as computer vision, image and video processing, and object recognition.",1
527,"we present a method for region identification in multiple images . a set of regions in different images and the correspondences on their boundaries can be thought of as a boundary in the multi-dimensional space formed by the product of the individual image domains . we minimize an energy functional on the space of such boundaries , thereby identifying simultaneously both the optimal regions in each image and the optimal correspondences on their boundaries . we use a ratio form for the energy functional , thus enabling the global minimization of the energy functional using a polynomial time graph algorithm , among other desirable properties . we choose a simple form for this energy that favours boundaries that lie on high intensity gradients in each image , while encouraging correspondences between boundaries in different images that match intensity values . the latter tendency is weighted by a novel heuristic energy that encourages the boundaries to lie on disparity or optical flow discontinuities , although no dense optical flow or disparity map is computed .",0
528,"This paper introduces the Distribution Family of Similarity Distances (DFSD) for assessing similarity between features in object and scene recognition algorithms. The proposed approach defines a distribution of distances based on feature values extracted from images. The DFSD approach utilizes various similarity functions, such as l-p norms, to compute the distances between feature vectors. The proposed method also accounts for the distribution of feature values in the images to improve the accuracy of the similarity measures. The DFSD approach is evaluated on several scene categorization tasks and compared with existing similarity measures. The experimental results demonstrate that the proposed approach outperforms existing methods in terms of accuracy and robustness to noise and outliers. The proposed method has potential applications in various domains, such as computer vision, image and video processing, and object recognition.",1
529,"assessing similarity between features is a key step in object recognition and scene categorization tasks . we argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not . intuitively one would expect that similarities between features could arise from any distribution . in this paper , we will derive the contrary , and report the theoretical result that l p-norms -- a class of commonly applied distance metrics -- from one feature vector to other vectors are weibull-distributed if the feature values are correlated and non-identically distributed . besides these assumptions being realistic for images , we experimentally show them to hold for various popular feature extraction algorithms , for a diverse range of images . this fundamental insight opens new directions in the assessment of feature similarity , with projected improvements in object and scene recognition algorithms .",0
530,"This paper proposes a new algorithm for computing upper bounds in functional E-MAJSAT problems using a d-DNNF representation. The approach is based on a branch-and-bound solver and a map solver, which work together to compute the upper bounds. The algorithm uses a compilation language to convert the functional E-MAJSAT problem into a search tree, and then computes the upper bounds by traversing the search tree using the d-DNNF representation. The proposed approach is compared to a baseline branch-and-bound algorithm, and the experimental results show that the new algorithm outperforms the baseline on several benchmarks. The proposed algorithm can be used in probabilistic conformant planning, where functional E-MAJSAT problems arise frequently.",1
531,"we present a new algorithm for computing upper bounds for an optimization version of the e-majsat problem called functional e-majsat . the algorithm utilizes the compilation language d-dnnf which underlies several state-of-the-art algorithms for solving related problems . this bound computation can be used in a branch-and-bound solver for solving functional e-majsat . we then present a technique for pruning values from the branch-and-bound search tree based on the information available after each bound computation . we evaluated the proposed techniques in a map solver and a probabilistic conformant planner . in both cases , our experiments showed that the new techniques improved the efficiency of state-of-the-art solvers by orders of magnitude .",0
532,"This paper proposes a new clustering algorithm called ""Clustering via Concave Minimization."" The algorithm aims to cluster data points by minimizing a piecewise-linear concave function that is computed using a closed-form bilinear program. The algorithm is compared to the k-median algorithm and evaluated on real-world databases such as the Wisconsin Diagnostic Breast Cancer Database and the Wisconsin Prognostic Breast Cancer Database. The k-median algorithm is known for its correctness, but this new algorithm is shown to have faster convergence rates and produces better clustering results. The use of polyhedral sets and survival curves is also discussed in the context of clustering.",1
533,"the problem of assigning m points in the n-dimensional real space r n to k clusters is formulated as that of determining k centers in r n such that the sum of distances of each point to the nearest center is minimized . if a polyhedral distance is used , the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program : minimizing a bilinear function on a polyhe-dral set . a fast nite k-median algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program . computational testing on a number of real-world databases was carried out . on the wisconsin diagnostic breast cancer database , k-median training set correct-ness was comparable to that of the k-median algorithm , however its testing set correctness was better . additionally , on the wisconsin prognostic breast cancer database , distinct and clinically important survival curves were extracted by the k-median algorithm , whereas the k-median algorithm failed to obtain such distinct survival curves for the same database .",0
534,"This paper proposes a new biological sequence querying approach based on time-frequency methods. While the widely-accepted BLAST alignment approach and gapped query-based alignment methods have limitations in handling low-complexity regions and repetitive segments, the proposed approach employs highly-localized basis functions in the time-frequency plane to improve the significance of sub-sequence pre-processing. The paper demonstrates the effectiveness of this approach and compares it to existing methods. The results suggest that the proposed approach outperforms existing methods in terms of accuracy and speed, indicating the potential of time-frequency based querying for biological sequence analysis.",1
535,"we investigate the use of time-frequency methods to query biological sequences in search of regions of similarity or critical relationships among the sequences . existing querying approaches are insensitive to repeats , especially in low-complexity regions , and do not provide much support for efciently querying sub-sequences with inserts and deletes -lrb- or gaps -rrb- . our time-frequency methods uses highly-localized basis functions and multiple transformations in the tf plane to map characters in a sequence as well as different properties of a sub-sequence , such as its position in the sequence or number of gaps between sub-sequences . we analyze gapped query-based alignment methods using transformations in the tf plane while demonstrating the time-frequency methods 's possible operation in real-time without pre-processing . the time-frequency methods 's performance is compared to the widely-accepted blast alignment approach , and a signicance improvement is observed for queries with repetitive segments .",0
536,"This paper proposes a new saliency metric called Spatially Binned ROC (Receiver Operating Characteristic), which provides a comprehensive evaluation of saliency algorithms with spatial biases. The proposed method uses a graph-based visual saliency approach that incorporates dynamic visual attention and adaptive whitening to generate context-aware saliency maps. The method then computes the shuffled ROC metric by comparing the saliency maps against datasets of human fixations that have been randomly sampled. The shuffled ROC metric considers the central fixations and spatial biases of the human fixations to rank the saliency algorithms. The paper presents experimental results showing the effectiveness of the proposed metric in evaluating saliency algorithms, especially for those with spatial biases. The proposed method is expected to facilitate the development of saliency algorithms and their large-scale benchmarking.",1
537,"a recent trend in saliency algorithm development is large-scale benchmarking and algorithm ranking with ground truth provided by datasets of human fixations . in order to accommodate the strong bias humans have toward central fixations , shuffled roc metric is common to replace traditional roc metrics with a shuffled roc metric which uses randomly sampled fixations from other images in the database as the negative set . however , the shuffled roc introduces a number of problematic elements , including a fundamental assumption that shuffled roc metric is possible to separate visual salience and image spatial arrangement . we argue that shuffled roc metric is more informative to directly measure the effect of spatial bias on algorithm performance rather than try to correct for shuffled roc metric . to capture and quantify these known sources of bias , we propose a novel metric for measuring saliency algorithm performance : the spatially binned roc -lrb- sproc -rrb- . this metric provides direct insight into the spatial biases of a saliency algorithm without sacrificing the intuitive raw performance evaluation of traditional roc metrics . by quantitatively measuring the bias in saliency algorithms , researchers will be better equipped to select and optimize the most appropriate algorithm for a given task . we use a baseline measure of inherent algorithm bias to show that adaptive whitening saliency -lsb- 14 -rsb- , attention by information maximiza-tion -lsb- 8 -rsb- , and dynamic visual attention -lsb- 20 -rsb- provide the least spatially biased results , suiting them for tasks in which there is no information about the underlying spatial bias of the stimuli , whereas algorithms such as graph based visual saliency -lsb- 18 -rsb- and context-aware saliency -lsb- 15 -rsb- have a significant inherent central bias .",0
538,"In the field of image recognition and classification, the One-Shot similarity kernel has emerged as a powerful tool for generating a one-shot similarity score between two images. This kernel is a conditionally positive definite kernel that has been specifically designed to capture the similarities and differences between images. By using this one-shot similarity measure, it is possible to generate an image representation that is highly accurate and can be used for a variety of applications, including face recognition and multi-class identification. The descriptor generation process involves using the one-shot similarity score to identify key features of an image and then using an LDA classifier to generate a descriptor that accurately represents the image. The results of this study demonstrate the efficacy of the One-Shot similarity kernel in generating highly accurate image representations that can be used for a wide range of applications.",1
539,"the one-shot similarity measure has recently been introduced in the context of face recognition where one-shot similarity measure was used to produce state-of-the-art results . given two vectors , their one-shot similarity score reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of '' negative '' examples . the potential of this approach has thus far been largely un-explored . in this paper we analyze the one-shot score and show that : -lrb- 1 -rrb- when using a version of lda as the underlying classifier , this score is a conditionally positive definite kernel and may be used within kernel-methods -lrb- e.g. , svm -rrb- , -lrb- 2 -rrb- one-shot similarity measure can be efficiently computed , and -lrb- 3 -rrb- that one-shot similarity measure is effective as an underlying mechanism for image representation . we further demonstrate the effectiveness of the one-shot similarity score in a number of applications including multi-class identification and descriptor generation .",0
540,"Multi-task learning and transfer learning are popular approaches to address data scarcity and improve the generalization of machine learning models. However, analyzing the risk and stability of such models is challenging due to the complex inter-task relations and lack of knowledge about the underlying data distribution. In this paper, we propose a nonparametric framework for risk and stability analysis of multi-task/transfer learning problems using simulated and real data. Our framework is based on a reweighting matrix that captures the source domain task relations and smoothness assumptions. We demonstrate the effectiveness of our approach by applying it to various multi-task/transfer learning problems and comparing it to state-of-the-art methods. Our results show that our nonparametric framework can accurately estimate the risk and stability of multi-task/transfer learning models and outperforms existing approaches. This work provides a new perspective on analyzing the performance of multi-task/transfer learning models and can potentially improve the design and evaluation of such models.",1
541,"multi-task learning attempts to simultaneously leverage data from multiple domains in order to estimate related functions on each domain . for example , a special case of multi-task learning , transfer learning , is often employed when one has a good estimate of a function on a source domain , but is unable to estimate a related function well on a target domain using only target data . multi-task/transfer learning problems are usually solved by imposing some kind of '' smooth '' relationship among/between tasks . in this paper , we study how different smoothness assumptions on task relations affect the upper bounds of algorithms proposed for these multi-task/transfer learning problems under different settings . for general multi-task learning , we study a family of algorithms which utilize a reweighting matrix on task weights to capture the smooth relationship among tasks , which has many instantiations in existing literature . furthermore , for multi-task learning in a transfer learning framework , we study the recently proposed algorithms for the '' model shift '' , where the conditional distribution p -lrb- y | x -rrb- is allowed to change across tasks but the change is assumed to be smooth . in addition , we illustrate our results with experiments on both simulated and real data .",0
542,"Fully observable nondeterministic planning is a challenging problem in artificial intelligence that requires finding a sequence of actions to achieve a goal in an uncertain environment. In this paper, we explore the use of symmetry reduction techniques to improve the efficiency of the Fast Downward planner and the LAO ⇤ algorithm for solving such planning problems. Specifically, we investigate the role of structural symmetries in reducing the search space and improving the heuristic search for nondeterministic planning problems. We demonstrate the effectiveness of our approach by applying it to a range of classical planning and nondeterministic planning problems and comparing it to existing state-of-the-art methods. Our results show that the use of symmetry reduction techniques and structural symmetries can significantly improve the efficiency of heuristic search for planning problems, leading to faster and more accurate solutions. This work provides a new perspective on addressing the challenge of nondeterministic planning and demonstrates the potential of symmetry reduction techniques for improving the performance of AI planning algorithms.",1
543,"symmetry reduction has significantly contributed to the success of classical planning as heuristic search . however , it is an open question if symmetry reduction techniques can be lifted to fully observable nondeterministic planning . we generalize the concepts of structural symmetries and symmetry reduction to fond planning and specifically to the lao ⇤ algorithm . our base implementation of lao ⇤ algorithm in the fast downward planner is competitive with the lao ⇤ algorithm - based fond planner mynd . our experiments further show that symmetry reduction can yield strong performance gains compared to our base implementation of lao ⇤ algorithm .",0
544,"Low bitrate audio coding is a challenging problem in audio signal processing that requires compressing multi-channel sources into a low bitrate coding scheme without sacrificing sound quality. In this paper, we propose a novel low bitrate coding method using generalized adaptive gain shape vector quantization across channels to mitigate the loss of higher frequencies and spatial image. Our method is designed to handle severe truncation of the side channel and coded channels while maintaining a low decoder complexity. We demonstrate the effectiveness of our approach by comparing it to existing low bitrate audio codecs and showing that it achieves a high level of sound quality, with less muffled sound and less spectrum truncation. Furthermore, we show that our method is able to maintain the stereo content of the audio signal while reducing redundancy in the low frequencies. Our results suggest that the proposed method can be used as a promising alternative to existing low bitrate coding methods for multi-channel audio signals. This work provides a new perspective on low bitrate audio coding and can potentially contribute to the development of better audio codecs for various applications.",1
545,"audio coding at low bitrates suffers from artifacts due to spectrum truncation . typical audio codecs code multi-channel sources using transforms across the channels to remove redundancy such as middle -lrb- mid -rrb- - side -lrb- m/s -rrb- coding . at low bitrates , the spectrum of the coded channels is truncated and the spectrum of the channels with lower energy , such as the side channel , is truncated severely , sometimes entirely . this results in a muffled sound due to truncation of all coded channels beyond a certain frequency . it also results in a loss of spatial image even at low frequencies due to severe truncation of the side channel . previously we have developed a low bitrate coding method to combat the loss of higher frequencies caused by spectrum truncation . in this paper , we present a novel low bitrate audio coding scheme to mitigate the loss of spatial image . listening tests show that the combination of the two low bitrate coding methods results in a audio codec that can get good quality even at bitrates as low as 32kbps for stereo content with low decoder complexity .",0
546,"Sphere decoding is a well-known technique for decoding signals transmitted over multiple-input and multiple-output channels. One of the key challenges in sphere decoding is controlling the search radius of candidate lattice points while maintaining low computational complexity. In this paper, we propose a dynamic radius update strategy and an independent radius selection scheme to control the search radius of the tree-pruned sphere decoding algorithm. Specifically, we investigate the interplay between candidate lattice point search radius and performance, and propose a novel radius control strategy that allows for negligible performance penalty while significantly reducing computational complexity. We evaluate the proposed radius control strategy on various lattice-based decoding problems and show that it outperforms existing radius control methods in terms of computational complexity and decoding accuracy. Our results demonstrate the effectiveness of the proposed dynamic radius update strategy and independent radius selection scheme in reducing the complexity of sphere decoding while maintaining high decoding accuracy. This work provides a new perspective on radius control in sphere decoding and can potentially contribute to the development of better decoding algorithms for various applications.",1
547,"in this paper , we propose a novel radius control strategy for sphere decoding referred to as inter search radius control that provides further improvement of the computational complexity with minimal extra cost and negligible performance penalty . the proposed radius control strategy focuses on the sphere radius control strategy when a candidate lattice point is found . for this purpose , the dynamic radius update strategy as well as the lattice independent radius selection scheme are jointly exploited . from simulations in multiple-input and multiple-output channels , it is shown that the proposed radius control strategy provides a substantial improvement in complexity with near-ml performance .",0
548,"This paper proposes a speaker verification method that integrates dynamic and static features using a subspace method. The method uses robust normalization of speech feature variations and applies principal component analysis to create a speaker eigenspace. By combining dynamic and static features, the method achieves robust speaker verification. Phonetic information is also incorporated to improve speaker information. The proposed method is evaluated using speech data and a Gaussian mixture model (GMM) for speaker recognition. The results demonstrate that the subspace method and principal component analysis are effective for speaker verification, and that the integration of dynamic and static features improves performance without increasing computational complexity.",1
549,"in speaker recognition , it is a problem that variation of speech features is caused by sentences and time difference . speech data includes a phonetic information and a speaker information . if they are separated each other , robust speaker verification will be realized by using only the speaker information . however , it is difficult to separate the speaker information from the phonetic information included in speech data at present . from this viewpoint , we propose a speaker verification method using a subspace method based on principal component analysis in order to extract only the speaker information included in speech data . we also propose dynamic and static features of each speaker presented in the speaker eigenspace as well as their integration for robust normalization of speech feature variations . we carried out comparative experiments between the proposed speaker verification method and conventional gmm to show an effectiveness of our proposed speaker verification method . as a result , integrated dynamic and static features in speaker eigenspace were shown to be effective for speaker verification .",0
550,"This paper proposes a new approach for theory-guided induction of logic programs, based on the inference of regular languages. The approach involves the use of resolution-based techniques, recursive clauses, base clauses, and covering techniques, to generate a logic program that can be represented as a finite-state automaton. The paper presents a detailed theoretical analysis of the approach, including the use of resolvents and generalization techniques. The proposed method is evaluated on a set of benchmark problems, demonstrating its effectiveness in inducing accurate logic programs. The results show that the proposed approach outperforms other state-of-the-art methods for theory-guided induction of logic programs, providing a promising direction for future research in this area.",1
551,"previous resolution-based approaches to theory-guided induction of logic programs produce hypotheses in the form of a set of re-solvents of a theory , where the resolvents represent allowed sequences of resolution steps for the initial theory . there are , however , many characterizations of allowed sequences of resolution steps that can not be expressed by a set of resolvents . one approach to this theory-guided induction of logic programs is presented , the system mer-lin , which is based on an earlier technique for learning nite-state automata that represent allowed sequences of resolution steps . merlin extends the previous technique in three ways : i -rrb- negative examples are considered in addition to positive examples , ii -rrb- a new strategy for performing generalization is used , and iii -rrb- a technique for converting the learned automaton to a logic program is included . results from experiments are presented in which merlin outperforms both a system using the old strategy for performing generalization , and a traditional covering technique . the latter result can be explained by the limited expressiveness of hypotheses produced by covering and also by the fact that covering needs to produce the correct base clauses for a recursive denition before producing the recursive clauses . merlin on the other hand does not require that particular examples of the base cases are given , since both base clauses and recursive clauses can be inferred from a single example .",0
552,"This paper presents a fast semidefinite programming (SDP) approach to solving binary quadratic programs (BQPs) which arise in many computer vision problems such as image segmentation, co-segmentation, clustering, and registration. The proposed approach formulates BQPs as SDPs, allowing for the use of spectral methods and relaxation techniques to find approximate solutions. The dual optimization approach is used to speed up the computation, and the proposed method is shown to be effective for large-scale BQPs. The computational complexity of the proposed method is evaluated and compared to other SDP formulations. The paper also shows the effectiveness of the proposed approach in solving co-segmentation and clustering problems in computer vision.",1
553,"many computer vision problems can be formulated as binary quadratic programs . two classic relaxation methods are widely used for solving binary quadratic programs , namely , spectral methods and semidefinite programming , each with their own advantages and disadvantages . binary quadratic programs is simple and easy to implement , but its bound is loose . binary quadratic programs has a tighter bound , but its computational complexity is high for large scale problems . we present a new sdp formulation for binary quadratic programs , with two desirable properties . first , sdp formulation has a similar relaxation bound to conventional sdp formulations . second , compared with conventional sdp formulations , the new sdp formulation leads to a significantly more efficient and scalable dual optimization approach , which has the same degree of complexity as spectral methods . extensive experiments on various applications including clustering , image segmen-tation , co-segmentation and registration demonstrate the usefulness of our sdp formulation for solving large-scale bqps .",0
554,"In this paper, we propose a VLSI network of spiking neurons for learning to classify complex patterns using the perceptron learning rule. Our network consists of integrate-and-fire neurons with bistable synapses, which are capable of exhibiting complex dynamics such as oscillations and chaotic behavior. We introduce plastic synapses with local spike timing-dependent plasticity to adjust the synaptic weights and learn the pattern classifications. Our proposed VLSI network demonstrates efficient learning of complex patterns of mean firing rates with high accuracy. The experimental results show that the proposed network can classify complex patterns with significantly higher accuracy compared to traditional classifiers. Our approach provides a promising alternative to existing learning methods for complex pattern recognition in applications such as image and speech recognition.",1
555,"we propose a compact , low power vlsi network of spiking neurons which can learn to classify complex patterns of mean firing rates on -- line and in real -- time . the network of integrate-and-fire neurons is connected by bistable synapses that can change their weight using a local spike -- based plasticity mechanism . learning is supervised by a teacher which provides an extra input to the output neurons during training . the synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher -lrb- as in the perceptron learning rule -rrb- . we present experimental results that demonstrate how this vlsi network is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates .",0
556,"The paper proposes a novel method for recovering the surface geometry of an object from its apparent contours using the cusps in the contours. The method exploits the spatio-temporal derivatives of the contours and the epipolar parametrization of viewer motion to compute the nonsingular apparent contours. From the nonsingular contours, the Gauss curvature formulae are used to derive the geometry of the surface. The proposed method can handle large variations in object motion and provides accurate surface reconstruction. The paper demonstrates the effectiveness of the method on various real-world datasets.",1
557,"it is known that the deformations of the apparent contours of a surface under perspective projection and viewer motion enable the recovery of the geometry of the surface , for example by utilising the epipolar parametrization . these methods break down with apparent contours that are singular i.e. with cusps . in this paper we study this situation in detail and show how , nevertheless , the surface geometry -lrb- including the gauss curvature and mean curvature of the surface -rrb- can be recovered by following the cusps . indeed the for-mulae are much simpler in this case and require lower spatio-temporal derivatives than in the general case of nonsingular apparent contours . we give a simulated example , and also show that following cusps does not by itself provide us with information on ego-motion .",0
558,"In the field of image recognition and classification, the One-Shot similarity kernel has emerged as a powerful tool for generating a one-shot similarity score between two images. This kernel is a conditionally positive definite kernel that has been specifically designed to capture the similarities and differences between images. By using this one-shot similarity measure, it is possible to generate an image representation that is highly accurate and can be used for a variety of applications, including face recognition and multi-class identification. The descriptor generation process involves using the one-shot similarity score to identify key features of an image and then using an LDA classifier to generate a descriptor that accurately represents the image. The results of this study demonstrate the efficacy of the One-Shot similarity kernel in generating highly accurate image representations that can be used for a wide range of applications.",1
559,"the one-shot similarity measure has recently been introduced in the context of face recognition where one-shot similarity measure was used to produce state-of-the-art results . given two vectors , their one-shot similarity score reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of '' negative '' examples . the potential of this approach has thus far been largely un-explored . in this paper we analyze the one-shot score and show that : -lrb- 1 -rrb- when using a version of lda as the underlying classifier , this score is a conditionally positive definite kernel and may be used within kernel-methods -lrb- e.g. , svm -rrb- , -lrb- 2 -rrb- one-shot similarity measure can be efficiently computed , and -lrb- 3 -rrb- that one-shot similarity measure is effective as an underlying mechanism for image representation . we further demonstrate the effectiveness of the one-shot similarity score in a number of applications including multi-class identification and descriptor generation .",0
560,"Multi-task learning and transfer learning are popular approaches to address data scarcity and improve the generalization of machine learning models. However, analyzing the risk and stability of such models is challenging due to the complex inter-task relations and lack of knowledge about the underlying data distribution. In this paper, we propose a nonparametric framework for risk and stability analysis of multi-task/transfer learning problems using simulated and real data. Our framework is based on a reweighting matrix that captures the source domain task relations and smoothness assumptions. We demonstrate the effectiveness of our approach by applying it to various multi-task/transfer learning problems and comparing it to state-of-the-art methods. Our results show that our nonparametric framework can accurately estimate the risk and stability of multi-task/transfer learning models and outperforms existing approaches. This work provides a new perspective on analyzing the performance of multi-task/transfer learning models and can potentially improve the design and evaluation of such models.",1
561,"multi-task learning attempts to simultaneously leverage data from multiple domains in order to estimate related functions on each domain . for example , a special case of multi-task learning , transfer learning , is often employed when one has a good estimate of a function on a source domain , but is unable to estimate a related function well on a target domain using only target data . multi-task/transfer learning problems are usually solved by imposing some kind of '' smooth '' relationship among/between tasks . in this paper , we study how different smoothness assumptions on task relations affect the upper bounds of algorithms proposed for these multi-task/transfer learning problems under different settings . for general multi-task learning , we study a family of algorithms which utilize a reweighting matrix on task weights to capture the smooth relationship among tasks , which has many instantiations in existing literature . furthermore , for multi-task learning in a transfer learning framework , we study the recently proposed algorithms for the '' model shift '' , where the conditional distribution p -lrb- y | x -rrb- is allowed to change across tasks but the change is assumed to be smooth . in addition , we illustrate our results with experiments on both simulated and real data .",0
562,"Fully observable nondeterministic planning is a challenging problem in artificial intelligence that requires finding a sequence of actions to achieve a goal in an uncertain environment. In this paper, we explore the use of symmetry reduction techniques to improve the efficiency of the Fast Downward planner and the LAO ⇤ algorithm for solving such planning problems. Specifically, we investigate the role of structural symmetries in reducing the search space and improving the heuristic search for nondeterministic planning problems. We demonstrate the effectiveness of our approach by applying it to a range of classical planning and nondeterministic planning problems and comparing it to existing state-of-the-art methods. Our results show that the use of symmetry reduction techniques and structural symmetries can significantly improve the efficiency of heuristic search for planning problems, leading to faster and more accurate solutions. This work provides a new perspective on addressing the challenge of nondeterministic planning and demonstrates the potential of symmetry reduction techniques for improving the performance of AI planning algorithms.",1
563,"symmetry reduction has significantly contributed to the success of classical planning as heuristic search . however , it is an open question if symmetry reduction techniques can be lifted to fully observable nondeterministic planning . we generalize the concepts of structural symmetries and symmetry reduction to fond planning and specifically to the lao ⇤ algorithm . our base implementation of lao ⇤ algorithm in the fast downward planner is competitive with the lao ⇤ algorithm - based fond planner mynd . our experiments further show that symmetry reduction can yield strong performance gains compared to our base implementation of lao ⇤ algorithm .",0
564,"Low bitrate audio coding is a challenging problem in audio signal processing that requires compressing multi-channel sources into a low bitrate coding scheme without sacrificing sound quality. In this paper, we propose a novel low bitrate coding method using generalized adaptive gain shape vector quantization across channels to mitigate the loss of higher frequencies and spatial image. Our method is designed to handle severe truncation of the side channel and coded channels while maintaining a low decoder complexity. We demonstrate the effectiveness of our approach by comparing it to existing low bitrate audio codecs and showing that it achieves a high level of sound quality, with less muffled sound and less spectrum truncation. Furthermore, we show that our method is able to maintain the stereo content of the audio signal while reducing redundancy in the low frequencies. Our results suggest that the proposed method can be used as a promising alternative to existing low bitrate coding methods for multi-channel audio signals. This work provides a new perspective on low bitrate audio coding and can potentially contribute to the development of better audio codecs for various applications.",1
565,"audio coding at low bitrates suffers from artifacts due to spectrum truncation . typical audio codecs code multi-channel sources using transforms across the channels to remove redundancy such as middle -lrb- mid -rrb- - side -lrb- m/s -rrb- coding . at low bitrates , the spectrum of the coded channels is truncated and the spectrum of the channels with lower energy , such as the side channel , is truncated severely , sometimes entirely . this results in a muffled sound due to truncation of all coded channels beyond a certain frequency . it also results in a loss of spatial image even at low frequencies due to severe truncation of the side channel . previously we have developed a low bitrate coding method to combat the loss of higher frequencies caused by spectrum truncation . in this paper , we present a novel low bitrate audio coding scheme to mitigate the loss of spatial image . listening tests show that the combination of the two low bitrate coding methods results in a audio codec that can get good quality even at bitrates as low as 32kbps for stereo content with low decoder complexity .",0
566,"Sphere decoding is a well-known technique for decoding signals transmitted over multiple-input and multiple-output channels. One of the key challenges in sphere decoding is controlling the search radius of candidate lattice points while maintaining low computational complexity. In this paper, we propose a dynamic radius update strategy and an independent radius selection scheme to control the search radius of the tree-pruned sphere decoding algorithm. Specifically, we investigate the interplay between candidate lattice point search radius and performance, and propose a novel radius control strategy that allows for negligible performance penalty while significantly reducing computational complexity. We evaluate the proposed radius control strategy on various lattice-based decoding problems and show that it outperforms existing radius control methods in terms of computational complexity and decoding accuracy. Our results demonstrate the effectiveness of the proposed dynamic radius update strategy and independent radius selection scheme in reducing the complexity of sphere decoding while maintaining high decoding accuracy. This work provides a new perspective on radius control in sphere decoding and can potentially contribute to the development of better decoding algorithms for various applications.",1
567,"in this paper , we propose a novel radius control strategy for sphere decoding referred to as inter search radius control that provides further improvement of the computational complexity with minimal extra cost and negligible performance penalty . the proposed radius control strategy focuses on the sphere radius control strategy when a candidate lattice point is found . for this purpose , the dynamic radius update strategy as well as the lattice independent radius selection scheme are jointly exploited . from simulations in multiple-input and multiple-output channels , it is shown that the proposed radius control strategy provides a substantial improvement in complexity with near-ml performance .",0
568,"This paper proposes a speaker verification method that integrates dynamic and static features using a subspace method. The method uses robust normalization of speech feature variations and applies principal component analysis to create a speaker eigenspace. By combining dynamic and static features, the method achieves robust speaker verification. Phonetic information is also incorporated to improve speaker information. The proposed method is evaluated using speech data and a Gaussian mixture model (GMM) for speaker recognition. The results demonstrate that the subspace method and principal component analysis are effective for speaker verification, and that the integration of dynamic and static features improves performance without increasing computational complexity.",1
569,"in speaker recognition , it is a problem that variation of speech features is caused by sentences and time difference . speech data includes a phonetic information and a speaker information . if they are separated each other , robust speaker verification will be realized by using only the speaker information . however , it is difficult to separate the speaker information from the phonetic information included in speech data at present . from this viewpoint , we propose a speaker verification method using a subspace method based on principal component analysis in order to extract only the speaker information included in speech data . we also propose dynamic and static features of each speaker presented in the speaker eigenspace as well as their integration for robust normalization of speech feature variations . we carried out comparative experiments between the proposed speaker verification method and conventional gmm to show an effectiveness of our proposed speaker verification method . as a result , integrated dynamic and static features in speaker eigenspace were shown to be effective for speaker verification .",0
570,"This paper proposes a new approach for theory-guided induction of logic programs, based on the inference of regular languages. The approach involves the use of resolution-based techniques, recursive clauses, base clauses, and covering techniques, to generate a logic program that can be represented as a finite-state automaton. The paper presents a detailed theoretical analysis of the approach, including the use of resolvents and generalization techniques. The proposed method is evaluated on a set of benchmark problems, demonstrating its effectiveness in inducing accurate logic programs. The results show that the proposed approach outperforms other state-of-the-art methods for theory-guided induction of logic programs, providing a promising direction for future research in this area.",1
571,"previous resolution-based approaches to theory-guided induction of logic programs produce hypotheses in the form of a set of re-solvents of a theory , where the resolvents represent allowed sequences of resolution steps for the initial theory . there are , however , many characterizations of allowed sequences of resolution steps that can not be expressed by a set of resolvents . one approach to this theory-guided induction of logic programs is presented , the system mer-lin , which is based on an earlier technique for learning nite-state automata that represent allowed sequences of resolution steps . merlin extends the previous technique in three ways : i -rrb- negative examples are considered in addition to positive examples , ii -rrb- a new strategy for performing generalization is used , and iii -rrb- a technique for converting the learned automaton to a logic program is included . results from experiments are presented in which merlin outperforms both a system using the old strategy for performing generalization , and a traditional covering technique . the latter result can be explained by the limited expressiveness of hypotheses produced by covering and also by the fact that covering needs to produce the correct base clauses for a recursive denition before producing the recursive clauses . merlin on the other hand does not require that particular examples of the base cases are given , since both base clauses and recursive clauses can be inferred from a single example .",0
572,"This paper presents a fast semidefinite programming (SDP) approach to solving binary quadratic programs (BQPs) which arise in many computer vision problems such as image segmentation, co-segmentation, clustering, and registration. The proposed approach formulates BQPs as SDPs, allowing for the use of spectral methods and relaxation techniques to find approximate solutions. The dual optimization approach is used to speed up the computation, and the proposed method is shown to be effective for large-scale BQPs. The computational complexity of the proposed method is evaluated and compared to other SDP formulations. The paper also shows the effectiveness of the proposed approach in solving co-segmentation and clustering problems in computer vision.",1
573,"many computer vision problems can be formulated as binary quadratic programs . two classic relaxation methods are widely used for solving binary quadratic programs , namely , spectral methods and semidefinite programming , each with their own advantages and disadvantages . binary quadratic programs is simple and easy to implement , but its bound is loose . binary quadratic programs has a tighter bound , but its computational complexity is high for large scale problems . we present a new sdp formulation for binary quadratic programs , with two desirable properties . first , sdp formulation has a similar relaxation bound to conventional sdp formulations . second , compared with conventional sdp formulations , the new sdp formulation leads to a significantly more efficient and scalable dual optimization approach , which has the same degree of complexity as spectral methods . extensive experiments on various applications including clustering , image segmen-tation , co-segmentation and registration demonstrate the usefulness of our sdp formulation for solving large-scale bqps .",0
574,"In this paper, we propose a VLSI network of spiking neurons for learning to classify complex patterns using the perceptron learning rule. Our network consists of integrate-and-fire neurons with bistable synapses, which are capable of exhibiting complex dynamics such as oscillations and chaotic behavior. We introduce plastic synapses with local spike timing-dependent plasticity to adjust the synaptic weights and learn the pattern classifications. Our proposed VLSI network demonstrates efficient learning of complex patterns of mean firing rates with high accuracy. The experimental results show that the proposed network can classify complex patterns with significantly higher accuracy compared to traditional classifiers. Our approach provides a promising alternative to existing learning methods for complex pattern recognition in applications such as image and speech recognition.",1
575,"we propose a compact , low power vlsi network of spiking neurons which can learn to classify complex patterns of mean firing rates on -- line and in real -- time . the network of integrate-and-fire neurons is connected by bistable synapses that can change their weight using a local spike -- based plasticity mechanism . learning is supervised by a teacher which provides an extra input to the output neurons during training . the synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher -lrb- as in the perceptron learning rule -rrb- . we present experimental results that demonstrate how this vlsi network is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates .",0
576,"The paper proposes a novel method for recovering the surface geometry of an object from its apparent contours using the cusps in the contours. The method exploits the spatio-temporal derivatives of the contours and the epipolar parametrization of viewer motion to compute the nonsingular apparent contours. From the nonsingular contours, the Gauss curvature formulae are used to derive the geometry of the surface. The proposed method can handle large variations in object motion and provides accurate surface reconstruction. The paper demonstrates the effectiveness of the method on various real-world datasets.",1
577,"it is known that the deformations of the apparent contours of a surface under perspective projection and viewer motion enable the recovery of the geometry of the surface , for example by utilising the epipolar parametrization . these methods break down with apparent contours that are singular i.e. with cusps . in this paper we study this situation in detail and show how , nevertheless , the surface geometry -lrb- including the gauss curvature and mean curvature of the surface -rrb- can be recovered by following the cusps . indeed the for-mulae are much simpler in this case and require lower spatio-temporal derivatives than in the general case of nonsingular apparent contours . we give a simulated example , and also show that following cusps does not by itself provide us with information on ego-motion .",0
578,"In the field of image recognition and classification, the One-Shot similarity kernel has emerged as a powerful tool for generating a one-shot similarity score between two images. This kernel is a conditionally positive definite kernel that has been specifically designed to capture the similarities and differences between images. By using this one-shot similarity measure, it is possible to generate an image representation that is highly accurate and can be used for a variety of applications, including face recognition and multi-class identification. The descriptor generation process involves using the one-shot similarity score to identify key features of an image and then using an LDA classifier to generate a descriptor that accurately represents the image. The results of this study demonstrate the efficacy of the One-Shot similarity kernel in generating highly accurate image representations that can be used for a wide range of applications.",1
579,"the one-shot similarity measure has recently been introduced in the context of face recognition where one-shot similarity measure was used to produce state-of-the-art results . given two vectors , their one-shot similarity score reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of '' negative '' examples . the potential of this approach has thus far been largely un-explored . in this paper we analyze the one-shot score and show that : -lrb- 1 -rrb- when using a version of lda as the underlying classifier , this score is a conditionally positive definite kernel and may be used within kernel-methods -lrb- e.g. , svm -rrb- , -lrb- 2 -rrb- one-shot similarity measure can be efficiently computed , and -lrb- 3 -rrb- that one-shot similarity measure is effective as an underlying mechanism for image representation . we further demonstrate the effectiveness of the one-shot similarity score in a number of applications including multi-class identification and descriptor generation .",0
580,"Multi-task learning and transfer learning are popular approaches to address data scarcity and improve the generalization of machine learning models. However, analyzing the risk and stability of such models is challenging due to the complex inter-task relations and lack of knowledge about the underlying data distribution. In this paper, we propose a nonparametric framework for risk and stability analysis of multi-task/transfer learning problems using simulated and real data. Our framework is based on a reweighting matrix that captures the source domain task relations and smoothness assumptions. We demonstrate the effectiveness of our approach by applying it to various multi-task/transfer learning problems and comparing it to state-of-the-art methods. Our results show that our nonparametric framework can accurately estimate the risk and stability of multi-task/transfer learning models and outperforms existing approaches. This work provides a new perspective on analyzing the performance of multi-task/transfer learning models and can potentially improve the design and evaluation of such models.",1
581,"multi-task learning attempts to simultaneously leverage data from multiple domains in order to estimate related functions on each domain . for example , a special case of multi-task learning , transfer learning , is often employed when one has a good estimate of a function on a source domain , but is unable to estimate a related function well on a target domain using only target data . multi-task/transfer learning problems are usually solved by imposing some kind of '' smooth '' relationship among/between tasks . in this paper , we study how different smoothness assumptions on task relations affect the upper bounds of algorithms proposed for these multi-task/transfer learning problems under different settings . for general multi-task learning , we study a family of algorithms which utilize a reweighting matrix on task weights to capture the smooth relationship among tasks , which has many instantiations in existing literature . furthermore , for multi-task learning in a transfer learning framework , we study the recently proposed algorithms for the '' model shift '' , where the conditional distribution p -lrb- y | x -rrb- is allowed to change across tasks but the change is assumed to be smooth . in addition , we illustrate our results with experiments on both simulated and real data .",0
582,"Fully observable nondeterministic planning is a challenging problem in artificial intelligence that requires finding a sequence of actions to achieve a goal in an uncertain environment. In this paper, we explore the use of symmetry reduction techniques to improve the efficiency of the Fast Downward planner and the LAO ⇤ algorithm for solving such planning problems. Specifically, we investigate the role of structural symmetries in reducing the search space and improving the heuristic search for nondeterministic planning problems. We demonstrate the effectiveness of our approach by applying it to a range of classical planning and nondeterministic planning problems and comparing it to existing state-of-the-art methods. Our results show that the use of symmetry reduction techniques and structural symmetries can significantly improve the efficiency of heuristic search for planning problems, leading to faster and more accurate solutions. This work provides a new perspective on addressing the challenge of nondeterministic planning and demonstrates the potential of symmetry reduction techniques for improving the performance of AI planning algorithms.",1
583,"symmetry reduction has significantly contributed to the success of classical planning as heuristic search . however , it is an open question if symmetry reduction techniques can be lifted to fully observable nondeterministic planning . we generalize the concepts of structural symmetries and symmetry reduction to fond planning and specifically to the lao ⇤ algorithm . our base implementation of lao ⇤ algorithm in the fast downward planner is competitive with the lao ⇤ algorithm - based fond planner mynd . our experiments further show that symmetry reduction can yield strong performance gains compared to our base implementation of lao ⇤ algorithm .",0
584,"Low bitrate audio coding is a challenging problem in audio signal processing that requires compressing multi-channel sources into a low bitrate coding scheme without sacrificing sound quality. In this paper, we propose a novel low bitrate coding method using generalized adaptive gain shape vector quantization across channels to mitigate the loss of higher frequencies and spatial image. Our method is designed to handle severe truncation of the side channel and coded channels while maintaining a low decoder complexity. We demonstrate the effectiveness of our approach by comparing it to existing low bitrate audio codecs and showing that it achieves a high level of sound quality, with less muffled sound and less spectrum truncation. Furthermore, we show that our method is able to maintain the stereo content of the audio signal while reducing redundancy in the low frequencies. Our results suggest that the proposed method can be used as a promising alternative to existing low bitrate coding methods for multi-channel audio signals. This work provides a new perspective on low bitrate audio coding and can potentially contribute to the development of better audio codecs for various applications.",1
585,"audio coding at low bitrates suffers from artifacts due to spectrum truncation . typical audio codecs code multi-channel sources using transforms across the channels to remove redundancy such as middle -lrb- mid -rrb- - side -lrb- m/s -rrb- coding . at low bitrates , the spectrum of the coded channels is truncated and the spectrum of the channels with lower energy , such as the side channel , is truncated severely , sometimes entirely . this results in a muffled sound due to truncation of all coded channels beyond a certain frequency . it also results in a loss of spatial image even at low frequencies due to severe truncation of the side channel . previously we have developed a low bitrate coding method to combat the loss of higher frequencies caused by spectrum truncation . in this paper , we present a novel low bitrate audio coding scheme to mitigate the loss of spatial image . listening tests show that the combination of the two low bitrate coding methods results in a audio codec that can get good quality even at bitrates as low as 32kbps for stereo content with low decoder complexity .",0
586,"Sphere decoding is a well-known technique for decoding signals transmitted over multiple-input and multiple-output channels. One of the key challenges in sphere decoding is controlling the search radius of candidate lattice points while maintaining low computational complexity. In this paper, we propose a dynamic radius update strategy and an independent radius selection scheme to control the search radius of the tree-pruned sphere decoding algorithm. Specifically, we investigate the interplay between candidate lattice point search radius and performance, and propose a novel radius control strategy that allows for negligible performance penalty while significantly reducing computational complexity. We evaluate the proposed radius control strategy on various lattice-based decoding problems and show that it outperforms existing radius control methods in terms of computational complexity and decoding accuracy. Our results demonstrate the effectiveness of the proposed dynamic radius update strategy and independent radius selection scheme in reducing the complexity of sphere decoding while maintaining high decoding accuracy. This work provides a new perspective on radius control in sphere decoding and can potentially contribute to the development of better decoding algorithms for various applications.",1
587,"in this paper , we propose a novel radius control strategy for sphere decoding referred to as inter search radius control that provides further improvement of the computational complexity with minimal extra cost and negligible performance penalty . the proposed radius control strategy focuses on the sphere radius control strategy when a candidate lattice point is found . for this purpose , the dynamic radius update strategy as well as the lattice independent radius selection scheme are jointly exploited . from simulations in multiple-input and multiple-output channels , it is shown that the proposed radius control strategy provides a substantial improvement in complexity with near-ml performance .",0
588,"This paper proposes a speaker verification method that integrates dynamic and static features using a subspace method. The method uses robust normalization of speech feature variations and applies principal component analysis to create a speaker eigenspace. By combining dynamic and static features, the method achieves robust speaker verification. Phonetic information is also incorporated to improve speaker information. The proposed method is evaluated using speech data and a Gaussian mixture model (GMM) for speaker recognition. The results demonstrate that the subspace method and principal component analysis are effective for speaker verification, and that the integration of dynamic and static features improves performance without increasing computational complexity.",1
589,"in speaker recognition , it is a problem that variation of speech features is caused by sentences and time difference . speech data includes a phonetic information and a speaker information . if they are separated each other , robust speaker verification will be realized by using only the speaker information . however , it is difficult to separate the speaker information from the phonetic information included in speech data at present . from this viewpoint , we propose a speaker verification method using a subspace method based on principal component analysis in order to extract only the speaker information included in speech data . we also propose dynamic and static features of each speaker presented in the speaker eigenspace as well as their integration for robust normalization of speech feature variations . we carried out comparative experiments between the proposed speaker verification method and conventional gmm to show an effectiveness of our proposed speaker verification method . as a result , integrated dynamic and static features in speaker eigenspace were shown to be effective for speaker verification .",0
590,"This paper proposes a new approach for theory-guided induction of logic programs, based on the inference of regular languages. The approach involves the use of resolution-based techniques, recursive clauses, base clauses, and covering techniques, to generate a logic program that can be represented as a finite-state automaton. The paper presents a detailed theoretical analysis of the approach, including the use of resolvents and generalization techniques. The proposed method is evaluated on a set of benchmark problems, demonstrating its effectiveness in inducing accurate logic programs. The results show that the proposed approach outperforms other state-of-the-art methods for theory-guided induction of logic programs, providing a promising direction for future research in this area.",1
591,"previous resolution-based approaches to theory-guided induction of logic programs produce hypotheses in the form of a set of re-solvents of a theory , where the resolvents represent allowed sequences of resolution steps for the initial theory . there are , however , many characterizations of allowed sequences of resolution steps that can not be expressed by a set of resolvents . one approach to this theory-guided induction of logic programs is presented , the system mer-lin , which is based on an earlier technique for learning nite-state automata that represent allowed sequences of resolution steps . merlin extends the previous technique in three ways : i -rrb- negative examples are considered in addition to positive examples , ii -rrb- a new strategy for performing generalization is used , and iii -rrb- a technique for converting the learned automaton to a logic program is included . results from experiments are presented in which merlin outperforms both a system using the old strategy for performing generalization , and a traditional covering technique . the latter result can be explained by the limited expressiveness of hypotheses produced by covering and also by the fact that covering needs to produce the correct base clauses for a recursive denition before producing the recursive clauses . merlin on the other hand does not require that particular examples of the base cases are given , since both base clauses and recursive clauses can be inferred from a single example .",0
592,"This paper presents a fast semidefinite programming (SDP) approach to solving binary quadratic programs (BQPs) which arise in many computer vision problems such as image segmentation, co-segmentation, clustering, and registration. The proposed approach formulates BQPs as SDPs, allowing for the use of spectral methods and relaxation techniques to find approximate solutions. The dual optimization approach is used to speed up the computation, and the proposed method is shown to be effective for large-scale BQPs. The computational complexity of the proposed method is evaluated and compared to other SDP formulations. The paper also shows the effectiveness of the proposed approach in solving co-segmentation and clustering problems in computer vision.",1
593,"many computer vision problems can be formulated as binary quadratic programs . two classic relaxation methods are widely used for solving binary quadratic programs , namely , spectral methods and semidefinite programming , each with their own advantages and disadvantages . binary quadratic programs is simple and easy to implement , but its bound is loose . binary quadratic programs has a tighter bound , but its computational complexity is high for large scale problems . we present a new sdp formulation for binary quadratic programs , with two desirable properties . first , sdp formulation has a similar relaxation bound to conventional sdp formulations . second , compared with conventional sdp formulations , the new sdp formulation leads to a significantly more efficient and scalable dual optimization approach , which has the same degree of complexity as spectral methods . extensive experiments on various applications including clustering , image segmen-tation , co-segmentation and registration demonstrate the usefulness of our sdp formulation for solving large-scale bqps .",0
594,"In this paper, we propose a VLSI network of spiking neurons for learning to classify complex patterns using the perceptron learning rule. Our network consists of integrate-and-fire neurons with bistable synapses, which are capable of exhibiting complex dynamics such as oscillations and chaotic behavior. We introduce plastic synapses with local spike timing-dependent plasticity to adjust the synaptic weights and learn the pattern classifications. Our proposed VLSI network demonstrates efficient learning of complex patterns of mean firing rates with high accuracy. The experimental results show that the proposed network can classify complex patterns with significantly higher accuracy compared to traditional classifiers. Our approach provides a promising alternative to existing learning methods for complex pattern recognition in applications such as image and speech recognition.",1
595,"we propose a compact , low power vlsi network of spiking neurons which can learn to classify complex patterns of mean firing rates on -- line and in real -- time . the network of integrate-and-fire neurons is connected by bistable synapses that can change their weight using a local spike -- based plasticity mechanism . learning is supervised by a teacher which provides an extra input to the output neurons during training . the synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher -lrb- as in the perceptron learning rule -rrb- . we present experimental results that demonstrate how this vlsi network is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates .",0
596,"The paper proposes a novel method for recovering the surface geometry of an object from its apparent contours using the cusps in the contours. The method exploits the spatio-temporal derivatives of the contours and the epipolar parametrization of viewer motion to compute the nonsingular apparent contours. From the nonsingular contours, the Gauss curvature formulae are used to derive the geometry of the surface. The proposed method can handle large variations in object motion and provides accurate surface reconstruction. The paper demonstrates the effectiveness of the method on various real-world datasets.",1
597,"it is known that the deformations of the apparent contours of a surface under perspective projection and viewer motion enable the recovery of the geometry of the surface , for example by utilising the epipolar parametrization . these methods break down with apparent contours that are singular i.e. with cusps . in this paper we study this situation in detail and show how , nevertheless , the surface geometry -lrb- including the gauss curvature and mean curvature of the surface -rrb- can be recovered by following the cusps . indeed the for-mulae are much simpler in this case and require lower spatio-temporal derivatives than in the general case of nonsingular apparent contours . we give a simulated example , and also show that following cusps does not by itself provide us with information on ego-motion .",0
598,"This paper proposes the application of co-training to improve the performance of reference resolution tasks without the need for manual labeling. Reference resolution is a challenging natural language processing task that involves identifying and linking noun phrases to their corresponding entities in a text. In this study, a co-training classifier is used to learn from unlabeled data and improve the performance of reference resolution. The results demonstrate that co-training can significantly improve the accuracy of reference resolution tasks, making it a promising approach for reducing the reliance on manual labeling in this field.",1
599,"in this paper , we investigate the practical applicability of co-training for the task of building a classifier for reference resolution . we are concerned with the question if co-training can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance .",0
600,"This paper proposes the use of recurrent neural network (RNN) predictors for EEG signal compression. EEG data recording is widely used in neurophysiology research and clinical applications, but the large amount of data generated makes storage and transmission difficult. In this study, a novel DPCM scheme is proposed to compress EEG signals using RNN predictors. The computational cost of the proposed scheme is lower than existing methods, and the compression performance is evaluated using EEG data. The results demonstrate that the proposed RNN-based predictors can effectively compress EEG signals while maintaining high signal quality. Additionally, a training strategy is proposed to optimize the performance of the RNN predictors. Overall, the proposed method shows promise for improving the efficiency and practicality of EEG data recording and storage.",1
601,the progress of digital electroencephalography gave rise to the problem of eeg data recording . in this paper a dpcm scheme for eeg data recording is discussed . in particular the performance of a class of pre-dictors based on recurrent neural networks is presented . the training strategy is accurately described and the results of a comparison with some other classical linear and static neural predictors are given . the proposed recurrent neural predictor demonstrates to be competitive with the others in ooering good performance at a very low computational cost .,0
602,"This paper presents an improved spoken query transcription system using co-occurrence information. The automatic transcription of voice queries is an important component of voice search and recognition applications, but it remains a challenging problem due to variations in the syntactic and grammatical structure of spoken language. In this study, a co-occurrence based approach is proposed to capture the relative accuracy of different keywords in spoken queries. A mobile web scoring function is then used to estimate the accuracy of the transcription at the co-occurrence level. The proposed approach is evaluated using a dataset of spoken queries, and the results demonstrate that it outperforms existing methods in terms of transcription accuracy. Additionally, the proposed approach is compared with a language modeling approach for voice search recognition, and it is shown that the co-occurrence based approach achieves comparable performance with lower computational cost. Overall, the proposed method shows promise for improving the accuracy and efficiency of speech applications that rely on automatic transcription of spoken queries.",1
603,"spoken queries are a natural medium for searching the mobile web . language modeling for voice search recognition offers different challenges compared to more conventional speech applications . the challenges arise from the fact that spoken queries are usually a set of keywords and do not have a syntactic and grammatical structure . this paper describes a co-occurrence based approach to improve the accuracy of voice queries automatic transcription . with the right choice of scoring function and co-occurrence level , we show that co-occurrence information gives a 2 % relative accuracy improvement over a state of the art system .",0
604,"This paper presents an analysis of the sample complexity and performance bounds for non-parametric approximate linear programming (ALP). ALP is a method for solving Markov decision processes (MDPs) with infinite action spaces by approximating the value function using a set of basis functions. In this study, the approximation architecture is based on a smoothness assumption for the value function, and the Bellman equation is used to obtain an approximate solution. The analysis considers the effect of noise on the ALP algorithm and shows that the method is robust to small amounts of noise. The sample complexity and performance bounds of the algorithm are also derived, providing theoretical guarantees for its effectiveness. The results demonstrate that non-parametric ALP can achieve good performance with a relatively small number of samples, making it a promising approach for real-world transitions where the number of possible actions is large or infinite. Overall, the study contributes to our understanding of the strengths and limitations of non-parametric ALP for solving MDPs with infinite action spaces.",1
605,"one of the most difficult tasks in value function approximation for markov decision processes is finding an approximation architecture that is expressive enough to capture the important structure in the value function , while at the same time not overfitting the training samples . recent results in non-parametric approximate linear programming , have demonstrated that this can be done effectively using nothing more than a smoothness assumption on the value function . in this paper we extend these results to the case where samples come from real world transitions instead of the full bellman equation , adding robustness to noise . in addition , we provide the first max-norm , finite sample performance guarantees for any form of alp . non-parametric approximate linear programming is amenable to problems with large -lrb- multidimensional -rrb- or even infinite action spaces , and does not require a model to select actions using the resulting approximate solution .",0
606,"This paper describes a computer simulation study comparing two models of mixed-initiative dialogues: the unrestricted initiative model and the restricted initiative model. In human-human dialogues, mixed-initiative refers to a conversational interaction in which both participants can initiate and respond to suggestions or proposals. The unrestricted initiative model allows both participants to initiate solutions and offers, whereas the restricted initiative model limits one participant to suggesting solutions while the other participant chooses among them. The computer simulation is used to compare the solution quality and efficiency of the two models in a range of scenarios. The results show that the unrestricted initiative model generally performs better in terms of solution quality, but the restricted initiative model can be more efficient in certain situations. Overall, the study highlights the potential benefits and drawbacks of each model for mixed-initiative dialogues and provides insights into the design and optimization of such dialogues in human-computer interactions. The use of computer simulation also",1
607,"in this paper , we use computer simulation to better understand mixed-initiative dialogues . we compare two models of mixed-initiative : unrestricted initiative , where either participant can take over control at any point ; and restricted initiative model where one participant keeps control and the other plays a secondary role , but greater than what single-initiative allows . we find that restricted initiative model results in similar solution quality as unrestricted , less communication effort , and similar or less reasoning effort . these results agree with our empirical studies on human-human dialogues , in which we find that participants seem to follow the restricted initiative model .",0
608,"This paper proposes the use of matrix filters for passive sonar, which can effectively solve localization and detection problems. By performing linear filtering operations through convex optimization techniques, matrix filters can effectively remove unwanted components from measured sensor data. The minimal distortion of sensor outputs achieved by matrix filters ensures that they are suitable for use in the context of passive sonar. Specifically, matrix filters are used to address unwanted components and perform measurements on sensor data, ultimately enabling improved detection and localization in passive sonar applications.",1
609,"this paper introduces matrix filters as a tool for localiza-tion and detection problems in passive sonar . the outputs of an array of sensors , at some given frequency , can be represented by a vector of complex numbers . a linear filtering operation on the sensor outputs can be expressed as the multiplication of a matrix -lrb- called a matrix filters -rrb- times this vector . the purpose of a matrix filters is to attenuate unwanted components in the measured sensor data while passing desired components with minimal distortion . matrix filters are designed by defining an appropriate pass band and stop band and solving a convex optimization problem . this paper formulates the design of matrix filters for passive sonar and gives two examples .",0
610,"This paper presents an adaptive monopulse processor for angle estimation in scenarios involving mainbeam jamming and coherent interference. The proposed technique employs a distortionless spatial array pattern, which is adaptively adjusted to suppress interference and cancel out the effects of mainbeam jamming. The processor uses space-time monopulse techniques to estimate target angles in the presence of terrain scattered interference and coherent multi-path effects. Mountaintop data is utilized to achieve interference suppression, and the processor employs spatially adaptive processing to optimize performance. The proposed monopulse processor is effective in scenarios involving both jamming and interference, and can be used to accurately estimate target angles in challenging environments.",1
611,"mainbeam jamming poses a particularly diicult challenge for conventional monopulse radars . in such cases spatially adaptive processing provides some interference suppression when the target and jammer are not exactly co-aligned , but the resulting array pattern is too distorted to be suitable for monopulse processing . the presence of coherent multi-path in the form of terrain scattered interference is normally considered a nuisance source of interference . however , it can also be exploited to suppress mainbeam jamming with space-time processing . here we present a method for incorporating space-time processing into monopulse processing to yield a space-time monopulse processor with dis-tortionless spatial array patterns that can achieve far better mainbeam jamming cancelation and target angle estimation than has been previously possible . performance results for the space-time monopulse processor are obtained for mountaintop data containing a jammer and terrain scattered interference , that demonstrate a dramatic improvement in performance over conventional monopulse and spatially adaptive monopulse .",0
612,"This paper proposes a novel approach to 3D human face recognition through the fusion of summation invariants. The recognition of the 3D surface of human faces is an important problem, particularly in the context of the Grand Challenge v1.0 dataset. In this work, semi-local summation invariant features are used to extract meaningful information from 3D facial depth maps. These maps are generated from 3D facial data and are an effective way of representing the 3D surface of a human face. The proposed approach utilizes a copyrighted component known as summation invariants, which are derived from rectangular region features. By fusing multiple summation invariants, the proposed approach achieves high accuracy in 3D human face recognition, particularly with respect to features such as the nose. The results demonstrate that the proposed approach outperforms existing techniques, and has the potential to be an effective solution to the problem of 3D human face recognition.",1
613,"however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any copyrighted component of this work in other works must be obtained from the ieee . abstract a novel family of 2d and 3d geometrically invariant features , called summation invariants is proposed for the recognition of the 3d surface of human faces . focusing on a rectangular region surrounding the nose of a 3d facial depth map , a subset of the so called semi-local sum-mation invariant features is extracted . then the similarity between a pair of 3d facial depth maps is computed to determine whether semi-local sum-mation invariant features belong to the same person . out of many possible combinations of these set of features , we select , through careful experimentation , a subset of features that yields best combined performance . tested with the 3d facial data from the ongoing face recognition grand challenge v1 .0 dataset , the proposed new features exhibit significant performance improvement over the baseline algorithm distributed with the dataset .",0
614,"This paper proposes an improved spectral and prosodic transformation method in STRAIGHT-based voice conversion. The former voice conversion system suffers from poor discrimination and speech quality due to the use of excitation-dependent and excitation-independent components. In this work, a spectral conversion method is introduced, which is based on the use of the STRAIGHT spectrum. The proposed method is capable of accurately representing prosodic characteristics, which are essential for achieving high-quality voice conversion. A codebook mapping technique is employed to generate the spectral representation of the converted speech. Additionally, a mixture of Gaussian (MoG) model is used to improve the prosodic conversion process. The MoG model is shown to be effective in capturing the complex distribution of prosodic characteristics, and is used to improve the overall performance of the voice conversion system. The proposed approach is evaluated using objective measures and subjective listening tests, and the results demonstrate the effectiveness of the improved method in achieving high-quality voice conversion.",1
615,"this paper presents a novel spectral conversion method by considering the glottal effect on straight spectrum to improve the performance of former voice conversion system based on codebook mapping . by introducing mog model into spectral representation , straight spectrum is decomposed into excitation-dependent and excitation-independent components , which are transformed separately . besides , mog model is adopted to measure the prosodic characteristics of different speakers and realize prosodic conversion . listening test proves that proposed spectral conversion method can effectively improve the discrimination and speech quality of converted speech at the same time .",0
616,"This paper proposes a novel approach for assessing image realism using color compatibility. Realistic and unrealistic images can be distinguished based on the distributions of colors in their color combinations. In this work, the proposed approach involves recoloring image regions to generate a composite image that is then classified based on its color distribution. The process of recoloring image regions involves changing the colors of individual image regions to better match those of natural images. This approach enables realistic compositing of images, and the resulting composite images are then analyzed based on their color distribution. The proposed approach is evaluated using objective measures, and the results demonstrate its effectiveness in accurately distinguishing between realistic and unrealistic images. The proposed approach has the potential to be used in a variety of applications, such as image editing and post-processing, where image realism is an important factor.",1
617,"why does placing an object from one photograph into another often make the colors of that object suddenly look wrong ? one possibility is that humans prefer distributions of colors that are often found in nature ; that is , we find pleasing these color combinations that we see often . another possibility is that humans simply prefer colors to be consistent within an image , regardless of what they are . in this paper , we explore some of these issues by studying the color statistics of a large dataset of natural images , and by looking at differences in color distribution in realistic and unrealistic images . we apply our findings to two problems : 1 -rrb- classifying composite images into realistic vs. non-realistic , and 2 -rrb- recoloring image regions for realistic com-positing .",0
618,"This paper presents a block sparse excitation based all-pole modeling approach for speech. The proposed method utilizes a block sparse structure in the excitation sequence and applies sparse Bayesian learning methods to estimate the model parameters. A weighted linear combination of the generalized input sequence and white noise is used to model both voiced and unvoiced speech. The linear prediction approach is applied to estimate the all-pole filter coefficients, and an expectation-maximization based procedure is used to update the block sparse structure. The proposed method is evaluated on a speech modeling task, and the results demonstrate its effectiveness in accurately modeling speech signals. The approach shows promising results in modeling both voiced and unvoiced speech signals, and the proposed method has potential applications in speech analysis, synthesis, and compression.",1
619,"in this paper , it is shown that an appropriate model for voiced speech is an all-pole filter excited by a block sparse excitation sequence . the modeling approach is generalized in a novel manner to deal with a wide spectrum of speech signal ; voiced speech , unvoiced speech and mixed excitation speech . in this context , the input sequence to the all-pole model is modeled as a suitable weighted linear combination of a block sparse signal and white noise . we develop the corresponding estimation procedure to reconstruct the generalized input sequence and model parameters via sparse bayesian learning methods employing the expectation-maximization based procedure . rigorous experiments have been performed to show the efficacy of our proposed model for the speech modeling task . by imposing a block sparse structure on the input sequence , the problems associated with the commonly used linear prediction approach is alleviated leading to a more robust modeling scheme .",0
620,"This paper proposes an integrated pronunciation learning approach for automatic speech recognition (ASR) using probabilistic lexical modeling. ASR systems typically rely on acoustic and lexical resources, with the latter including pronunciation lexicons. In this work, the authors investigate a phoneme-based ASR system with a phoneme-based pronunciation lexicon and a grapheme-based ASR system approach with hand-crafted pronunciations. They also consider the use of a grapheme-to-phoneme (G2P) converter and the integration of pronunciation learning during the G2P training stage. The proposed approach is evaluated for lexical resource constrained ASR tasks and compared to a stage-based approach. Experimental results show that the integrated approach outperforms the baseline models and provides a more efficient and effective means for ASR system development.",1
621,"standard automatic speech recognition systems use phoneme-based pronunciation lexicon prepared by linguistic experts . when the hand crafted pronunciations fail to cover the vocabulary of a new domain , a grapheme-to-phoneme converter is used to extract pronunciations for new words and then a phoneme-based asr system is trained . g2p converters are typically trained only on the existing lexicons . in this paper , we propose a grapheme-based asr approach in the framework of probabilistic lexical mod-eling that integrates pronunciation learning as a stage in asr system training , and exploits both acoustic and lexical resources -lrb- not necessarily from the domain or language of interest -rrb- . the proposed grapheme-based asr approach is evaluated on four lexical resource constrained asr tasks and compared with the conventional two stage approach where g2p training is followed by asr system development .",0
622,"Metaphors are a pervasive aspect of language and play an important role in many aspects of communication. However, the mechanisms that underlie metaphor use are not well understood. In this paper, we propose a game-theoretic model of metaphorical bargaining, which provides a framework for understanding how metaphors are used in the context of negotiation and persuasion. We show that the model is able to capture many of the key features of observed linguistic behavior, and that it has implications for a wide range of fields, including political communication and computational linguistics. By providing a formal model of metaphorical bargaining, we hope to contribute to a better understanding of the role that metaphors play in communication and to stimulate further research in this area.",1
623,"we present a game-theoretic model of bargaining over a metaphor in the context of political communication , find its equilibrium , and use game-theoretic model to rationalize observed linguistic behavior . we argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conflicting pressures , and suggest applications of interest to computational linguists .",0
624,"This paper proposes a background modeling and subtraction scheme that works on the temporal variability of pixel intensities and the spatial variation of region statistics. The authors focus on the use of generic low-level detection metrics and application-dependent criteria to improve the detection of foreground objects. Specifically, the paper suggests using distributional signatures of intensity or color distributions to detect foreground objects and discusses how to adapt the algorithm for textured backgrounds. The proposed approach can be applied to various environmental monitoring applications and offers a processing power advantage compared to segmentation algorithms. The paper also evaluates the use of generic low-level detection metrics in the background subtraction algorithms. Overall, the proposed background subtraction approach provides an efficient and effective way to detect foreground objects in monitoring applications.",1
625,"environmental monitoring applications present a challenge to current background subtraction algorithms that analyze the temporal variability of pixel intensities , due to the complex texture and motion of the scene . they also present a challenge to segmentation algorithms that compare intensity or color distributions between the foreground and the background in each image independently , because objects of interest such as animals have adapted to blend in . therefore , we have developed a background modeling and subtraction scheme that analyzes the temporal variation of intensity or color distributions , instead of either looking at temporal variation of point statistics , or the spatial variation of region statistics in isolation . distributional signatures are less sensitive to movements of the tex-tured background , and at the same time distributional signatures are more robust than individual pixel statistics in detecting foreground objects . they also enable slow background update , which is crucial in monitoring applications where processing power comes at a premium , and where foreground objects , when present , may move less than the background and therefore disappear into it when a fast update scheme is used . our background modeling and subtraction scheme compares favorably with the state of the art both in generic low-level detection metrics , as well as in application-dependent criteria .",0
626,"This paper proposes a model-based object tracking approach using raw time-of-flight (ToF) observations at a low frame-rate of 300Hz. The method is designed to work with off-the-shelf depth cameras, which are commonly used in consumer applications, and to handle fast or unpredictable motion. The proposed approach utilizes phase-based ToF sensing and is able to track fast-moving objects with high accuracy and robustness. The effectiveness of the proposed method is demonstrated through experiments with various challenging scenarios. The results show that the proposed approach outperforms existing methods in terms of tracking accuracy and robustness, and has potential for practical applications in monitoring and tracking fast-moving objects.",1
627,"consumer depth cameras have dramatically improved our ability to track rigid , articulated , and deformable 3d objects in real-time . however , depth cameras have a limited temporal resolution -lrb- frame-rate -rrb- that restricts the accuracy and robustness of tracking , especially for fast or unpredictable motion . in this paper , we show how to perform model-based object tracking which allows to reconstruct the object 's depth at an order of magnitude higher frame-rate through simple modifications to an off-the-shelf depth camera . we focus on phase-based time-of-flight sensing , which reconstructs each low frame-rate depth image from a set of short exposure ` raw ' infrared captures . these raw captures are taken in quick succession near the beginning of each depth frame , and differ in the modulation of their active illumination . we make two contributions . first , we detail how to perform model-based object tracking against these raw captures . second , we show that by reprogramming the camera to space the raw captures uniformly in time , we obtain a 10x higher frame-rate , and thereby improve the ability to track fast-moving objects .",0
628,"This paper presents a novel approach for optimal decision making called ""Threshold Learning"". The approach utilizes a two-factor reward-modulated learning rule, inspired by the Williams' REINFORCE method, to learn a threshold that maximizes decision optimality. The learning rule is modelled after the basal ganglia function and utilizes Wald's cost function and competitive stochastic evidence accumulation to optimize decision thresholds. Additionally, the approach employs Bayesian optimization and Gaussian process learning rules to model the reward function. The proposed approach is evaluated using a drift-diffusion model and the results show that threshold learning improves decision accuracy and robustness compared to other reinforcement learning approaches. The approach has potential applications in fields such as neuroscience and artificial intelligence.",1
629,"decision making under uncertainty is commonly modelled as a process of competitive stochastic evidence accumulation to threshold -lrb- the drift-diffusion model -rrb- . however , it is unknown how animals learn these decision thresholds . we examine threshold learning by constructing a reward function that averages over many trials to wald 's cost function that defines decision optimality . these reward function are highly stochastic and hence challenging to optimize , which we address in two ways : first , a simple two-factor reward-modulated learning rule derived from williams ' reinforce method for neural networks ; and second , bayesian optimization of the reward function with a gaussian process . bayesian optimization converges in fewer trials than reinforce but is slower computationally with greater variance . the williams ' reinforce method is also a better model of acquisition behaviour in animals and a similar learning rule has been proposed for modelling basal ganglia function .",0
630,"This paper proposes a method for active control of acoustic signals in multiple input multiple output systems. The goal is to minimize the maximum error signal in the acoustic field by cancelling the unwanted noise field. Two algorithms are used in this work, a steepest descent iterative algorithm and a 1-norm minimization algorithm. The measured data is used to improve the performance of the active control system. The results show that the 1-norm minimization algorithm is effective in reducing the uniform final noise field. The proposed method can be applied in various situations where active control of acoustic signals is required.",1
631,this paper deals with multiple input multiple output systems for active control of acoustic signals . these multiple input multiple output systems are used when the acoustic eld is complex and therefore a number of sensors are necessary to estimate the sound eld a n d a n umber of sources to create the cancelling eld . a steepest descent iterative algorithm is applied to minimise the p-norm of a multiple input multiple output systems composed by the output signals of a microphone array . the existing algorithms deal with the 2-norm of this multiple input multiple output systems . this paper describes a general framework that covers the existing multiple input multiple output systems and then it focuses on the 1-norm minimisation algorithm . the 1-norm minimisation algorithm based on the 1-norm minimises the output signal which has the greatest power . it is shown by means of simulations using measured data from a real room that the 1-norm minimisation algorithm leads to a more uniform nal noise eld than the existing algorithms .,0
632,"The paper proposes a novel approach for action recognition from arbitrary views using 3D exemplars. The method employs temporal Markov dependency and exemplar-based HMM to recognize actions. The approach uses smoothly moving camera and 3D reconstruction to obtain dimensional occupancy grids. The method takes into account view parameters and relative orientations of the recognition scenarios. The recognition process is performed by estimating latent variables, and recognizing phase is done by comparing image projections with prior knowledge. The proposed approach is evaluated on real datasets and outperforms state-of-the-art methods in terms of recognition accuracy.",1
633,"in this paper , we address the problem of learning compact , view-independent , realistic 3d models of human actions recorded with multiple cameras , for the purpose of recognizing those same actions from a single or few cameras , without prior knowledge about the relative orienta-tions between the cameras and the subjects . to this aim , we propose a new framework where we model actions using three dimensional occupancy grids , built from multiple viewpoints , in an exemplar-based hmm . the novelty is , that a 3d reconstruction is not required during the recognition phase , instead learned 3d reconstruction are used to produce 2d image information that is compared to the observations . parameters that describe image projections are added as latent variables in the recognition process . in addition , the temporal markov dependency applied to view parameters allows them to evolve during recognition as with a smoothly moving camera . the effectiveness of the framework is demonstrated with experiments on real datasets and with challenging recognition scenarios .",0
634,"This paper addresses the problem of non-negative source separation and investigates the range of admissible solutions and conditions for the uniqueness of the solution. Specifically, it explores the challenges posed by ordering and scale ambiguities and indeterminacies in the context of non-negative mixing coefficients. The paper proposes a method to resolve these ambiguities and indeterminacies to obtain a unique solution for non-negative source separation. The results of this study provide insights into the properties of non-negative sources and their mixing coefficients, which can have important applications in various fields, such as audio and image processing.",1
635,"a main issue in source separation is to deal with the inde-terminacies . well known are the ordering and scale ambiguities , but other types of indeterminacies may also occur . in this paper we address these indeterminacies in the case of non-negative sources and non-negative mixing coefficients . on the one hand , we fully develop the case of two sources . on the other hand , in the general case we formulate necessary conditions for the uniqueness of the solution -lrb- up to ordering and scale ambiguities -rrb- .",0
636,The paper proposes a learning approach for image decomposition and object categorization using coupled conditional random fields (CRFs). The approach is based on a computational system that utilizes low-level cues of contour and adaptive fusion of visual information. Multiple decomposed visual cues are used to train a single-layer CRF model that incorporates discriminative cues. The proposed approach is evaluated on natural images and achieves competitive results on object categorization tasks. The results demonstrate the effectiveness of the coupled CRF model in learning and recognizing object categories from complex visual scenes.,1
637,"this paper proposes a computational system of object categorization based on decomposition and adaptive fusion of visual information . a coupled conditional random field is developed to model the interaction between low level cues of contour and texture , and to decompose contour and texture in natural images . the advantages of using coupled rather than single-layer random fields are demonstrated with model learning and evaluation . multiple decomposed visual cues are adaptively combined for object categorization to fully leverage different discrimina-tive cues for different classes . experimental results show that the proposed computational system of object categorization of '' recognition-through-decomposition-and-fusion '' achieves better performance than most of the state-of-the-art methods , especially when only a limited number of training samples are available .",0
638,This paper investigates how recurrent mechanisms in the visual processing pathway lead to complex cell responses through recurrent amplification of feedforward input in cortical circuitry. The authors propose a network model of recurrent cortical amplification and demonstrate how it can account for the properties of complex cells observed in visual cortex. Their findings suggest that recurrent amplification plays an important role in shaping complex cell responses and highlight the importance of considering recurrent mechanisms in neural processing models.,1
639,"cortical amplification has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex . less appreciated is the fact that the same form of amplification can also be used to de-tune or broaden selectivity . using a network model with recurrent cortical circuitry , we propose that the spatial phase invariance of complex cell responses arises through recurrent amplification of feedforward input . neurons in the network model respond like simple cells at low gain and complex cells at high gain . similar recurrent mechanisms may play a role in generating invariant representations of feedforward input elsewhere in the visual processing pathway .",0
640,This paper discusses interruptible pure exploration in multi-armed bandit problems. The authors propose a novel algorithm that combines discriminative bucketing and Monte-Carlo Tree Search to solve this problem. They analyze the performance of their algorithm in both the interruptible and non-interruptible settings and compare it to other popular strategies such as ε-greedy. The authors show that their algorithm achieves better performance than existing strategies in terms of sample complexity and regret bounds. The proposed algorithm is useful for sequential decision problems where conservatively exploring all possible options may not be feasible.,1
641,"interruptible pure exploration in multi-armed bandits is a key component of monte-carlo tree search algorithms for sequential decision problems . we introduce discriminative bucketing , a novel family of strategies for pure exploration in multi-armed bandits , which allows for adapting recent advances in non-interruptible strategies to the interruptible setting , while guaranteeing exponential-rate performance improvement over time . our experimental evaluation demonstrates that the corresponding instances of discriminative bucketing favorably compete both with the currently popular strategies ucb1 and ε-greedy , as well as with the conservative uniform sampling .",0
642,"This paper proposes a semiparametric model for Bayesian reader identification, which uses aggregate features of eye movements to identify individuals. The model assumes a parametric distribution family for the characteristic gaze patterns and introduces flexible semiparametric models for the word fixation durations. In particular, the paper uses a Gaussian process to model the saccade amplitudes, which allows for a more accurate modeling of gaze control. The proposed model is compared with other approaches, and experiments show that the semiparametric model outperforms existing parametric density models in terms of reader identification accuracy.",1
643,"we study the problem of identifying individuals based on their characteristic gaze patterns during reading of arbitrary text . the motivation for this problem is an unobtrusive biomet-ric setting in which a user is observed during access to a document , but no specific challenge protocol requiring the user 's time and attention is carried out . existing models of individual differences in gaze control during reading are either based on simple aggregate features of eye movements , or rely on paramet-ric density models to describe , for instance , saccade amplitudes or word fixation durations . we develop flexible semiparametric models of eye movements during reading in which densities are inferred under a gaussian process prior centered at a parametric distribution family that is expected to approximate the true distribution well . an empirical study on reading data from 251 individuals shows significant improvements over the state of the art .",0
644,This paper proposes a Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation. The model is applied and evaluated on Chinese-English test sets provided by the National Institute of Standards and Technology (NIST). The proposed model aims to enhance the hierarchical phrase-based machine translation by introducing the notion of lexical cohesion. The model employs trigger pairs of source and target language words to capture the cohesive relationships between words. The results demonstrate that the proposed model significantly improves the document-level machine translation performance by capturing the underlying coherence and cohesion of the texts.,1
645,"in this paper , we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation . we integrate the bilingual lexical cohesion trigger model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 bleu points on average over the baseline on nist chinese-english test sets .",0
646,"This paper proposes an approach for sequential grouping of cochannel speech, aiming to segregate voiced speech from unvoiced speech in a concurrent speech stream. The proposed method is an unsupervised approach that utilizes onset/offset based analysis within-group concurrent pitches, pretrained speaker models, and cepstral features to separate the speech signals. The paper evaluates the proposed method against model-based methods, and shows that it outperforms previous methods in sequential organization of cochannel speech. The results demonstrate the effectiveness of the proposed approach in speech segregation and its potential for real-world applications.",1
647,"model-based methods for sequential organization in cochannel speech require pretrained speaker models and often prior knowledge of participating speakers . we propose an unsupervised approach to sequential organization of cochannel speech . based on cepstral features , we first cluster voiced speech into two speaker groups by maximizing the ratio of between-and within-group distances penalized by within-group concurrent pitches . to group unvoiced speech , we employ an onset/offset based analysis to generate time-frequency segments . unvoiced segments are then labeled by the complementary portions of segregated voiced speech . our unsupervised approach does not require any pretrained model and is computationally simple . evaluations and comparisons show that the proposed unsupervised approach outperforms a model-based method in terms of speech segregation .",0
648,"This paper explores the integration of dynamic speech modalities into context decision trees in order to improve the performance of speech recognition systems. Specifically, the study focuses on the speaker's dialect and the speaking rate of their utterances. The Janus Speech Recognizer is used as a tool for evaluating the effectiveness of incorporating these dynamic modalities into context decision trees. The results show that integrating these modalities leads to significant error rate reductions, particularly for spoken utterances with varying dialects and speaking rates. The research presented in this paper has implications for the speech recognition community, as it provides a novel approach to improving speech recognition performance by leveraging context decision trees and dynamic speech modalities.",1
649,"context decision trees are widely used in the speech recognition community . besides questions about pho-netic classes of a phone 's context , questions about their position within a word lee88 -rsb- and questions about the gender of the current speaker rc99 -rsb- have been used so far . in this paper we additionally incorporate questions about current modalities of the spoken utterance like the speaker 's dialect , the speaking rate , the signal to noise ratio , the latter two of which may change while speaking one utterance . we present a framework that treats all these modalities in a uniform way . experiments with the janus speech recognizer have produced error rate reductions of up to 10 % when compared to systems that do not use modality questions .",0
650,"This paper presents a novel classification algorithm called Symmetric Maximized Minimal Distance in Subspace (SMMS). The SMMS algorithm is designed to improve the accuracy of classification tasks, specifically in the context of face authentication. The algorithm is based on the concept of symmetric maximized minimal distance, which is used to measure the distance between samples in the feature space. The SMMS algorithm is optimized to find the decision boundary that maximizes the distance between genuine and imposter samples. This approach improves the performance of classification algorithms, such as support vector machines, in identifying genuine samples and rejecting imposter samples. The SMMS algorithm achieves optimality by operating in a subspace, where the dimensionality of the feature space is reduced without losing discriminative power. Experimental results show that the SMMS algorithm outperforms existing classification algorithms in terms of accuracy and robustness against imposter attacks. This research contributes to the development of more effective classification algorithms that can be used in various applications beyond face authentication.",1
651,"we introduce a new classification algorithm based on the concept of symmetric maximized minimal distance in subspace -lrb- smms -rrb- . given the training data of authentic samples and imposter samples in the feature space , smms tries to identify a subspace in which all the authentic samples are close to each other and all the imposter samples are far away from the authentic samples . the optimality of the subspace is determined by maximizing the minimal distance between the authentic samples and the imposter samples in the subspace . we present a procedure to achieve such optimality and to identify the decision boundary . the verification procedure is simple since we only need to project the test sample to the subspace and compare it against the decision boundary . using face authentication as an example , we show that the proposed algorithm outperforms several other algorithms based on support vector machines .",0
652,"This paper investigates the use of prosodic cues for identifying discourse genres in French speech. The study focuses on two specific genres: political speech and sport commentary speech, which are known to have distinct speaking styles and contextual environments. The research is motivated by the perceptual similarity between these two genres, which poses a challenge for non-native speakers in identifying them accurately. The study employs speech synthesis techniques to generate samples of both genres with varying acoustic prosodic cues. Native and non-native speakers are then asked to identify the genre of each sample based on the prosodic cues. The results reveal that specific prosodic cues, such as pitch range and duration, are more indicative of a particular genre than others. The study also highlights the importance of contextual information in genre identification, as the same prosodic cues can be interpreted differently depending on the genre. The research presented in this paper has implications for improving the identification ability of discourse genres in French speech, which has practical applications in areas such as speech recognition and natural language processing.",1
653,"speech can be divided into discourse genres based on the contextual environment it occurs in -lrb- e.g. political speech , sport commentary speech , etc. -rrb- . the present study investigated whether listeners can distinguish between speech from different discourse genres on the basis of acoustic prosodic cues only 1 . in a perception experiment with delexicalized speech 70 listeners with varying experience in french -lrb- native speakers , non-native speakers , and non-speakers -rrb- were asked to identify four different types of discourse genres -lrb- church service , political , journal , and sport commentary -rrb- . results revealed a fair identification ability with a significant increase in performance with increasing experience in french . identification ability was used to cluster discourse genres according to their perceptual similarity . the possible application of the results for the evaluation of speaking style speech synthesis will be discussed .",0
654,"This paper proposes a new voice conversion technique using deep belief nets (DBNs) and high-order eigen spaces. The technique aims to address the challenge of preserving speaker individuality in voice conversion while improving the conversion quality. The approach is based on a deep architecture that learns a mapping between the cepstrum space of the source and target speakers. The DBNs are trained using a large dataset of speech samples and optimized using both subjective and objective criteria. The high-order eigen spaces are used to capture phonological information and abstractions that are critical for preserving the speaker's individuality. The proposed technique is compared to a Gaussian mixture model-based method, and the results show that the DBN-based approach outperforms the baseline method in terms of conversion quality and the preservation of speaker individuality. The research presented in this paper contributes to the development of more effective voice conversion techniques that can be used in applications such as speech synthesis and voice transformation.",1
655,"this paper presents a voice conversion technique using deep belief nets to build high-order eigen spaces of the source/target speakers , where voice conversion technique is easier to convert the source speech to the target speech than in the traditional cepstrum space . deep belief nets have a deep architecture that automatically discovers abstractions to maximally express the original input features . if we train the deep belief nets using only the speech of an individual speaker , voice conversion technique can be considered that there is less phonologi-cal information and relatively more speaker individuality in the output features at the highest layer . training the deep belief nets for a source speaker and a target speaker , we can then connect and convert the speaker individuality abstractions using neural networks . the converted abstraction of the source speaker is then brought back to the cepstrum space using an inverse process of the deep belief nets of the target speaker . we conducted speaker-voice conversion experiments and confirmed the efficacy of our voice conversion technique with respect to subjective and objective criteria , comparing voice conversion technique with the conventional gaussian mixture model-based method .",0
656,"This paper presents a Bayesian approach to context clustering for Hidden Markov Model (HMM)-based speech recognition. The proposed method uses a cross valid prior distribution determination technique and a variational Bayesian method to approximate the posterior distributions of model parameters. The distribution determination technique enables the tuning of model structure and parameters by incorporating prior information from previous experiments. The cross-validation technique is used to select the optimal prior distributions by evaluating the predictive distributions on a validation set. The proposed approach is compared with a baseline method using a statistical technique, and the results show that the Bayesian approach outperforms the baseline in terms of predictive accuracy. The study also shows that the variational Bayesian method can reduce the computational cost of calculating the posterior distributions. The research presented in this paper contributes to the development of more effective and efficient context clustering techniques for HMM-based speech recognition, which has practical applications in areas such as automatic speech recognition and natural language processing.",1
657,"this paper proposes a prior distribution determination technique using cross validation for speech recognition based on the bayesian approach . the distribution determination technique is a statistical technique for estimating reliable predictive distributions by marginalizing model parameters and its approximate version , the variational bayesian method has been applied to hmm-based speech recognition . since prior distributions representing prior information about model parameters affect the posterior distributions and model selection , the determination of prior distributions is an important problem . however , it has not been thoroughly investigate in speech recognition . the proposed distribution determination technique can determine reliable prior distributions without tuning parameters and select an appropriate model structure dependently on the amount of training data . continuous phoneme recognition experiments show that the proposed distribution determination technique achieved a higher performance than the conventional methods .",0
658,"This paper proposes a method for learning factored sparse inverse covariance matrices for Gaussian mixture models (GMMs). The method leverages the conditional independence properties of GMMs to factorize the inverse covariance matrix into smaller matrices, which enables efficient computation and parsimony. The proposed method uses non-linear EM update equations to learn the factorization, which involves estimating the linear regressive coefficients between the elements of the factorized matrices. The learned factorization can be used in HMM-based speech recognition systems to model the observation probability density functions of the acoustic features. The experimental results show that the proposed method can achieve comparable performance to full-covariance Gaussian mixtures while using significantly fewer parameters. The research presented in this paper contributes to the development of more efficient and effective methods for modeling the covariance matrices of GMMs, which has practical applications in areas such as speech recognition, image processing, and pattern recognition.",1
659,"most hmm-based speech recognition systems use gaussian mixtures as observation probability density functions . an important goal in all such hmm-based speech recognition systems is to improve parsimony . one method is to adjust the type of covariance matrices used . in this work , fac-tored sparse inverse covariance matrices are introduced . based on í 1/4 í factorization , the inverse covariance matrix can be represented using linear regressive coefficients which 1 -rrb- correspond to sparse patterns in the inverse covariance matrix -lrb- and therefore represent conditional independence properties of the gaussian -rrb- , and 2 -rrb- , result in a method of partial tying of the covariance matrices without requiring non-linear em update equations . results show that the performance of full-covariance gaussians can be matched by factored sparse inverse covariance gaussians having significantly fewer parameters .",0
660,"This paper proposes an enhanced wall clutter mitigation technique for compressed through-the-wall radar imaging using joint Bayesian sparse signal recovery. The technique involves a single-signal compressed sensing model and a sparsifying wavelet dictionary. A subspace projection technique is used to improve image formation and to recover sparse signals. The proposed method also includes joint Bayesian sparse approximation framework, which is effective in reducing wall clutter and increasing signal sparsity. The algorithm is evaluated using antenna signal data and antenna location data. The results demonstrate high-quality images with improved reconstruction accuracy and reduced wall clutter. The proposed technique is expected to be beneficial for applications such as search and rescue operations, law enforcement, and military operations.",1
661,"this paper addresses the problem of wall clutter mitigation in compressed sensing through-the-wall radar imaging , where a different set of frequencies is sensed at different antenna locations . a joint bayesian sparse approximation framework is first employed to reconstruct all the signals simultaneously by exploiting signal sparsity and correlations between antenna signals . this is in contrast to previous approaches where the signal at each antenna location is reconstructed independently . furthermore , to promote sparsity and improve reconstruction accuracy , a sparsifying wavelet dictionary is employed in the sparse signal recovery . following wall clutter mitigation , a subspace projection technique is applied to remove wall clutter , prior to image formation . experimental results on real data show that the proposed approach produces significantly higher reconstruction accuracy and requires far fewer measurements for forming high-quality images , compared to the single-signal compressed sensing model , where each antenna signal is reconstructed independently .",0
662,"This paper proposes a Probabilistic Linear Discriminant Analysis (PLDA)-based acoustic model for speech recognition that incorporates bottleneck features to improve recognition accuracy. The model uses a hybrid deep neural network and Gaussian mixture model system with subspace Gaussian mixture models and PLDA. The PLDA-based model is evaluated using the Switchboard dataset, a large vocabulary conversational telephone speech corpus. The proposed model shows significant word error reduction compared to baseline systems that use only high-dimensional acoustic features. The paper also investigates the effect of intra-frame feature correlations and demonstrates the benefits of using bottleneck features for recognition accuracy. The proposed system achieves state-of-the-art performance on the Switchboard dataset.",1
663,"we have recently proposed a new plda-based acoustic model based on prob-abilistic linear discriminant analysis which enjoys the flexibility of using higher dimensional acoustic features , and is more capable to capture the intra-frame feature correlations . in this paper , we investigate the use of bottleneck features obtained from a deep neural network for the plda-based acoustic model . experiments were performed on the switchboard dataset -- a large vocabulary conversational telephone speech corpus . we observe significant word error reduction by using the bottleneck features . in addition , we have also compared the plda-based acoustic model to three others using gaussian mixture models , subspace gmms and hybrid deep neural networks , and plda can achieve comparable or slightly higher recognition accuracy from our experiments .",0
664,"This paper proposes a discriminative framework for anomaly detection in large videos. In contrast to classical density estimation approaches, this framework takes into account the temporal ordering of anomalies and makes fewer assumptions about the anomaly detection setting. Specifically, the framework incorporates early context assumptions to better capture low-probability events and learns discriminative models from complex videos. By using training sequences, the discriminative learning approach can handle high-dimensional models and achieve better performance on detecting anomalies in large video datasets. Overall, this framework provides a promising approach to anomaly detection in complex and high-dimensional video data.",1
665,"we address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of temporal ordering . current algorithms in anomaly detection setting are based on the classical density estimation approach of learning high-dimensional models and finding low-probability events . these algorithms are sensitive to the order in which anomalies appear and require either training data or early context assumptions that do not hold for longer , more complex videos . by defining anomalies as examples that can be distinguished from other examples in the same video , our definition inspires a shift in approaches from classical density estimation to simple discriminative learning . our contributions include a novel framework for anomaly detection setting that is -lrb- 1 -rrb- independent of temporal ordering of anomalies , and -lrb- 2 -rrb- unsupervised , requiring no separate training sequences . we show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing training sequences from standard datasets .",0
666,"This paper proposes a triangulation approach for answering ranking in community question answering by incorporating three key input components: syntactic and semantic embeddings, lexical similarity features, and domain-specific features. These input components are integrated using a pairwise neural network architecture, which is trained to rank answers based on their relatedness to the question. The authors demonstrate that the combination of these feature types results in improved performance over using any one type alone. The graph highlights the relationships between the different input components and their usage in the proposed architecture. The experimental results suggest that the proposed approach outperforms existing methods and can be applied to various community forums with different domain-specific features.",1
667,"we address the problem of answering new questions in community forums , by selecting suitable answers to already asked questions . we approach the task as an answer ranking problem , adopting a pairwise neural network architecture that selects which of two competing answers is better . we focus on the utility of the three types of similarities occurring in the triangle formed by the original question , the related question , and an answer to the related comment , which we call relevance , relatedness , and appropriateness . our proposed pairwise neural network architecture models the interactions among all input components using syntactic and semantic embeddings , lexical matching , and domain-specific features . it achieves state-of-the-art results , showing that the three similarities are important and need to be mod-eled together . our experiments demonstrate that all feature types are relevant , but the most important ones are the lexical similarity features , the domain-specific features , and the syntactic and semantic embeddings .",0
668,"This paper presents written-domain language modeling approaches for automatic speech recognition (ASR) systems, which aim to improve the verbalization of written-domain vocabulary items. The study focuses on both lexical and non-lexical entities, such as e-mail addresses, phone numbers, URLs, and dollar amounts, and proposes finite-state modeling techniques to handle data sparsity problems. The paper also introduces a decomposition-recomposition approach to deal with out-of-vocabulary words, as well as denormalization rules for written domain entities. The graph illustrates the relationships between various entities and techniques used in the proposed approach, including the use of verbal-domain language modeling for written-domain language modeling approaches and the relationship between phone numbers, URLs, and e-mail addresses as non-lexical entities. The results show that the proposed approach improves ASR transcript rendering accuracy and can effectively handle the verbalization of both lexical and non-lexical entities in the written domain.",1
669,"language modeling for automatic speech recognition systems has been traditionally in the verbal domain . in this paper , we present finite-state modeling techniques that we developed for language modeling in the written domain . the first finite-state modeling techniques we describe is for the verbalization of written-domain vocabulary items , which include lexical and non-lexical entities . the second finite-state modeling techniques is the decomposition -- recomposition approach to address the out-of-vocabulary and the data sparsity problems with non-lexical entities such as urls , e-mail addresses , phone numbers , and dollar amounts . we evaluate the proposed written-domain language modeling approaches on a very large vocabulary speech recognition system for en-glish . we show that the written-domain language modeling approaches improves the speech recognition and the asr transcript rendering accuracy in the written domain over a baseline system using a verbal-domain language model . in addition , the written-domain language modeling approaches is much simpler since written-domain language modeling approaches does not require complex and error-prone text normalization and denormalization rules , which are generally required for verbal-domain language modeling .",0
670,"This paper presents a non-iterative linear algorithm for the computation of the quadrifocal tensor, which is used in projective scene reconstruction from multiple images. The quadrifocal tensor is a 4th-order tensor that relates the projections of points in 3D space onto three cameras, and its extraction from camera matrices is a fundamental problem in computer vision. The proposed algorithm is compared with existing iterative methods, and it is shown that the non-iterative linear algorithm achieves comparable algebraic accuracy while being more efficient. The graph highlights the relationship between the non-iterative linear algorithm and its usage for achieving algebraic accuracy, as well as the evaluation of iterative methods using synthetic data. The experimental results demonstrate the effectiveness of the proposed algorithm for accurate computation of the quadrifocal tensor, which can lead to improved projective scene reconstruction from multiple images.",1
671,"this paper gives a practical and accurate algorithm for the computation of the quadrifocal tensor and extraction of camera matrices from it . previous methods for using the quadrifocal tensor in projective scene reconstruction have not emphasized accuracy of the algorithm in conditions of noise . methods given in this paper minimize algebraic error either through a non-iterative linear algorithm , or two alternative iterative algorithms . it is shown by experiments with synthetic data that the iterative methods , though minimizing algebraic , rather than more correctly geometric error measured in the image , give almost optimal results .",0
672,"This paper presents a sparsity-based spatial spectrum estimation technique for direction-of-arrival (DOA) estimation using co-prime arrays. Co-prime arrays are a specific structure of antenna arrays that enable higher degrees-of-freedom in DOA estimation, as compared to traditional arrays. The proposed method exploits the sparsity of the signal model in the virtual aperture formed by the co-array and estimates the DOAs of the sources by solving an optimization problem based on the sparse representation of the signal. The paper evaluates the DOA estimation performance of the proposed method using simulated data and compares it with other state-of-the-art methods. The graph illustrates the relationship between the co-prime arrays and their usage for DOA estimation, as well as the concept of the virtual aperture formed by the co-array and the array aperture. The results show that the proposed method achieves higher DOA estimation performance as compared to other methods and demonstrates the effectiveness of sparsity-based techniques for DOA estimation using co-prime arrays.",1
673,"in this paper , we propose co-prime arrays for effective direction-of-arrival estimation . to fully utilize the virtual aperture achieved in the difference co-array constructed from a co-prime array structure , sparsity-based spatial spectrum estimation technique is exploited . compared to existing techniques , the proposed technique achieves better utilization of the co-array aperture and thus results in increased degrees-of-freedom as well as improved doa estimation performance .",0
674,"This paper proposes a Bayesian Model Averaging Naive Bayes (BMA-NB) method that can average over an exponential number of feature models in linear time. Naive Bayes (NB) is a popular classification algorithm but can suffer from poor performance when the number of features is large. BMA-NB uses a globally optimal feature subset and selects the best feature models by Bayesian model averaging. BMA-NB outperforms both NB and Support Vector Machines (SVMs) in terms of classification accuracy. The proposed method provides a powerful feature selection approach that can be used to improve the performance of various classifiers. This paper also presents a theoretical analysis of the asymptotic limit of data and the number of NB feature models. The results show that the proposed method can achieve a higher classification accuracy than traditional feature selection methods, such as SVMs. The paper concludes that BMA-NB is an effective and efficient approach for selecting relevant features and improving the performance of classifiers.",1
675,"naive bayes -lrb- nb -rrb- is well-known to be a simple but effective classifier , especially when combined with feature selection . unfortunately , feature selection methods are often greedy and thus can not guarantee an optimal feature set is selected . an alternative to feature selection is to use bayesian model averaging -lrb- bma -rrb- , which computes a weighted average over multiple predictors ; when the different predictor models correspond to different feature sets , bma has the advantage over feature selection that its predictions tend to have lower variance on average in comparison to any single model . in this paper , we show for the first time that bma-nb classifier is possible to exactly evaluate bma over the exponentially-sized powerset of nb feature models in linear-time in the number of features ; this yields an algorithm about as expensive to train as a single nb model with all features , but yet provably converges to the globally optimal feature subset in the asymptotic limit of data . we evaluate this novel bma-nb classifier on a range of datasets showing that bma-nb classifier never underperforms nb -lrb- as expected -rrb- and sometimes offers performance competitive -lrb- or superior -rrb- to classifiers such as svms and logistic regression while taking a fraction of the time to train .",0
676,"This paper presents a high-quality voice morphing system that uses a linear transformation of the spectral envelope and pitch scaling to morph the voice of a source speaker into that of a target speaker. The system takes into account the high spectral variance of unvoiced sounds and uses glottal coupling to generate a natural phase dispersion. The paper evaluates the system using speaker identification scores and listening tests, which demonstrate the perceived audio quality of the voice morphing. Results show that the proposed system outperforms previous methods in terms of phase incoherence and unnatural phase dispersion, and achieves high audio quality. The system also preserves the prosody of the source speaker and can be used for various applications such as speech synthesis, voice conversion, and animation.",1
677,"voice morphing is a technique for modifying a source speaker 's speech to sound as if voice morphing was spoken by some designated target speaker . most of the recent approaches to voice morphing apply a linear transformation to the spectral envelope and pitch scaling to modify the prosody . whilst these methods are effective , they also introduce artifacts arising from the effects of glottal coupling , phase incoherence , unnatural phase dispersion and the high spectral variance of unvoiced sounds . a practical voice morphing system must account for these if high audio quality is to be preserved . this paper describes a complete voice morphing system and the enhancements needed for dealing with the various artifacts , including a novel method for synthesising natural phase dispersion . each technique is assessed individually and the overall performance of the voice morphing system evaluated using listening tests . overall voice morphing is found that the enhancements significantly improve speaker identification scores and perceived audio quality .",0
678,"This paper proposes a local occlusion detection method for deforming 3D scenes using well-defined local topological invariants. The method is designed to work with bounded deformations and to handle occlusions caused by deforming objects, such as cloth and hand motions. The mathematical representation of the deforming scenes is based on spatio-temporal derivatives, and the occlusion detector matches occlusions against a zero false positive rate. The proposed method is tested on image sequences of natural scenes with color variation, and the results show that the method can detect occlusions with high accuracy. The mathematical framework developed in this work can be used to improve the 3D structure of man-made and natural scenes.",1
679,"occlusions provide critical cues about the 3d structure of man-made and natural scenes . we present a mathematical framework and algorithm to detect and localize occlusions in image sequences of scenes that include deforming objects . our occlusion detector works under far weaker assumptions than other detectors . we prove that occlu-sions in deforming scenes occur when certain well-defined local topologi-cal invariants are not preserved . our framework employs these invariants to detect occlusions with a zero false positive rate under assumptions of bounded deformations and color variation . the novelty and strength of this methodology is that it does not rely on spatio-temporal derivatives or matching , which can be problematic in scenes including deforming objects , but is instead based on a mathematical representation of the underlying cause of occlusions in a deforming 3d scene . we demonstrate the effectiveness of the occlusion detector using image sequences of natural scenes , including deforming cloth and hand motions .",0
680,"This paper presents a language model adaptation approach that uses text classification to improve language models (LMs) for a specific domain. The proposed approach involves using k-means algorithm to classify the text and creating domain LMs and background LMs. The background LM is pruned to reduce the size of the LM and improve the relative word error rate reductions. The domain LMs and the pruned background LM are then combined using linear interpolation. Experiments were conducted using trigram LMs, and the results show that the proposed approach outperforms other LM adaptation methods. The proposed approach achieves significant improvements in LM performance, especially for domains with limited data. Therefore, this approach can be a valuable tool for improving the performance of LMs in various domains.",1
681,"in our paper , we divide the corpus into 8 domains through text classification using k-means algorithm , and calculate the trigram lms for each one . but the experiment shows the performance in some ones becomes worse . in order to solve this problem , we try to do the lm adaptation based on the domain lms . the adaptation is done by mixing the domain lms with the background lm by a linear interpolation . relative word error rate reductions of between 5 and 10 % over the pruned background lm are achieved .",0
682,"This paper proposes a Bayesian approach for learning control policies from trajectory preference queries. The querying process is active and involves selecting the most informative queries to maximize learning efficiency. A Bayesian model is used to infer the latent target policy from the trajectory preference queries, and a set of candidate policies are evaluated based on their likelihood under the inferred model. The proposed approach allows for more efficient policy learning compared to random selection of queries. Results show that the approach outperforms other state-of-the-art methods in terms of sample efficiency and query complexity.",1
683,"we consider the problem of learning control policies via trajectory preference queries to an expert . in particular , the learning control policies presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred . the learning control policies 's goal is to elicit a latent target policy from the expert with as few queries as possible . to tackle this problem we propose a novel bayesian model of the querying process and introduce two methods that exploit this bayesian model to actively select expert queries . experimental results on four benchmark problems indicate that our bayesian model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection .",0
684,"This paper presents a method for generating high performance pruned FFT implementations using a program generation system called Spiral. The proposed method uses Kronecker product notation to express the FFT computation as a matrix multiplication, which is then optimized for a multicore processor such as Intel Core2Duo. The program generation system automatically prunes the FFT computation by removing unused inputs, resulting in a smaller problem size that can be efficiently vectorized. Experimental results show that the proposed method achieves high performance for pruned FFT implementations while maintaining accuracy.",1
685,"we derive a recursive general-radix pruned cooley-tukey fast fourier transform -lrb- fft -rrb- algorithm in kronecker product notation . the algorithm is compatible with vectorization and parallelization required on state-of-the-art multicore cpus . we include the pruned fft algorithm into the program generation system spiral , and automatically generate optimized implementations of the pruned fft for the intel core2duo multicore processor . experimental results show that using the pruned fft can indeed speed up the fastest available fft implementations by up to 30 % when the problem size and the pattern of unused inputs and outputs are known in advance .",0
686,"This paper presents a method to improve the accuracy of a constraint dependency parser for German language by incorporating supertag information. Supertags are additional labels assigned to words in a sentence that encode syntactic and semantic information. The paper proposes a decision process that uses models of supertags to guide the parsing process by weighting the constraints. The effectiveness of this approach is evaluated in terms of parsing accuracy and supertag accuracy. Results show that the use of supertags leads to significant improvements in parsing accuracy, especially for sentences with high syntactic complexity. Overall, the paper demonstrates the potential of using supertags as a valuable source of information to enhance the performance of dependency parsers.",1
687,"we investigate the utility of supertag information for guiding an existing dependency parser of german . using weighted constraints to integrate the additionally available information , the decision process of the dependency parser of german is influenced by changing its preferences , without excluding alternative structural interpretations from being considered . the paper reports on a series of experiments using varying models of su-pertags that significantly increase the parsing accuracy . in addition , an upper bound on the accuracy that can be achieved with perfect supertags is estimated .",0
688,"This paper proposes a system for easy contextual intent prediction and slot detection. The system addresses the sequential tagging problem and reduces error rates for both intent prediction and slot detection. The approach is based on predicting intents and slots per-utterance and incorporates context and discourse information to improve accuracy. The system uses a combination of support vector machines (SVMs), hidden Markov models (HMMs), and conditional random fields (CRFs) with various features including SVM features and CRF features. Additionally, intra-session utterances are taken into account to improve performance. The proposed system is evaluated on several SLU tasks and achieves significant improvements in accuracy for both intent prediction and slot detection.",1
689,"we investigate the incorporation of context into the spoken language understanding -lrb- slu -rrb- sub-tasks of intent prediction and slot detection . using a corpus that contains information about whole sessions rather than just single utterances , we experiment with the incorporation of information from previous intra-session utterances into the slu tasks on a given utterance . for slot detection , we find no significant increase using crf features indicating slots in previous utterances . for intent prediction , we achieve error rate reductions of upto 8.7 % by incorporating the intent of the previous utterance as an svm feature , and similar gains when treating intent prediction as a sequential tagging problem with svm-hmms . u 1 get clip show me the -lsb- firefly -rsb- content − name -lsb- trailer -rsb- type show me the -lsb- firefly -rsb- content − name -lsb- trailer -rsb- type u 2 find info who directed -lsb- it -rsb- content − name − ref who directed -lsb- it -rsb- content − name − ref u 3 find content what else has -lsb- he -rsb- director − ref done what else has -lsb- he -rsb- director − ref done u 4 play content play -lsb- the avengers -rsb- content − name plane -lsb- avatars -rsb- content − name traditionally , both intents and -lsb- slots -rsb- are predicted per-utterance , while ignoring previous utterances within the session . however , the data is gathered not one utterance at a time but one session at a time ; each utterance occurs in the context of a larger discourse . we examine the effect of incorporating information from previous intra-session utterances -lrb- ab hinc , context -rrb- . context can serve as an additional source of information and help get around other errors such as those introduced during the asr process .",0
690,"This paper presents a method for evaluating the information content of documents using factoid analysis and human annotation. The approach involves identifying shared atomic information units and measuring information overlap between documents. A weighted factoid score is then computed for each document based on the number and quality of its factoids. The system rankings produced by this evaluation are shown to be stable over time, suggesting that the approach is a reliable way to evaluate document summaries. The paper also explores the use of string similarity and unigrams in factoid annotation, and provides insights into the intrinsic evaluation of summary quality.",1
691,"we present a new approach to intrinsic summary evaluation , based on initial experiments in van halteren and teufel -lrb- 2003 -rrb- , which combines two novel aspects : comparison of information content -lrb- rather than string similarity -rrb- in gold standard and system summary , measured in shared atomic information units which we call factoids , and comparison to more than one gold standard summary -lrb- in our data : 20 and 50 summaries respectively -rrb- . in this paper , we show that factoid annotation is highly reproducible , introduce a weighted factoid score , estimate how many summaries are required for stable system rankings , and show that the fac-toid scores can not be sufficiently approximated by unigrams and the duc information overlap measure .",0
692,"Abstract:

This paper presents an empirical study of an information synthesis task using an information synthesis testbed. The main goal of this study is to investigate the effectiveness of sentence overlap and n-gram overlap similarity metrics in measuring the quality of synthesized information. The study evaluates the performance of both metrics on the information synthesis task and compares their results. The findings suggest that sentence overlap is more effective than n-gram overlap in measuring the quality of synthesized information. The study provides insights into the effectiveness of different similarity metrics and can inform the development of more effective methods for evaluating information synthesis tasks.",1
693,"this paper describes an empirical study of the '' information synthesis '' task , defined as the process of -lrb- given a complex information need -rrb- extracting , organizing and interrelating the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . two main results are presented : a -rrb- the creation of an information synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b -rrb- an empirical comparison of similarity metrics between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports . a metric based on key concepts overlap gives better results than metrics based on n-gram overlap -lrb- such as rouge -rrb- or sentence overlap .",0
694,"This paper presents a new approach to 3D object recognition using analysis-by-synthesis. The proposed method leverages invariant features and geometric interpretations of the world to reconstruct candidate 3D object models from measured visual evidence. The forward synthesis model is used to generate visual templates for each candidate reconstruction. An inference strategy is then used to match the templates with the input data to detect and recognize objects. The approach is evaluated on several datasets using a ""brute-force"" approach and measured by visual evidence. Results show that this approach outperforms other similarity metrics such as n-gram overlap and sentence overlap. Overall, the analysis-by-synthesis approach is an effective method for 3D object recognition and reconstruction.",1
695,"we introduce a new approach for recognizing and reconstructing 3d objects in images . our approach is based on an analysis by synthesis strategy . a forward synthesis model constructs possible geometric interpretations of the world , and then selects the interpretation that best agrees with the measured visual evidence . the forward synthesis model synthesizes visual templates defined on invariant features . these visual templates are discriminatively trained to be accurate for inverse estimation . we introduce an efficient '' brute-force '' approach to inference that searches through a large number of candidate reconstructions , returning the optimal one . one benefit of such an approach is that recognition is inherently -lrb- re -rrb- constructive . we show state of the art performance for detection and reconstruction on two challenging 3d object recognition datasets of cars and cuboids .",0
696,"This paper presents a domain-independent binary classifier for automatic speech recognition (ASR) confidence scoring. The goal is to develop a system that can be applied to any domain without the need for retraining. The proposed system uses normalized mutual information to cluster data across different domains, which are then used to train cluster-specific classifiers. The cluster purity is also analyzed to ensure high-quality training data. Experimental results show that the domain-independent binary classifier outperforms single-classifier and ASR hypothesis classifiers in terms of accuracy and robustness across domains.",1
697,"this work addresses the problem of developing a domain-independent binary classifier for a test domain given labeled data from several training domains where the test domain is not necessarily present in training data . the domain-independent binary classifier accepts or rejects the asr hypothesis based on the confidence generated by the asr hypothesis . in the proposed approach , training data is grouped into across-domain clusters and separate cluster-specific classifiers are trained . one of the main findings is that the cluster purity and the normalized mutual information of the clusters are not very high which suggests that the domains might not necessarily be natural clusters . the performance of these cluster-specific classifiers is better than that of : -lrb- a -rrb- a single domain-independent binary classifier trained on data from all the domains , and -lrb- b -rrb- a set of classifiers trained separately for each of the training domains . at an operating point corresponding to low false accept , the correct accept of the proposed technique is on an average 2.3 % higher than that obtained by the single-classifier or the individual train-domain classifiers .",0
698,"This paper proposes a method for automatic clustering of visemes, which are atomic units of speech used in viseme-based audiovisual speech synthesis techniques. The many-to-one phoneme-to-viseme mapping used in these techniques often results in standardized viseme sets, but the quality of synthesis can be improved by clustering visemes based on visual speech information perceived by humans. The proposed method uses an optimization algorithm to group visemes based on their similarity in the visual domain, leading to improved audiovisual coherence in the synthetic speech. The effectiveness of the method is demonstrated by evaluating the perceived quality of the synthesized speech using the MPEG-4 standard for visemes.",1
699,"a common approach in visual speech synthesis is the use of visemes as atomic units of speech . in this paper , phoneme-based and viseme-based audiovisual speech synthesis techniques are compared in order to explore the balancing between data availability and an improved audiovisual coherence for synthesis optimization . a technique for automatic viseme clustering is described and it is compared to the standardized viseme set described in mpeg-4 . both objective and subjective testing indicated that a phoneme-based approach leads to better synthesis results . in addition , the test results improve when more different visemes are defined . this raises some questions on the widely applied viseme-based approach . it appears that a many-to-one phoneme-to-viseme mapping is not capable of describing all subtle details of the visual speech information . in addition , with viseme-based synthesis the perceived synthesis quality is affected by the loss of audiovisual coherence in the synthetic speech .",0
700,"This paper presents an approach for automatic word stress marking and syllabification for Catalan text-to-speech systems. The proposed method employs linguistically rule-based automatic algorithms, including orthographic and phonological syllabification algorithms, and stress marker algorithms. The orthographic syllabification algorithm is a hyponym of linguistically rule-based automatic algorithms, and it is used in conjunction with the phonological syllabification algorithm. The stress marker algorithm is also a hyponym of linguistically rule-based automatic algorithms, and it is used in conjunction with the orthographic syllabification algorithm to determine word stress markers. The proposed approach achieved high word accuracy rates in stress and syllable prediction, and it improves synthetic intelligibility and prosody prediction. The results show the effectiveness of using linguistically rule-based automatic algorithms in Catalan text-to-speech conversion.",1
701,"stress and syllabification are essential attributes for several components in text-to speech systems . stress are responsible for improving grapheme-to-phoneme conversion rules and for enhancing the synthetic intelligibility , since stress and syllable are key units in prosody prediction . this paper presents three linguistically rule-based automatic algorithms for catalan text-to-speech conversion : a word stress marker , an orthographic syllabification algorithm and a phonological syllabification algorithm . the linguistically rule-based automatic algorithms were implemented and tested . the results gave rise to the following word accuracy rates : 100 % for the stress marker algorithm , 99.7 % for the orthographic syllabification algorithm and 99.8 % for the phonological syllabification algorithm .",0
702,This paper investigates the naturalness of an utterance based on the automatically retrieved commonsense information. The authors examine the relationship between a user's behavior and the common knowledge that is automatically retrieved from text-mining and commonsense processing. They propose a holistic system that combines these two approaches to generate com-monsensical utterances. A word-spotting method is used to identify keywords that trigger the retrieval of commonsense knowledge. The authors evaluate the naturalness of the generated utterances through affective computing techniques. Their findings demonstrate the potential of using commonsense processing to improve the naturalness of language generation systems and highlight the importance of incorporating contextual knowledge in NLP achievements.,1
703,"in this research we investigated user 's behavior while facing a system coping with common knowledge about keywords and compared it with not only classic word-spotting method but also with random text-mining . we show how even a simple implementation of our idea can enrich the conversation and increase the naturalness of computer 's utterances . our results show that even very com-monsensical utterances are more natural than classic approaches and also methods we developed to make a conversation more interesting . for arousing opinion exchange during the session , we will also briefly introduce our idea of combining latest nlp achievements into one holistic system where the main engine we want to base on commonsense processing and affective computing .",0
704,"This paper presents a parameterless algorithm for detecting line segments and elliptical arcs in natural images. The algorithm utilizes a non-iterative ellipse fitting technique to detect elliptical features and a word-spotting method to identify line segments. Unlike many existing approaches, this algorithm does not require manual parameter tuning, making it more user-friendly. The proposed method is evaluated on both computer-generated and natural images and is shown to achieve high accuracy in detecting elliptical features. The algorithm is especially effective in detecting elliptical arcs with enhanced ellipse fitting. Overall, the results demonstrate the effectiveness of the proposed method for line segment and elliptical arc detection in natural images without the need for manual parameter tuning.",1
705,"we propose a combined line segment and elliptical arc detector , which formally guarantees the control of the number of false positives and requires no parameter tuning . the accuracy of the detected elliptical features is improved by using a novel non-iterative ellipse fitting technique , which merges the algebraic distance with the gradient orientation . the performance of the elliptical arc detector is evaluated on computer-generated images and on natural images .",0
706,"This paper presents a differential tracking method for non-rigid objects based on a spatial-appearance model (SAM). The SAM allows recovery of all motion parameters and handles large object scale changes and dramatic appearance deformations. The proposed method employs localized and global matching criteria, including matching histograms and global spatial structures, to maximize the likelihood of tracking. A closed form solution is presented for differential motion analysis, which involves scaling and rotation, and handles local appearance variations. The method is shown to be effective in tracking non-rigid objects under partial occlusions and dramatic appearance changes. Experimental results demonstrate that the proposed method outperforms pixel-based SSD and other localized matching criteria in terms of tracking accuracy.",1
707,"a fundamental issue in differential motion analysis is the compromise between the flexibility of the matching criterion for image regions and the ability of recovering the motion . localized matching criteria , e.g. , pixel-based ssd , may enable the recovery of all motion parameters , but it does not tolerate much appearance changes . on the other hand , global criteria , e.g. , matching histograms , can accommodate dramatic appearance changes , but may be blind to some motion parameters , e.g. , scaling and rotation . this paper presents a novel closed form solution that integrates the advantages of both in a principled way based on a spatial-appearance model that combines local appearances variations and global spatial structures . this closed form solution can capture a large variety of appearance variations that are attributed to the local non-rigidity . at the same time , this closed form solution enables efficient recovery of all motion parameters . a maximum likelihood matching criterion is defined and rigorous analytical results are obtained that lead to a closed form solution to motion tracking . very encouraging results demonstrate the effectiveness and efficiency of the proposed closed form solution for tracking non-rigid objects that exhibit dramatic appearance deformations , large object scale changes and partial occlusions .",0
708,"This paper proposes a speech quality measure for VoIP systems based on the wavelet-based Bark coherence function. The Bark coherence function is a type of coherence function that measures the correlation coefficients between two signals as a function of frequency. The proposed approach uses a wavelet series expansion to compute the Bark coherence function and employs signal decomposition to handle variable delay and linear distortions. The method also incorporates a cognition module that takes into account the loudness of speech signals. The performance of the proposed approach is compared with the widely used Perceptual Speech Quality Measure (PSQM) and it is found that the wavelet-based Bark coherence function outperforms PSQM in measuring the quality of VoIP speech data, especially in the presence of non-stationary noise and other types of distortions that affect coherence function measures. The proposed approach can be used for digital mobile systems that rely on VoIP for voice communication over the internet.",1
709,"the bark coherence function -lsb- 1 -rsb- defines a coherence function with loudness speech as a new cognition module , robust to linear distortions due to the analog interface of digital mobile system . preliminary experiments have shown the superiority of bark coherence function over current measures . in this paper , a new bark coherence function suitable for voip is developed . the new bark coherence function is based on the wavelet series expansion that provides good frequency resolution while keeping good time locality . the proposed wavelet based bark coherence function is robust to variable delay often observed in internet telephony such as voip . we also show that the refinement of time synchronization after signal decomposition can improve the performance of the wavelet based bark coherence function . the time synchronization was performed with voip speech data . the correlation coefficients and the standard error of estimates computed using the wavelet based bark coherence function showed noticeable improvement over the psqm that is recommended by wavelet based bark coherence function .",0
710,"This paper presents an approach for efficient construction of long-range language models using log-linear interpolation. The authors demonstrate how to improve the performance of language models by combining word and class models, smoothed 4-gram language model, and grammar-based approaches. They show that by rescoring word lattices with a combination of these models, the overall performance of the system can be improved. The authors also evaluate their approach on the Penn Treebank dataset and demonstrate that it outperforms previous approaches. The proposed log-linear interpolation technique allows for efficient combination of different language models and can be applied to medium-sized vocabulary tasks such as the Wall Street Journal task. Overall, the paper provides a novel approach for constructing long-range language models that can improve the accuracy of natural language processing tasks.",1
711,"in this paper we examine the construction of long-range language models using log-linear interpolation and how this can be achieved effectively . particular attention is paid to the efficient computation of the normalisation in the long-range language models . using the penn treebank for experiments we argue that the perplexity performance demonstrated recently in the literature using grammar-based approaches can actually be achieved with an appropriately smoothed 4-gram language model . using such a model as the baseline , we demonstrate how further improvements can be obtained using log-linear interpolation to combine distance word and class models . we also examine the performance of similar model combinations for rescoring word lattices on a medium-sized vocabulary wall street journal task .",0
712,"This paper discusses the use of the Hidden Markov Model Toolkit (HTK) for broadcast news transcription. The authors propose several techniques to improve the performance of large vocabulary speech recognition systems, including maximum likelihood linear regression and unsupervised model adaptation. They also use cache-based language modeling to improve the efficiency of the system. The paper presents the results of experiments on clean and noisy read speech tasks and evaluates the system using decoder-guided segmentation and segment clustering features. The results show that the proposed techniques improve the performance of the system and make it suitable for broadcast news transcription.",1
713,"this paper examines the issues in extending a large vocabulary speech recognition system designed for clean and noisy read speech tasks to handle broadcast news transcription . results using the 1995 darpa h 4 e v aluation data set are presented for dierent front-end analyses and use of unsupervised model adaptation using maximum likelihood linear regression . the large vocabulary speech recognition system for the 1996 h4 evaluation is then described . large vocabulary speech recognition system includes a number of new features over previous htk large vocabulary systems including decoder-guided segmentation , segment clustering , cache-based language modelling , and combined map and mllr adaptation . the large vocabulary speech recognition system runs in multiple passes through the data and the detailed results of each pass are given .",0
714,"This paper proposes a two-step approach to minimize the total power consumption for multiuser video transmission over CDMA wireless networks. The approach considers video coding bit rate and video compression complexity along with transmitter power as the key parameters affecting the quality of video transmission. The first step involves the use of a fast algorithm to compute the computation burden and transmitter power required to transmit the video with an acceptable quality level. In the second step, a full search complexity algorithm is used to determine the optimal combination of video compression complexity and transmitter power for minimizing the total power consumption. The proposed approach achieves a good balance between video quality and power consumption, making it suitable for CDMA cell networks with limited power resources.",1
715,"in this work , we consider a cdma cell with multiple terminals transmitting video signals . we minimize the sum of signal processing and transmitter power while the received quality at each terminal is guaranteed . the system parameters to be adjusted include video coding bit rate , video compression complexity and transmitter power . instead of full search in the space of bit rate , complexity , transmitter power ¡ for all users , we design a two-step fast algorithm to reduce the computation burden in the base station . in our two-step fast algorithm , the search in the base station is over the space of complexity only . our results indicate that for the same class of video users , the one who is closer to the base station compresses at less complexity . this is used to further reduce the computation required by our two-step fast algorithm .",0
716,"This paper presents a new approach for Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization. The proposed algorithm is used to extract sparse row features from multiple views and cluster them simultaneously. The method aims to discover co-clustering structures by identifying clusters that are shared across multiple views. The algorithm is evaluated on benchmark datasets, and its performance is compared with other state-of-the-art multi-view co-clustering methods. Results show that the proposed approach achieves better clustering performance, while reducing the computation burden.",1
717,"when multiple views of data are available for a set of subjects , co-clustering aims to identify subject clusters that agree across the different views . we explore the problem of co-clustering when the underlying clusters exist in different sub-spaces of each view . we propose a proximal alternating linearized minimization algorithm that simultaneously decomposes multiple data matrices into sparse row and columns vectors . this proximal alternating linearized minimization algorithm is able to group subjects consistently across the views and simultaneously identify the subset of features in each view that are associated with the clusters . the proposed proximal alternating linearized minimization algorithm can globally converge to a critical point of the problem . a simulation study validates that the proposed proximal alternating linearized minimization algorithm can identify the hypothesized clusters and their associated features . comparison with several latest multi-view co-clustering methods on benchmark datasets demonstrates the superior performance of the proposed proximal alternating linearized minimization algorithm .",0
718,"This paper proposes a method for markerless kinematic model and motion capture from volume sequences of human subjects. The method is model-free and does not require markers, making it suitable for capturing articulated kinematic structures. The proposed approach uses arbitrary kinematic topology to construct a kinematic model that can be fit to both synthetically generated and captured volume sequences. The kinematic model is represented by a skeleton curve that captures joint angles and nonlinear axes, which can be used to generate motion sequences and kinematic postures. The method is evaluated on benchmark datasets and demonstrates promising results for markerless motion capture of articulated kinematic structures.",1
719,"we present an approach for model-free markerless motion capture of articulated kinematic structures . this approach is centered on our method for generating underlying nonlinear axes -lrb- or a skeleton curve -rrb- of a volume of genus zero -lrb- i.e. , without holes -rrb- . we describe the use of skeleton curves for deriving a kinematic model and motion -lrb- in the form of joint angles over time -rrb- from a captured volume sequence . our motion capture method uses a skeleton curve , found in each frame of a volume sequence , to automatically determine kinematic postures . these skeleton curves are aligned to determine a common kinematic model for the volume sequence . the derived kinematic model is then reapplied to each frame in the volume sequence to find the motion sequence suited to this kinematic model . we demonstrate our method on several types of motion , from synthetically generated volume sequences with an arbitrary kinematic topology , to human volume sequences captured from a set of multiple calibrated cameras .",0
720,This paper proposes a new approach for guiding a Head-driven Phrase Structure Grammar (HPSG) parser by incorporating both semantic and pragmatic expectations. The aim is to improve natural language understanding by using socio-pragmatic context and knowledge to guide syntactic parsing. The proposed approach combines syntactic and socio-pragmatic knowledge to narrow down the search space and improve parsing accuracy. The paper introduces a parser that incorporates both semantic and pragmatic expectations and evaluates its performance on benchmark datasets. The results show that the proposed approach outperforms traditional syntactic parsers in terms of accuracy and speed. The paper concludes by discussing the potential of the approach to improve natural language understanding and generation.,1
721,"1 efficient natural language generation has been successfully demonstrated using highly compiled knowledge about speech acts and their related social actions . a design and prototype implementation of a parser which utilizes this same pragmatic knowledge to efficiently guide parsing is presented . such guidance is shown to prune the search space and thus avoid needless processing of pragmatically unlikely constituent structures . introduction the use of purely syntactic knowledge during the parse phase of natural language understanding yields considerable local ambiguity -lrb- consideration of impossible subeonstituents -rrb- as well global ambiguity -lrb- construction of syntactically valid parses not applicable to the socio-pragmatic context -rrb- . this research investigates bringing socio-pragmatic knowledge to bear during the parse , while maintaining a domain independent grammar and parser . the particular technique explored uses knowledge about the pragmatic context to order the consideration of proposed parse constituents , thus guiding the parser to consider the best -lrb- wrt the expectations -rrb-",0
722,"This paper proposes a unified learning scheme, called Bayesian-Kullback Ying-Yang machines, which combines major supervised and unsupervised learning methods. The scheme integrates three learning methods: Bayesian-Kullback learning scheme, least square learning, and EM & EM algorithm, to achieve maximum information preservation and learn Bayesian representations. Information geometry and joint density are also used to guide the learning process. The paper presents the ying-yang machine and the Helmholtz machine as specific implementations of the proposed scheme. The proposed approach provides a comprehensive framework for learning with both pragmatic and syntactic knowledge, and is demonstrated on various learning tasks, including natural language understanding and generation.",1
723,"a bayesian-kullback learning scheme , called ying-yang machine , is proposed based on the two complement but equivalent bayesian representations for joint density and their kullback divergence . not only the bayesian-kullback learning scheme unifies existing major supervised and unsu-pervised learnings , including the classical maximum likelihood or least square learning , the maximum information preservation , the em & em algorithm and information geometry , the recent popular helmholtz machine , as well as other learning methods with new variants and new results ; but also the bayesian-kullback learning scheme provides a number of new learning methods .",0
724,"This paper proposes a suboptimal embedding algorithm with low complexity for binary data hiding. The suboptimal algorithm is based on weight approximation embedding (WAE), which is a linear embedding code. The algorithm uses a parity check matrix to determine the embedding procedure, and a maximal likelihood algorithm is used to find the coset leader. The proposed suboptimal WAE algorithm has lower complexity than the optimal algorithm and achieves similar embedding distortion. The paper presents a detailed analysis of the embedding complexity and provides experimental results on binary data sets to demonstrate the effectiveness of the proposed algorithm. Overall, this work provides a promising approach for suboptimal binary data hiding using WAE with low complexity.",1
725,"-- a novel suboptimal hiding algorithm for binary data based on weight approximation embedding , wae , is proposed . given a specified embedding rate , this suboptimal hiding algorithm exhibits an advantage of efficient binary embedding with reduced embedding complexity . the suboptimal hiding algorithm performs an embedding procedure through a parity check matrix . the optimal embedding based on maximal likelihood algorithm aims to locate the coset leader to minimize the embedding distortion . on the contrary , the suboptimal hiding algorithm looks for a target vector close to the coset leader in an efficiently iterative manner . given an linear embedding code c -lrb- n , k -rrb- , the embedding complexity using the optimal suboptimal hiding algorithm is o -lrb- 2 k -rrb- , while the complexity in the suboptimal wae is reduced to o -lrb- sk -rrb- where s is the average iterations .",0
726,"This paper presents a data-driven approach to concept-to-text generation in the ATIS domain using a discriminative system that utilizes a corpus of database records. The proposed method employs a probabilistic context-free grammar and a hyper-graph structure with weighted hypergraph scoring to perform content selection and generate text. A decoding algorithm is used to parse the problem, and local and global features are extracted from the input. The paper also includes a judgment elicitation study to evaluate the effectiveness of the data-driven method and the discriminative system. Results are compared using the BLEU metric, and the paper demonstrates that the discriminative reranking approach outperforms traditional generative methods.",1
727,"this paper proposes a data-driven method for concept-to-text generation , the task of automatically producing textual output from non-linguistic input . a key insight in our data-driven method is to reduce the tasks of content selection -lrb- '' what to say '' -rrb- and surface realization -lrb- '' how to say '' -rrb- into a common parsing problem . we define a probabilistic context-free grammar that describes the structure of the input -lrb- a corpus of database records and text describing some of them -rrb- and represent probabilistic context-free grammar compactly as a weighted hypergraph . the hyper-graph structure encodes exponentially many derivations , which we rerank discriminatively using local and global features . we propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting . experimental evaluation on the atis domain shows that our data-driven method outperforms a competitive discriminative system both using bleu and in a judgment elicitation study .",0
728,"This paper proposes a novel approach to continuous speech recognition based on a layered self-adjusting decoding graph. The decoding network is designed with a two-level hashing structure, which enables fast network expansion while maintaining the self-adjusting capability of the graph. A stack decoding scaffolding layer is introduced to efficiently decode the input, and recognition resources are evaluated for efficient decoding. The proposed method also includes a dynamic decoding approach to release decoder resources as needed during recognition. The paper demonstrates that the layered self-adjusting decoding graph improves recognition accuracy compared to traditional decoding networks. Results show that the proposed method outperforms general re-entrant decoding networks and highlights the benefits of using a layered self-adjusting decoding graph for continuous speech recognition.",1
729,"in this paper , an approach of continuous speech recognition based on layered self-adjusting decoding graph is described . it utilizes a scaolding layer to support fast network expansion and releasing . a two level hashing structure is also described . it introduces self-adjusting capability i n dynamic decoding on general re-entrant decoding network . in stack decoding , the scaolding layer in the proposed approach enables the decoder to look several layers into the future so that long span inter-word context dependency can be exactly preserved . experimental results indicate that highly ecient decoding can be achieved with a signicant savings on recognition resources .",0
730,"This paper presents a digital post compensation method for correcting sampling instant errors in the track-and-hold portion of a high-speed, high-resolution analog-to-digital converter (ADC). The proposed method utilizes mathematical models, including a nonlinear transfer function model, to evaluate and correct for the effects of nonlinear behavior in the input signal. The method employs an energy-free approach to perform background calibration and configuration of the ADC, and it is shown to be effective for both bipolar and MOS technologies. The paper also discusses the impact of the proposed digital compensation method on the spurious free dynamic range of the ADC. The proposed method is demonstrated to significantly improve ADC performance by correcting for sampling instant errors in track-and-hold circuits. Results show that the digital compensation method improves the accuracy and precision of the ADC, making it suitable for a range of high-speed and high-resolution applications.",1
731,track-and-hold -lrb- th -rrb- circuits in the front end of high-speed high-resolution analog-to-digital converters typically limit adc performance at high input signal frequencies . this paper develops mathematical models for adc implemented in both bipolar and mos technologies . the mathematical models are derived by analyzing the sampling instant error and reveal that the nonlinear behavior is dependent on the input signal and digital post compensation method 's derivatives . a digital post compensation method is then presented with digital post compensation method 's coefficients estimated using an energy-free method in a background calibration configuration . simulation results on a nonlinear th model show that the proposed digital post compensation method achieves a significant improvement in the spurious free dynamic range . the digital post compensation method is also applied to a commercially available adc to demonstrate digital post compensation method 's effectiveness .,0
732,"This paper introduces the concept of ""anytime belief revision,"" which provides a computer-based belief revision architecture that enables a belief revision system to operate using an anytime decision procedure. The proposed approach is based on the AGM paradigm for belief revision, and it is designed to work with a variety of theory bases. The paper describes the belief revision strategy, which includes guidelines for performing minimal change and maxi-adjustments to the belief set. The proposed approach employs a protomethodology for implementing anytime belief revision, and it utilizes a theorem prover to generate intelligent behavior. The paper also discusses the complexity of the proposed approach and provides recommendations for implementing the belief revision module. Results show that the proposed approach is effective for a range of belief revision systems, and it outperforms existing methods for belief revision. The proposed anytime belief revision architecture is shown to be flexible and scalable, making it suitable for a range of intelligent behavior applications.",1
733,"belief revision is a ubiquitous process underlying many forms of intelligent behaviour . the agm paradigm is a powerful framework for modeling and implementing belief revision systems based on the principle of minimal change ; agm paradigm provides a rich and rigorous foundation for computer-based belief revision architectures . maxi-adjustment is a belief revision strategy for theory bases that can be implemented using a standard theorem prover , and one that has been used successfully for several applications . in this paper we provide an anytime decision procedure for maxi-adjustments , and study its complexity . furthermore , we outline a set of guidelines that serve as a protomethodology for building belief revision systems employing a maxi-adjustment . the anytime decision procedure is under development in the belief revision module of the cin project .",0
734,"This paper proposes a locally optimal, buffer-constrained motion estimation and mode selection algorithm for video sequences. The goal of the algorithm is to achieve R-D optimal motion search and mode selection while satisfying a constant bit rate channel constraint. The proposed approach utilizes a nominal method for motion search and optimal mode selection, while taking into account the buffer constraints of the system. Lagrange multipliers are used to balance the distortion-rate trade-off in the video sequences. The paper discusses the motion search process and its role in achieving the R-D optimal solution. Results show that the proposed algorithm outperforms existing methods for video compression in terms of quality and compression ratio. The proposed algorithm is shown to be robust and suitable for a range of video sequences, making it a promising approach for real-world video compression applications.",1
735,"we describe a method of using a lagrange multiplier to make a locally optimal trade off between rate and distortion in the motion search for video sequences , while maintaining a constant bit rate channel . simulation of this method shows that it gives up to 3.5 db psnr improvement in a high motion sequence . a locally rate-distortion -lrb- r-d -rrb- optimal mode selection mechanism is also described . this method also gives significant quality benefit over the nominal method . though the benefit of these techniques is significant when used separately , when the optimal mode selection is combined with the r-d optimal motion search , it does not perform much better than the codec does with only the r-d optimal motion search .",0
736,"This paper presents a method for computing the paraphrasability of syntactic variants using web snippets. The authors address the challenge of data sparseness problem and propose the use of web snippets as an effective way to obtain a large-scale knowledge-base of paraphrases. The method is based on computing semantic equivalence between syntactic variants of predicate phrases, and the authors demonstrate its effectiveness for various natural language processing tasks. The paper also discusses the use of distributional similarity measures for computing paraphrasability and syntactic substitutability of predicate phrases. The proposed approach shows promising results and can be a valuable tool for computational linguistics.",1
737,"in a broad range of natural language processing tasks , large-scale knowledge-base of paraphrases is anticipated to improve their performance . the key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability , i.e. , paraphrasability , between given pair of expressions . this paper addresses the issues of computing paraphrasability , focusing on syntactic variants of predicate phrases . our model estimates paraphrasability based on traditional distributional similarity measures , where the web snippets are used to overcome the data sparseness problem in handling predicate phrases . several feature sets are evaluated through empirical experiments .",0
738,"This paper proposes a novel associative memory model based on sparse signal recovery techniques. Unlike traditional binary or q-ary Hopfield neural networks, the proposed model can store a super-polynomial or exponential number of n-length vectors using a sparse recovery of signals approach that requires only a near-linear number of coordinates. The model employs a neurally feasible algorithm for solving the sparse recovery problem, which involves generic random models and linear constraints. The proposed associative memory model also features a nearest neighbor structure that allows for efficient retrieval of stored vectors. The paper presents an iterative algorithm for implementing the model and demonstrates its effectiveness through simulations.",1
739,"an associative memory is a structure learned from a dataset m of vectors -lrb- signals -rrb- in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m -lrb- nearest neighbor -rrb- is provided as output , preferably via a fast iterative algorithm . traditionally , binary -lrb- or q-ary -rrb- hopfield neural networks are used to model the above structure . in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals . our basic premise is simple . for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy . provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem . assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o -lrb- n -rrb- . furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .",0
740,"This paper proposes a new class-based variable memory length Markov model for natural language processing. The model is an extension of the word-based variable memory length Markov model and is designed to capture the statistical dependencies between words in text. The class-based model uses a class-based probabilistic suffix tree to represent the text and provides a more efficient way of storing the model parameters. The proposed model is compared to the word-based bi-gram model, and the experimental results show that the class-based model outperforms the word-based model in terms of model size and accuracy. Additionally, the paper introduces a learning algorithm that can automatically learn the word-class relation from a large corpus of text. The algorithm is based on a node splitting strategy that minimizes the entropy of the class distribution. Overall, the proposed class-based variable memory length Markov model provides an effective way to capture the complex statistical dependencies between words in natural language text.",1
741,"in this paper , we present a class-based variable memory length markov model and its learning algorithm . this is an extension of a variable memory length markov model . our class-based variable memory length markov model is based on a class-based probabilistic suffix tree , whose nodes have an automatically acquired word-class relation . we experimentally compared our new class-based variable memory length markov model with a word-based bi-gram model , a word-based tri-gram model , a class-based bi-gram model , and a word-based variable memory length markov model . the results show that a class-based variable memory length markov model outperforms the other models in perplexity and model size .",0
742,"This paper proposes a method for classification problems that jointly uses dynamical classifiers and ambiguity plane features. The approach is motivated by the need for explicit modeling of long-term context in addition to the shorter-term context captured by ambiguity plane features. The method is applied to acoustically monitoring cutter wear in titanium machining, where sparsely labeled training data is available. Hidden Markov models are used to model the state of the system, and both static classification techniques and dynamic statistical models are employed. The results demonstrate the effectiveness of the joint use of these two types of features.",1
743,"this paper argues for using ambiguity plane features within dynamic statistical models for classification problems . the relative contribution of the two dynamic statistical models are investigated in the context of acoustically monitoring cutter wear during milling of titanium , an application where it is known that standard static classification techniques work poorly . experiments show that explicit modeling of long-term context via a hidden markov model state improves performance , but mainly by using this to augment sparsely labeled training data . an additional performance gain is achieved by using the shorter-term context of ambiguity plane features .",0
744,"This paper presents a method for improving text-to-speech synthesis using variable-length acoustic units. Traditional text-to-speech systems rely on concatenation-based systems with fixed-length phonetic descriptions. However, this method can result in unnatural sounding speech. The proposed approach uses variable-length acoustic units that can be combined to create a more natural and fluent acoustic message. The paper demonstrates that the use of variable-length acoustic units improves the quality of synthesized speech over traditional concatenation-based systems.",1
745,"the best voices in text-to-speech synthesis are currently obtained via acoustic units concatenation-based systems . in such acoustic units concatenation-based systems , the choice of units whose concatenations will produce an acoustic message is a crucial stage . moreover , it can be observed that current tts systems use acoustic units which most often correspond to variable-length phonetic descriptions . in this article , an original framework is proposed which allows the automatic determination of an optimum set of variable-length acoustic units .",0
746,"This paper proposes a low-power hybrid structure of digital matched filters (DMFs) for direct sequence spread spectrum (DSSS) systems. The DMFs have become a critical component in DSSS systems, and their power consumption is a major concern. To address this issue, the authors explore different low-power approaches, including the 128-tap DMF transposed-form structure and the direct-form structure, and propose a hybrid structure that combines the advantages of both. The graph shows that the direct-form structure and the transposed-form structure are used for the low-power approaches and the hybrid structure, respectively. The DMFs are used for DSSS systems, and the low-power approaches are used for the DMFs. The proposed hybrid structure can reduce the area overhead and power consumption while maintaining high performance. The experimental results demonstrate the effectiveness of the proposed approach.",1
747,"1 this paper presents a low-power structure of digital matched filters , which is proposed for direct sequence spread spectrum systems . traditionally , low-power approaches for dmfs are based on either the transposed-form structure or the direct-form one . a new hybrid structure that employs the direct-form structure for local addition and the transposed-form structure for global addition is used to take advantages of both structures . for a 128-tap dmf , the proposed dmfs that processes 32 addends a cycle consumes 46 % less power at the expense of 6 % area overhead as compared to the state-of-the-art low-power dmfs -lsb- 7 -rsb- .",0
748,"This paper investigates the knowledge compilation properties of tree-of-binary decision diagrams (BDDs). Knowledge compilation is a technique for transforming a knowledge representation into a more efficient form. In particular, the authors focus on the properties of the tree-of-BDDs (ToB) compilation, which is a popular knowledge compilation form. They compare ToB with the d-dnnf compilation, which is another well-known knowledge compilation form. The graph shows that the matching ToB and the d-dnnf compilation are compared. The authors analyze the properties of ToB, including its tree width and its decomposition heuristic, and evaluate its performance in clausal entailment and CNF complexity. The results show that ToB has several advantages over d-dnnf, including better performance in some scenarios. The paper contributes to the understanding of the properties of ToB and provides insights into its use in knowledge compilation.",1
749,"we present a cnf to tree-of-bdds -lrb- cnf -rrb- compiler with complexity at most exponential in the tree width . we then present algorithms for interesting queries on cnf . although some of the presented query algorithms are in the worst case exponential in the tree width , our experiments show that cnf can answer non-trivial queries like clausal entailment in reasonable time for several realistic instances . while our tob-tool compiles all the used 91 instances , d-dnnf compilation failed for 12 or 8 of them based on the decomposition heuris-tic used . also , on the succeeded instances , a d-dnnf compilation is up to 1000 times larger than the matching tob . the tob compilations are often an order of magnitude faster than the d-dnnf compilation . this makes cnf a quite interesting knowledge compilation form .",0
750,"This paper presents protocols for real-time multimedia data transmission over the Internet. Real-time multimedia data streams, such as those used for signal compression applications and networked multimedia services, require timely and reliable delivery over non-guaranteed quality of service networks. The authors discuss the challenges associated with transmitting real-time multimedia data over the Internet protocol and propose architectural elements and coding systems that can help to address these challenges. The graph shows that the non-guaranteed quality of service networks are used for real-time multimedia data streams, while the architectural elements are used for real-time data transmission. The paper also provides an overview of network protocols that can be used for real-time multimedia data transmission, including those that provide error control, congestion control, and packet retransmission. The experimental results demonstrate the effectiveness of the proposed protocols in ensuring reliable and timely delivery of real-time multimedia data over the Internet. This paper is relevant to researchers and practitioners interested in the design and implementation of protocols for real-time multimedia data transmission.",1
751,"the explosive growth of the internet and the intranets have attracted a great deal of attention to the implementation and performance of networked multimedia services . which involve the transport of real-time multimedia data streams over non-guaranteed quality of service networks based on the internet protocol . in this paper , i present an overview of the existing architectural elements supporting real-time data transmission over the internet . effective implementations of such systems require a thorough understanding of both the network protocols and the coding systems used for compressing the signals to be transmitted in real-time . the paper includes a section discussing the issues to be considered in designing signal compression applications suitable for network use .",0
752,"This paper addresses the automatic identification of learners' language background based on their writing in Czech. The authors focus on identifying speakers of Indo-European languages and propose a method for automatic identification using non-content based features from highly inflectional Czech data. The method employs a support vector machine (SVM) classifier for binary classification. The graph shows that highly inflectional data is used for non-content based features, and the SVM classifier is used for binary classification. The authors evaluate the performance of their method using precision and recall metrics and compare it to a baseline approach based on orthography. The experimental results demonstrate the effectiveness of the proposed method, which achieves higher precision and recall than the baseline approach. This paper contributes to the field of language identification and can be useful for researchers and practitioners interested in developing automatic identification systems for learners' language backgrounds.",1
753,"the goal of this study is to investigate whether learners ' written data in highly inflectional czech can suggest a consistent set of clues for automatic identification of the learners ' l1 background . for our experiments , we use texts written by learners of czech , which have been automatically and manually annotated for errors . we define two classes of learners : speakers of indo-european languages and speakers of non-indo-european languages . we use an svm classifier to perform the binary classification . we show that non-content based features perform well on highly inflectional data . in particular , features reflecting errors in orthography are the most useful , yielding about 89 % precision and the same recall . a detailed discussion of the best performing features is provided .",0
754,"This paper proposes a new class of lifting wavelet transform for guaranteeing losslessness of specific signals. The authors focus on the problem of lossy coding, which can occur during the rounding of signal values and scaling coefficient values in traditional lifting steps. The proposed method uses 9/7 wavelet scaling pairs and a white balance wavelet transform to ensure lossless coding of specific signals. The graph shows that the white balance signals are used for wavelet transform, and the scaling pairs are used for lossy coding. The authors evaluate the performance of their method using LSI processors and compare it to traditional lifting wavelet transforms. The experimental results demonstrate that the proposed method achieves losslessness of specific signals while maintaining good compression performance. This paper is relevant to researchers and practitioners interested in wavelet transform and lossless coding of specific signals.",1
755,"this paper proposes a new class of lifting wavelet transform which can guarantee losslessness of specific signals , e.g. white balance . the 5/3 wavelet transform composed of two lifting steps can reconstruct an input signal without any loss and has been utilized for lossless coding . the 9/7 wavelet contains two more lifting steps and two scaling pairs for effective lossy coding . however the losslessness is not guaranteed due to rounding of signal values and scaling coefficient values . this paper analyzes condition on word length -lrb- wl -rrb- and bit depth -lrb- bd -rrb- for the losslessness and proposes a new class of wavelet transform with '' dc lossless '' property which is a kind of specific losslessness . this can be utilized as a standard condition for algorithms or lsi processors to guarantee no error from the wavelet transform for white balance signals .",0
756,"This paper presents an energy minimization algorithm for solving binary and multilabel problems on 4-connected lattices using elimination. The authors propose an elimination algorithm to remove variables in a submodular function, which reduces the energy function to a smaller size. The eliminated variables are then added back using a back-substitution technique. The proposed algorithm is based on the alpha-expansion algorithm and graph-cuts/max-flow techniques. The graph shows that the alpha-expansion is used for multilabel problems, and the graph-cuts/max-flow is used for functions. The proposed algorithm is evaluated on images, and the results show that it achieves comparable or better performance than existing state-of-the-art algorithms. This paper is relevant to researchers and practitioners interested in energy minimization algorithms, submodular problems, and 4-connected lattices.",1
757,"we describe an energy minimization algorithm for functions defined on 4-connected lattices , of the type usually encountered in problems involving images . such functions are often minimized using graph-cuts/max-flow , but this energy minimization algorithm is only applicable to submodular problems . in this paper , we describe an energy minimization algorithm that will solve any binary problem , irrespective of whether it is submodular or not , and for multilabel problems we use alpha-expansion . the energy minimization algorithm is based on the elimination algorithm , which eliminates nodes from the graph until the remaining function is submodular . it can then be solved using max-flow . values of eliminated variables are recovered using back-substitution . we compare the energy minimization algorithm 's performance against alternative methods for solving submodular problems , with favourable results .",0
758,This paper proposes the use of the sparse random feature algorithm for solving regression and classification tasks in an infinite-dimensional Hilbert space. The algorithm is based on randomized coordinate descent and uses a kernel function to transform the input data into an infinite-dimensional space. The resulting sparse non-linear predictor has low memory and prediction time requirements. The paper shows that the algorithm can be seen as a coordinate descent method in the ℓ1-regularized objective function and provides an O(1/ϵ^2) convergence rate. A greedy boosting step is also introduced to improve the performance of the algorithm. The paper compares the proposed approach with other methods and shows its effectiveness in terms of accuracy and computational efficiency.,1
759,"in this paper , we propose a sparse random features algorithm , which learns a sparse non-linear predictor by minimizing an ℓ 1-regularized objective function over the hilbert space induced from a kernel function . by interpreting the sparse random features algorithm as randomized coordinate descent in an infinite-dimensional space , we show the proposed sparse random features algorithm converges to a solution within ϵ-precision of that using an exact kernel method , by drawing o -lrb- 1 / ϵ -rrb- random features , in contrast to the o -lrb- 1 / ϵ 2 -rrb- convergence achieved by current monte-carlo analyses of random features . in our experiments , the sparse random features algorithm obtains a sparse solution that requires less memory and prediction time , while maintaining comparable performance on regression and classification tasks . moreover , as an approximate solver for the infinite-dimensional ℓ 1-regularized problem , the sparse random features algorithm also enjoys better convergence guarantees than a boosting approach in the setting where the greedy boosting step can not be performed exactly .",0
760,"This paper presents a general stability training method for improving the robustness of deep neural networks in computer vision tasks. The method is designed to handle small input distortions, such as cropping, rescaling, and compression, which are common image processing techniques. The proposed approach is based on deep architectures and feature embeddings, and uses small perturbations in the visual input to improve the generalization of the network. The authors demonstrate the effectiveness of the approach on a range of tasks, including classification, near-duplicate detection, and similar-image ranking, using the Inception architecture. The results show that the stability training method can significantly improve the robustness of deep neural networks on noisy datasets.",1
761,"in this paper we address the issue of output instability of deep neural networks : small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network . such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks . we present a general stability training method to stabilize deep neural networks against small input distortions that result from various types of common image processing , such as compression , rescaling , and cropping . we validate our method by stabilizing the state-of-the-art inception architecture -lsb- 11 -rsb- against these types of distortions . in addition , we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection , similar-image ranking , and classification on noisy datasets .",0
762,"This paper proposes a zero-shot learning approach that uses visual abstraction to tackle the problem of learning difficult-to-describe concepts without training data. The proposed method utilizes abstract visualizations of visual concepts to transfer knowledge from other categories to the target categories. The approach is demonstrated on a human pose dataset and fine-grained visual categories, showing improved performance compared to other zero-shot learning methods. The results suggest that the proposed visual abstraction approach is effective in learning concepts that are not well-represented in the training data, and can be applied to a variety of modalities such as gaze concepts.",1
763,"one of the main challenges in learning fine-grained visual categories is gathering training images . recent work in zero-shot learning circumvents this challenge by describing categories via attributes or text . however , not all visual concepts , e.g. , two people dancing , are easily amenable to such descriptions . in this paper , we propose a new modality for zero-shot learning using visual abstraction to learn difficult-to-describe concepts . specifically , we explore concepts related to people and their interactions with others . our proposed modality allows one to provide training data by manipulating abstract visualizations , e.g. , one can illustrate interactions between two clipart people by manipulating each person 's pose , expression , gaze , and gender . the feasibility of our modality is shown on a human pose dataset and a new dataset containing complex interactions between two people , where we outperform several baselines . to better match across the two domains , we learn an explicit mapping between the abstract and real worlds .",0
764,"This paper proposes a method called Coverage-Optimized Retrieval, which aims to improve similarity-based retrieval for recommender systems. The method focuses on optimizing the coverage of the retrieval set to increase the likelihood of finding relevant items for the user. By optimizing coverage, the proposed method is able to improve the performance of similarity-based retrieval and enhance the accuracy of recommendations. Overall, the paper highlights the importance of coverage optimization in retrieval and its potential to enhance the performance of recommender systems.",1
765,"we present a generalization of similarity-based retrieval in recommender systems which ensures that for any case that is acceptable to the user , the retrieval set contains a case that is at least as good in an objective sense and so also likely to be acceptable . our approach recognizes that similarity to the target query is only one of several possible criteria according to which a given case might be considered at least as good as another .",0
766,"The paper proposes a preconditioned Forward-Backward approach for solving large-scale nonconvex spectral unmixing problems. The nonconvex problem is formulated as a non necessarily smooth function and is solved using an alternating minimization strategy. The approach is based on the majorize-minimize principle and uses variable metrics to improve convergence. The proposed method is applied to solve inverse problems in spectral unmixing for large-size signals. The optimization problem is solved using the Forward-Backward algorithm, which is preconditioned to improve the convergence rate. The approach is evaluated using a criterion based on Lipschitz differentiability and is shown to outperform existing methods for solving nonconvex spectral unmixing problems.",1
767,many inverse problems require to minimize a criterion being the sum of a non necessarily smooth function and a lipschitz differen-tiable function . such an optimization problem can be solved with the forward-backward algorithm which can be accelerated thanks to the use of variable metrics derived from the majorize-minimize principle . the convergence of this approach is guaranteed provided that the criterion satisfies some additional technical conditions . combining this method with an alternating minimization strategy will be shown to allow us to address a broad class of optimization problems involving large-size signals . an application example to a nonconvex spectral unmixing problem will be presented .,0
768,"This paper proposes a model-based sparse principal component analysis (PCA) method for sparse loading noisy PCA using an l0 penalty. The method uses a Bayesian information criterion associated model selection method and a generalized EM algorithm with an iterative hard thresholding estimation method. The paper compares this method to the sparse PCA method and shows its effectiveness on DNA microarray and simulated data. The iterative hard thresholding method and generalized EM algorithm are used for estimation, and the paper discusses their roles in the proposed method. The results show that the proposed method outperforms the sparse PCA method in terms of accuracy and sparsity.",1
769,in this paper we present a novel model based sparse principal component analysis method based on the l 0 penalty . we develop an estimation method based on the generalized em algorithm and iterative hard thresholding and an associated model selection method based on bayesian information criterion . the estimation method is compared to a previous sparse pca method using both simulated data and dna microarray data .,0
770,"This paper proposes a new learning method for deep convolutional architectures called ""Learning by Stretching"". The method involves stretching the weight matrix of a fixed network architecture in an iterative manner during backpropagation, which allows for improved accuracy on object recognition tasks. The proposed method makes use of tractable algorithms and deep architectures to achieve the stretching. The effectiveness of the method is demonstrated through experiments on AI tasks, showing significant improvements in accuracy over traditional deep convolutional networks. Overall, the proposed Learning by Stretching method offers a promising approach to improving the performance of deep neural networks.",1
771,"in recent years , deep architectures have gained a lot of prominence for learning complex ai tasks because of their capability to incorporate complex variations in data within the model . however , these models often need to be trained for a long time in order to obtain good results . in this paper , we propose a technique , called ` stretch-ing ' , that allows the same models to perform considerably better with very little training . we show that learning can be done tractably , even when the weight matrix is stretched to infinity , for some specific models . we also study tractable algorithms for implementing stretching in deep convolutional architectures in an iterative manner and derive bounds for its convergence . our experimental results suggest that the proposed stretched deep convolutional networks are capable of achieving good performance for many object recognition tasks . more importantly , for a fixed network architecture , one can achieve much better accuracy using stretching rather than learning the weights using backpropagation .",0
772,"In this paper, we propose a new method based on differential entropy to determine the optimal embedding parameters for a signal. Specifically, we explore the phase space representation of synthetic time series and investigate the effects of embedding dimension and time lag on the quality of embedding. We show that by applying our proposed method, we can effectively select the optimal embedding parameters and improve the accuracy of subsequent analysis tasks, such as time-series prediction. Additionally, we demonstrate that our method outperforms traditional approaches, such as the neural network-based time-delay embedding method and the adaptive filter-based method, in terms of accuracy and computational efficiency. Our work provides a valuable contribution to the field of signal processing and paves the way for future research on using differential entropy for analyzing time-series data.",1
773,"a novel method for determining the set of parameters for a phase space representation of a time series is proposed . based upon the differential entropy , both the optimal embedding dimension , and time lag , are simultaneously determined . the choice of these parameters is closely related to the length of the optimal tap input delay line of an adaptive filter or time-delay neural network . the method employs a single criterion -- the '' entropy ratio '' between the phase space representation of a signal and an ensemble of its surrogates -- and is first systematically tested on synthetic time series for which the optimal embedding parameters are known , after which it is verified on a number of benchmark real-world time series . the proposed entropy ratio method is shown to consistently outperform some well-established methods .",0
774,"This paper presents an investigation into the effect of initial phase on the two-tone separation problem using empirical mode decomposition (EMD), a nonlinear and nonstationary signal processing technique. The theoretical background of EMD and the tone separation problem are discussed, along with the transition region and the amplitude ratio of the two tones. An adaptive method is proposed for determining the optimal frequency ratio and initial phase, and the role of initial phase in the separation process is analyzed. The results indicate that the initial phase has a significant effect on the separation performance and that the adaptive method improves the accuracy of the separation. The study highlights the importance of considering initial phase in EMD-based signal processing applications.",1
775,"empirical mode decomposition -lrb- empirical mode decomposition -rrb- is an adaptive method for nonlinear and nonstationary signal processing . although the adaptive method is easy to implement and widely deployed , its theoretical background and limitations remain uncertain . this paper investigates the performance of empirical mode decomposition in two tone separation problem , especially for the transition region between perfect separation and failure , with emphasis on the effect of the initial phase . relationships between amplitude ratio , frequency ratio , initial phase and performance are derived .",0
776,"This study investigates the impact of different weighting schemes used in information retrieval on sentiment analysis classification accuracy. The paper compares the binary unigram weights and term frequency weights, both with and without document frequency smoothing, as well as the widely used tf.idf scheme and a sublinear function. A support vector machines classifier is trained on multiple data sets to measure classification accuracy. The results show that the sublinear function, which is used to reduce the impact of highly frequent terms, outperforms all other weighting schemes. Additionally, it is found that the choice of weighting scheme depends on the data set used, indicating the need for careful consideration when applying sentiment analysis in different contexts.",1
777,"most sentiment analysis approaches use as baseline a support vector machines classifier with binary unigram weights . in this paper , we explore whether more sophisticated feature weighting schemes from information retrieval can enhance classification accuracy . we show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy , especially when using a sublinear function for term frequency weights and document frequency smoothing . the techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge .",0
778,"In this paper, the authors propose a novel approach to binaural speech segregation in noisy and reverberant environments, where multiple sources are present. The proposed method utilizes a deep neural network (DNN) based binaural classification approach to segregate the desired speech signals from the undesired sources. The DNN is trained using untrained configurations of auditory scenes to improve its robustness. The binary classification method achieved significant improvement in speech segregation in both multisource and reverberant conditions. The study shows that the proposed approach outperforms conventional speech segregation algorithms, demonstrating the effectiveness of binaural DNN-based methods for reverberant speech segregation in challenging environments.",1
779,"while human listening is robust in complex auditory scenes , current speech segregation algorithms do not perform well in noisy and reverberant environments . this paper addresses the robustness in binaural speech segregation by employing binary classification based on deep neural networks -lrb- dnns -rrb- . we systematically examine dnn based binaural classification to untrained configurations . evaluations and comparisons show that dnn based binaural classification produces superior segregation performance in a variety of multisource and reverberant conditions .",0
780,"This paper presents a new approach called Bayesian Active Appearance Models (BAAMs) for statistical modeling of shape and texture in generic fitting scenarios. BAAMs extend the existing Active Appearance Models (AAMs) framework by introducing a probabilistic model and a Bayesian formulation. This allows for the calculation of texture parameters in a latent texture space, simultaneously with shape parameters, leading to a more accurate and robust model. The cost function in BAAMs is defined using a Gaussian noise model and a Gaussian prior, which captures the variability of the training data. The proposed simultaneous algorithms for shape and texture generation improve the performance of AAMs, especially in noisy and low-quality images. The results show that BAAMs outperform AAMs in terms of accuracy and robustness.",1
781,"in this paper we provide the first , to the best of our knowledge , bayesian formulation of one of the most successful and well-studied statistical models of shape and texture , i.e. active appearance models . to this end , we use a simple probabilistic model for texture generation assuming both gaussian noise and a gaussian prior over a latent texture space . we retrieve the shape parameters by formulating a novel cost function obtained by marginalizing out the latent texture space . this results in a fast implementation when compared to other simultaneous algorithms for fitting active appearance models , mainly due to the removal of the calculation of texture parameters . we demonstrate that , contrary to what is believed regarding the performance of active appearance models in generic fitting scenarios , optimization of the proposed cost function produces results that outperform discriminatively trained state-of-the-art methods in the problem of facial alignment '' in the wild '' .",0
782,"This paper presents an error mining technique for automatically detecting missing and erroneous information in parsing results. The technique involves using a syntactic lexicon to identify errors in the pre-parsing processing chain. The proposed method aims to improve the accuracy of parsing systems by automatically detecting and correcting errors. The effectiveness of the technique is demonstrated through experiments on various datasets. Results show that the error mining technique can significantly improve the accuracy of parsing systems by detecting and correcting errors that would otherwise be missed. Overall, this paper highlights the importance of error mining in parsing",1
783,"we introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems . we applied this error mining technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain . we were thus able to identify missing and erroneous information in these resources .",0
784,"This paper investigates the challenges of fitting regression models under differential privacy while defending against model inversion attacks. Specifically, we explore polynomial representations of the objective function and differential privacy mechanisms to preserve attribute privacy in sensitive and non-sensitive attributes. We analyze the theoretical efficacy and utility of the released model under the given privacy budget and functional mechanism. We also discuss the potential of model inversion attacks and their relationship with differential privacy mechanisms. Our results demonstrate the importance of balancing privacy preservation with model utility in differential privacy-preserving regression models.",1
785,"differential privacy preserving regression models guarantee protection against attempts to infer whether a subject was included in the training set used to derive a model . it is not designed to protect attribute privacy of a target individual when model inversion attacks are launched . in model inversion attacks , an adversary uses the released model to make predictions of sensitive attributes -lrb- used as input to the model -rrb- of a target individual when some background information about the target individual is available . previous research showed that existing differential privacy mechanisms can not effectively prevent model inversion attacks while retaining model efficacy . in this paper , we develop a novel approach which leverages the functional mechanism to perturb coefficients of the polynomial representation of the objective function but effectively balances the privacy budget for sensitive and non-sensitive attributes in learning the differential privacy preserving regression model . theoretical analysis and empirical evaluations demonstrate our approach can effectively prevent model inversion attacks and retain model utility .",0
786,"In this paper, we propose a novel approach for semantic segmentation of real-world image datasets using multiple graphs with block-diagonal constraints. Our method involves learning an affinity matrix using pairwise potential between dissimilar superpixels in semantic space. We then use this matrix to construct a multi-view affinity graph that is jointly optimized with a label-confidence matrix using block-diagonal constraints. Our divide-and-conquer strategy allows us to solve the optimization problem in closed form, leading to significant computational efficiency. Experimental results demonstrate the effectiveness of our approach in achieving state-of-the-art performance on several benchmark datasets. Our method provides a new direction for semantic segmentation by leveraging multiple graphs and block-diagonal constraints for more accurate and efficient learning of the affinity matrix.",1
787,"in this paper we propose a novel method for image semantic segmentation using multiple graphs . the multi-view affinity graph is constructed by leveraging the consistency between semantic space and multiple visual spaces . with block-diagonal constraints , we enforce the affinity matrix to be sparse such that the pairwise potential for dissimilar superpixels is close to zero . by a divide-and-conquer strategy , the optimization for learning affinity matrix is decomposed into several subproblems that can be solved in parallel . using the neighborhood relationship between superpixels and the consistency between affinity matrix and label-confidence matrix , we infer the semantic label for each superpixel of unlabeled images by minimizing an objective whose closed form solution can be easily obtained . experimental results on two real-world image datasets demonstrate the effectiveness of our method .",0
788,"In this paper, we present a novel image processing system for distinguishing second harmonic generation (SHG) images of mouse preterm labor via wavelet-based texture features. SHG microscopy is a powerful imaging technique for detecting premature cervical remodeling, a key indicator of preterm labor. However, distinguishing between normal pregnant cervix and preterm labor using SHG images is challenging due to their similar appearance. To address this, we propose a wavelet-based texture feature extraction method that captures subtle differences between the two types of cervix. Our method is evaluated on SHG images of both normal pregnant cervix and preterm labor, including those from artificial collagen gels and mifepristone-treated mice. The extracted texture features are used to train a classifier that achieves high detection rates on the test dataset. Our results demonstrate that wavelet-based texture features can effectively differentiate SHG images of normal pregnant cervix and preterm labor, which has potential for clinical applications in detecting preterm labor.",1
789,"this paper presents an image processing system for detecting mouse preterm labor using second harmonic generation microscopy images . two classes of shg images are considered : normal pregnant cervix and premature cervical remodeling induced by mifepristone . among the commonly used texture features in image processing , wavelet-based texture features together with previously utilized image features for shg microscopy of artificial collagen gels are identified to form an effective set of features for distinguishing the two classes of images . the results obtained indicate that correct detection rates above 98 % are achievable .",0
790,"This paper investigates the relationship between the partition function and random maximum a-posteriori (MAP) perturbations in models with ragged energy landscapes. Specifically, we explore the use of MAP solvers and graph-cuts to compute the partition function and analyze the effects of randomly perturbing the models. We show that the partition function can be expressed in terms of the max-statistics of random variables, and that random perturbations can significantly impact the estimation of the partition function. We propose a novel method for computing the partition function that takes into account the effects of random perturbations, and demonstrate its effectiveness on several benchmark datasets. Our results provide new insights into the behavior of partition function and the effects of random perturbations in models with ragged energy landscapes, with potential applications in areas such as image segmentation and computer vision.",1
791,"in this paper we relate the partition function to the max-statistics of random variables . in particular , we provide a novel framework for approximating and bounding the partition function using map inference on randomly perturbed models . as a result , we can use efficient map solvers such as graph-cuts to evaluate the corresponding partition function . we show that our method excels in the typical '' high signal-high coupling '' regime that results in ragged energy landscapes difficult for alternative approaches .",0
792,"This paper proposes an adaptive hedge algorithm for decision-theoretic online learning in a probabilistic setting. The hedge algorithm is a popular algorithm for online learning in which the learner makes a decision based on a weighted combination of different actions, with the weights updated at each round. However, the standard hedge algorithm assumes a fixed learning rate, which can lead to suboptimal performance in some cases. In this paper, we introduce an adaptive learning rate for the hedge algorithm that improves its performance in a wider range of scenarios. We provide a theoretical analysis of the algorithm and show that it achieves near-optimal regret bounds under mild assumptions. We also perform a simulation study to demonstrate the effectiveness of the proposed algorithm on several benchmark datasets. Our results show that the adaptive hedge algorithm can achieve significantly better performance than the standard hedge algorithm in many cases. Overall, our proposed algorithm provides a simple and effective solution to the problem of adaptive online learning, with potential applications in areas such as recommendation systems and online advertising.",1
793,"most methods for decision-theoretic online learning are based on the hedge algorithm , which takes a parameter called the learning rate . in most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance , leading to suboptimal performance on easy instances , for example when there exists an action that is significantly better than all others . we propose a new way of setting the learning rate , which adapts to the difficulty of the decision-theoretic online learning : in the worst case our procedure still guarantees optimal performance , but on easy instances it achieves much smaller regret . in particular , our adaptive method achieves constant regret in a probabilistic setting , when there exists an action that on average obtains strictly smaller loss than all other actions . we also provide a simulation study comparing our approach to existing methods .",0
794,"This paper proposes a multi-frame analysis method for the estimation of voice source and vocal tract characteristics in voiced speech. The method is based on source-filter separation and takes advantage of the harmonic structure of voiced speech. It involves the iterative approximation of the vocal tract transfer function and the voice source signal using oversimplified models. The proposed method improves upon existing approaches by incorporating multiple frames of speech signals, which enables more accurate estimation of the vocal tract characteristics. We evaluate the performance of the proposed method using both simulated and real speech data, and show that it outperforms existing methods in terms of estimation accuracy. Specifically, we demonstrate that the proposed method can accurately estimate both the voice source and the vocal tract transfer function, even in the presence of noise and other disturbances. Overall, our results suggest that the proposed multi-frame analysis method has the potential to improve the quality of speech processing applications such as speech recognition and synthesis.",1
795,"this paper presents a new approach for estimating voice source and vocal tract filter characteristics of voiced speech . when it is required to know the transfer function of a system in signal processing , the input and output of the system are experimentally observed and used to calculate the function . however , in the case of source-filter separation we deal with in this paper , only the output -lrb- speech -rrb- is observed and the characteristics of the system -lrb- vocal tract -rrb- and the input -lrb- voice source -rrb- must simultaneously be estimated . hence the estimate becomes extremely difficult , and it is usually solved approximately using oversimplified models . we demonstrate that these characteristics are separable under the assumption that they are independently controlled by different factors . the separation is realised using an iterative approximation along with the multi-frame analysis method , which we have proposed to find spectral envelopes of voiced speech with minimum interference of the harmonic structure .",0
796,"This paper presents a study on acoustic target classification using distributed sensor arrays. The non-stationarity of target signatures poses a significant challenge for accurate classification, especially in the presence of noise and other disturbances. To address this challenge, we propose a data fusion algorithm that combines the information from multiple sensors to improve classification accuracy. Our approach involves first extracting relevant features from the acoustic signals and then applying a machine learning algorithm to classify the targets. We evaluate the performance of our approach using simulated data as well as real-world data collected from distributed sensor arrays. Our results show that the proposed data fusion algorithm significantly improves the classification accuracy compared to using a single sensor. Specifically, we demonstrate that our approach achieves high accuracy even in the presence of noise and other disturbances, and outperforms existing methods for target classification using distributed sensor arrays. Our study highlights the importance of data fusion in improving the accuracy of acoustic target classification, and provides insights into the design of effective data fusion algorithms for distributed sensor arrays.",1
797,"target target classification using distributed sensor arrays remains a challenging problem due to the non-stationarity of target signatures , large geographical area coverage of sensor arrays , and the requirements of time-critical and reliable information delivery . in this paper , we develop an algorithm to derive effective and stable features from both the frequency and the time-frequency domains of the acoustic signals . a modified data fusion algorithm for distributed sensor arrays is also developed in order to integrate the target classification results from different sensors and provide fault-tolerance . by using data fusion , the accuracy of the target classification can be increased by as many as 50 % .",0
798,"This paper proposes a significance-based n-gram selection method for smaller, better language models. The approach involves modifying the Kneser-Ney smoothing method by introducing a weighted-difference pruning method and absolute discounting pruning method. The goal is to reduce model size while improving perplexity, and the proposed method achieves this by selecting only significant n-grams. The paper compares the proposed method to other pruning methods and smoothing methods, such as Katz back-off. The perplexity metric is used to evaluate the selection method and the results show that the significance-based n-gram selection method outperforms the other methods. Overall, the paper demonstrates that ""less is more"" when it comes to language models, and that selecting only significant n-grams can lead to smaller and better models.",1
799,"the recent availability of large corpora for training n-gram language models has shown the utility of models of higher order than just trigrams . in this paper , we investigate methods to control the increase in model size resulting from applying standard methods at higher orders . we introduce significance-based n-gram selection , which not only reduces model size , but also improves perplexity for several smoothing methods , including katz back-off and absolute discounting . we also show that , when combined with a new smoothing method and a novel variant of weighted-difference pruning , our selection method performs better in the trade-off between model size and perplexity than the best pruning method we found for modified kneser-ney smoothing .",0
800,"This paper introduces a new approach for game tree search algorithm called ""Minimum Proof Graphs"" and its application in ""Fastest-Cut-First Search Heuristics"". The proposed method provides approximations of sub-DAG values for arbitrary DAG inputs and reduces heuristic evaluation time. The paper presents the Minimum Proof Graph algorithm, which creates a minimum game tree using interior nodes of the DAG, with a linear time complexity. The proposed method improves the branching factor of the game tree and minimizes the number of nodes evaluated. Additionally, the paper introduces the Fastest-Cut-First Search Heuristics which employs the Minimum Proof Graphs to prioritize the order of game tree evaluation based on the minimax values of the nodes. The experimental results show that the proposed approach outperforms the alpha-beta pruning method and other state-of-the-art heuristics in terms of search time and node evaluation.",1
801,"alpha-beta is the most common game tree search algorithm , due to its high-performance and straightforward implementation . in practice one must find the best trade-off between heuristic evaluation time and bringing the subset of nodes explored closer to a minimum proof graph . in this paper we present a series of structural properties of minimum proof graphs that help us to prove that finding such graphs is np-hard for arbitrary dag inputs , but can be done in linear time for trees . we then introduce the class of fastest-cut-first search heuristics that aim to approximate minimum proof graphs by sorting moves based on approximations of sub-dag values and sizes . to explore how various aspects of the game tree -lrb- such as branching factor and distribution of move values -rrb- affect the performance of alpha-beta we introduce the class of '' prefix value game trees '' that allows us to label interior nodes with true minimax values on the fly without search . using these trees we show that by explicitly attempting to approximate a minimum game tree we are able to achieve performance gains over alpha-beta with common extensions .",0
802,"This paper presents a refined version of the wrapper approach for feature selection by introducing smoothed error estimates. The wrapper approach is used for selecting a subset of features by evaluating their classification error on a given learning algorithm. The proposed method uses Bayesian estimators to estimate the posterior probabilities of feature subsets, and a leave-one-out error estimate to calculate classification error. The paper also discusses the count bias issue that can occur in the leave-one-out estimate, and proposes a jackknife method to address it. The performance of the proposed method is evaluated through experiments, which demonstrate that the smoothed error estimates can provide a more accurate and stable feature subset selection. The results show that the proposed method outperforms the standard wrapper approach and other state-of-the-art feature selection methods.",1
803,"in the wrapper approach for feature selection , a popular criterion used is the leave-one-out estimate of the classification error . while being relatively unbiased , the leave-one-out error estimate is nonetheless known to exhibit a large variance , which can be detrimental especially for small samples . we propose reducing its variance -lrb- i.e. smoothing -rrb- at two levels . at the first level , we smooth the error count using estimates of posterior probabilities ; while at the second level , we smooth the posterior probability estimates themselves using bayesian estimation with conjugate priors . furthermore , we propose using the jackknife to reduce the bias inherent in bayesian estimators . we then show empirically that smoothing the error estimate gives improved performance in feature selection .",0
804,"This paper presents an algorithm called Line Net Global Vectorization for the continuous vectorization of a line net, which is a type of graphic entity. The algorithm aims to minimize the degradation of image quality that can occur during the vectorization process. The method is evaluated for vectorizing line drawings and its performance is analyzed theoretically. The algorithm uses a seed segment to track the line net and a postprocessing step to remove noise from the vectorized result. The paper compares the proposed algorithm with other vectorization algorithms in terms of entity tracking and vectorization performance. The experimental results demonstrate that the proposed algorithm is efficient and produces accurate vectorization results.",1
805,"in this paper , an efficient global algorithm for vectorizing line drawings is presented . global algorithm first extracts a seed segment of a graphic entity from a raster image to obtain its direction and width , then tracks the pixels under the guidance of the direction so that the tracking can track through junctions and is not affected by noise and degradation of image quality . thus , an entity will be vectorized in one step without postprocessing . the relations among lines are also used to realize the continuous vectorization of a line net . the speed and quality of vectorization are greatly improved with this global algorithm . the performance evaluation is carried out both by theoretical analysis and by experiments . comparisons with other vectorization algorithms are also made .",0
806,This paper presents a statistical approach for text-independent speaker recognition systems using pitch-dependent Gaussian mixture models (GMMs). The proposed method utilizes long-term prosodic features and short-term acoustic vectors to capture the speaker's identity. The ergodic hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are used in conjunction with the proposed pitch-dependent GMMs. The experimental results show that the proposed method outperforms other state-of-the-art methods in terms of accuracy and robustness. The paper demonstrates that the incorporation of pitch information in the GMMs can improve the performance of speaker recognition systems.,1
807,"gaussian mixture models -lrb- gmms -rrb- and ergodic hidden markov models have been successfully applied to model short-term acoustic vectors for speaker recognition systems . prosodic features are known to carry information concerning the speaker 's identity and prosodic features can be combined with the short-term acoustic vectors in order to increase the performance of the speaker recognition systems . in this paper , a statistical approach using pitch-dependent gmms for modeling speakers is presented . this new statistical approach is capable of simultaneously modeling the statistical distributions of the short-term acoustic vectors and long-term prosodic features .",0
808,"The paper presents a new machine learning algorithm called the Support Vector Decomposition Machine (SVDM), which is based on the Singular Value Decomposition (SVD) method. The algorithm is designed to solve classification problems and performs dimensionality reduction by creating lower-dimensional representations of the input features. The SVDM algorithm is compared to two-phase approaches and demonstrated to be more efficient in reducing the dimensionality of the feature space. The paper also discusses the application of SVDM to fMRI analysis and shows how it can improve classification accuracy. Overall, the proposed SVDM algorithm provides a novel approach for solving machine learning problems that require efficient dimensionality reduction and accurate classification.",1
809,"in machine learning problems with tens of thousands of features and only dozens or hundreds of independent training examples , dimensionality reduction is essential for good learning performance . in previous work , many researchers have treated the machine learning problems in two separate phases : first use an algorithm such as singular value decomposition to reduce the dimensionality of the data set , and then use a classification algorithm such as na &#239; ve bayes or support vector machines to learn a classifier . we demonstrate that it is possible to combine the two goals of dimensionality reduction and classification into a single learning objective , and present a novel and efficient algorithm which optimizes this objective directly . we present experimental results in fmri analysis which show that we can achieve better learning performance and lower-dimensional representations than two-phase approaches can .",0
810,"This paper proposes a 3D face recognition method based on the evolution of iso-geodesic distance curves. The method involves computing the iso-geodesic distance curves for non-neutral faces in a 3D face database and using the evolution angle functions to represent the curves. The evolution angle functions are then converted into a weight function, which is used to generate a one-dimensional function. This function is Euclidean invariant and used for matching faces. The proposed method was tested on a non-neutral face database and achieved high accuracy compared to other methods. The paper also presents a theoretical analysis of the method and discusses its potential applications in real-world scenarios.",1
811,"this paper presents a novel 3d face recognition method by means of the evolution of iso-geodesic distance curves . specifically , the proposed 3d face recognition method compares two neighboring iso-geodesic distance curves , and formalizes the evolution between them as a one-dimensional function , named evolution angle function , which is euclidean invariant . the novelty of this paper consists in formalizing 3d face by an evolution angle functions , and in computing the distance between two faces by that of two functions . experiments on face recognition grand challenge -lrb- frgc -rrb- ver2 .0 shows that our 3d face recognition method works very well on both neutral faces and non-neutral faces . by introducing a weight function , we also show a very promising result on non-neutral face database .",0
812,"This paper proposes a 2-dimensional processing method for speech signals, with a focus on pitch estimation. The method involves using a grating compression transform to map the one-dimensional speech signal onto a 2-dimensional plane, where harmonically-related signal components are represented as sine-wave grating patterns. A short-space 2-dimensional Fourier transform is then used to estimate the magnitude of the components. The paper presents two different pitch estimators based on this 2-dimensional processing: a sine-wave-based pitch estimator and a GCT-based pitch estimator. The performance of both estimators is evaluated using narrowband spectrograms in the time-frequency plane, and the results are compared with each other. The paper also discusses the impact of additive white noise on the pitch estimation and presents a method for two-speaker pitch estimation. The proposed 2-dimensional processing method provides a new perspective on speech signal analysis and has potential applications in speech recognition and other related fields.",1
813,"in this paper , we introduce a new approach to two-dimensional processing of the one-dimensional speech signal in the time-frequency plane . specifically , we obtain the short-space 2-d fourier transform magnitude of a narrowband spectrogram of the signal and show that this 2-d transformation maps harmonically-related signal components to a concentrated entity in the new 2-d plane . we refer to this series of operations as the '' grating compression transform '' , consistent with sine-wave grating patterns in the spectrogram reduced to smeared impulses . the grating compression transform '' forms the basis of a speech pitch estimator that uses the radial distance to the largest peak in the gct plane . using an average magnitude difference between pitch-contour estimates , the gct-based pitch estimator is shown to compare favorably to a sine-wave-based pitch estimator for all-voiced speech in additive white noise . an extension to a basis for two-speaker pitch estimation is also proposed .",0
814,This paper proposes a rate-splitting approach for robust multiuser multiple-input single-output (MISO) transmission. The goal is to solve classical robust design problems for power minimization subject to transmit power constraints and non-scaling uncertainty regions. The proposed strategy is based on rate-splitting and can handle bounded uncertainties in the channel state information. The feasibility problem is considered in the presence of multiuser interference and the signal-to-noise ratio (SNR) precoders are optimized. The rate-splitting based designs are compared to other approaches and shown to provide improved max-min degrees of freedom. The paper includes a detailed mathematical analysis of the proposed method and simulation results for multiuser MISO systems.,1
815,"for multiuser miso systems with bounded uncertainties in the channel state information , we consider two classical robust design problems : maximizing the minimum rate subject to a transmit power constraint , and power minimization under a rate constraint . contrary to conventional strategies , we propose a rate-splitting strategy where each message is divided into two parts , a common part and a private part . all common parts are packed into one super common message encoded using a shared codebook and decoded by all users , while private parts are independently encoded and retrieved by their corresponding users . we prove that rs-based designs achieve higher max-min degrees of freedom compared to conventional designs -lrb- max-min degrees of freedom -rrb- for uncertainty regions that scale with snr . for the special case of non-scaling uncertainty regions , max-min degrees of freedom contrasts with max-min degrees of freedom and achieves a non-saturating max-min rate . in the power minimization problem , max-min degrees of freedom is shown to combat the feasibility problem arising from multiuser interference in max-min degrees of freedom . a robust design of precoders for max-min degrees of freedom is proposed , and performance gains over max-min degrees of freedom are demonstrated through simulations .",0
816,"This paper presents a novel automated physiological-based diagnostic method for predicting post-traumatic stress disorder (PTSD) scores. A sparse combined regression-classification formulation is proposed for learning a physiological alternative to clinical PTSD scores. The method uses peripheral physiology measures, such as heart rate, to predict a binary diagnostic decision based on a physiological diagnostic score. The proposed cost function and learning formulation result in a sparse physiological diagnostic score with high validity and learning generalizability. The study compares the proposed sparse combined regression-classification approach with generic learning approaches and shows its effectiveness in predicting PTSD scores. The approach also utilizes virtual reality videos to enhance peripheral physiology measures, which are crucial in PTSD diagnosis and classification. The paper discusses the potential of the proposed method to replace the current clinician-coded interview approach and provide a more objective diagnostic method for mental pathologies.",1
817,"current diagnostic methods for mental pathologies , including post-traumatic stress disorder , involve a clinician-coded interview , which can be subjective . heart rate and skin conductance , as well as other peripheral physiology measures , have previously shown utility in predicting binary diagnostic decisions . the binary diagnostic decisions is easier , but misses important information on the severity of the patients condition . this work utilizes a novel experimental setup that exploits virtual reality videos and peripheral physiology for ptsd diagnosis . in pursuit of an automated physiology-based objective diagnostic method , we propose a learning formulation that integrates the description of the experimental data and expert knowledge on desirable properties of a physiological diagnostic score . from a list of desired criteria , we derive a new cost function that combines regression and classification while learning the salient features for predicting physiological score . the physiological score produced by sparse combined regression-classification is assessed with respect to three sets of criteria chosen to reflect design goals for an objective , physiological ptsd score : parsimony and context of selected features , diagnostic score validity , and learning generalizability . for these criteria , we demonstrate that sparse combined regression-classification performs better than more generic learning approaches .",0
818,"This paper presents a novel method for recovering video anomalies from spectrally compressed video frames using a 3-channel spectral video system. The proposed approach combines principal component pursuit with sparse matrix recovery to analyze anomalies in the 2-D spatial information and 3-D data cube of the video frames. The recovery of anomalies is achieved by analyzing the spectral signatures of the compressed video frames, which are captured using the Compressed Sensing Imaging (CASSI) system. The CASSI system is used to measure 2-D spectral information, which is then used to recover the 3-D data cube of the video frames. The recovered video anomalies have important applications in video surveillance, where stationary background and spectral information are important features of the analysis of anomalies. The paper provides experimental results demonstrating the effectiveness of the proposed approach in recovering video anomalies from spectrally compressed video frames.",1
819,"this paper addresses the problem of video anomaly recovery from a sequence of spectrally compressed video frames . analysis of anomalies occurring in both time and spectrum is important in video surveillance applications . we present a methodology for the recovery of anomalies such as moving objects and their spectral signatures from spectrally compressed video . the spectrally compressed video frames are obtained by using a coded aperture snapshot spectral imaging -lrb- cassi -rrb- system . the cassi system encodes a 3-d data cube containing both 2-d spatial information and spectral information in a single 2-d measurement . in the proposed methodology , we use the spectrally compressed video as columns of a large data matrix g g g. principal component pursuit is then used to decompose g g g into the stationary background and a sparse matrix capturing the anomalies in the foreground . the sparse matrix is then used jointly with g g g to recover the spectral information of the objects of interest . an example for the recovery of video anomalies in a 3-channel spectral video system -lrb- rgb -rrb- is presented .",0
820,"This paper proposes a novel approach for segmenting color histograms using active contours and shape gradients. The proposed method uses a theoretical framework based on the classical calculus of variation and shape derivative approach to minimize the distance between the region and boundary functionals. The approach is applied to color histograms, and the paper provides a theoretical analysis of the technique. The proposed approach uses image features and statistical features to construct derivative histograms, which are used to define the active contour energy criterion. The paper presents a numerical scheme for solving the evolution equation that governs the motion of the active contours. The proposed approach is applied to image segmentation and is demonstrated to be effective in segmenting video sequences. The paper also provides experimental results showing the effectiveness of the proposed method in minimizing the distance between the region and boundary functionals. Overall, the proposed method offers a new approach to image segmentation using active contours and shape gradients that has promising potential in various applications.",1
821,"we consider the problem of image segmentation using active contours through the minimization of an energy criterion involving both region and boundary functionals . these image segmentation are derived through a shape derivative approach instead of classical calculus of variation . the image segmentation can be elegantly derived without converting the region integrals into boundary integrals . from the derivative , we deduce the evolution equation of an active contour that makes it evolve towards a minimum of the criterion . we focus more particularly on statistical features globally attached to the region and especially to the probability density functions of image features such as the color histogram of a region . a theoretical framework is set for the minimization of the distance between two histograms for matching or tracking purposes . an application of this theoretical framework to the segmentation of color histograms in video sequences is then proposed . we briefly describe our numerical scheme and show some experimental results .",0
822,"This paper presents a novel small-footprint hybrid statistical/unit selection text-to-speech (TTS) synthesis system designed for agglutinative languages. The proposed system is based on a hybrid statistical unit selection TTS approach and is specifically designed for embedded devices with limited memory footprint. The paper provides a detailed analysis and evaluation of the proposed system, comparing it with a baseline HTS system. The evaluation includes intelligibility and quality scores as well as A/B preference tests and Blizzard Challenge tests. The proposed system outperforms the baseline HTS system in terms of speech quality and intelligibility, especially for agglutinative languages. The hybrid statistical unit selection TTS system is shown to be effective in reducing the memory footprint while maintaining high-quality speech synthesis. The paper provides insights into the design and implementation of the unit selection scheme for agglutinative languages, which require a different approach than other languages. The proposed system has promising potential for use in embedded devices that require TTS capabilities with limited memory resources. Overall, the paper provides a significant contribution to the field of TTS synthesis for agglutinative languages and offers a novel approach that can be applied to other languages and speech synthesis applications.",1
823,"despite its success , unit selection based text-to-speech synthesis has has some disadvantages such as sudden discontinuities in speech that distract the listeners . the hmm-based tts approach has been increasingly getting more attention from the tts research community . one of the advantage is the lack of spurious errors that are observed in the unit selection scheme . another advantage of the hmm-based tts approach is the small memory footprint requirement which makes hmm-based tts approach attractive for embedded devices . here , we propose a novel hybrid statistical unit selection tts system for agglutinative languages that aims at improving the quality of the baseline hts system while keeping the memory footprint small . the intelligi-bility and quality scores of the baseline hts system are comparable to the mos scores of english reported in the blizzard challenge tests . listeners preferred the baseline hts system over the baseline hts system in the a/b preference tests .",0
824,"This paper proposes a novel active learning approach for image segmentation that introduces geometric priors to improve the annotation process. The proposed approach is evaluated using electron microscopy and magnetic resonance image volumes, as well as natural 2D images. The paper describes how the active learning approach uses a segmentation classifier to select the most informative samples for annotation. By introducing geometric priors, the annotation process is made more efficient, as the classifier can use prior knowledge of the expected shape and structure of objects in the image to guide the annotation process. The paper presents results that demonstrate the effectiveness of the proposed approach compared to traditional active learning methods without geometric priors. The proposed approach shows improved accuracy and efficiency in annotation, particularly for 3D image volumes and planar patch segmentation. The paper provides insights into the use of geometric priors for improving the annotation process and demonstrates the benefits of incorporating geometric information in active learning for image segmentation. Overall, the paper provides a significant contribution to the field of image segmentation and active learning and offers a new approach that can be applied to a range of image segmentation tasks.",1
825,"we propose an active learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3d image volumes . to this end , we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2d planar patch , which makes it much easier to annotate than if they were randomly distributed in the volume . a simplified version of this active learning approach is effective in natural 2d images . we evaluated our active learning approach on electron microscopy and magnetic resonance image volumes , as well as on natural 2d images . comparing our active learning approach against several accepted baselines demonstrates a marked performance increase .",0
826,"This paper proposes a new approach to template protection in biometric recognition systems by using a digital modulation paradigm. The proposed method employs modulation constellations and turbo codes with soft-decoding to ensure high template security and verification rates. The approach is evaluated on the CASIA-IrisV4 database for iris biometrics, which exhibits significant intra-class variability. The results show that the proposed method outperforms other state-of-the-art techniques in terms of security and verification rates. The paper also discusses the biometric cryptosystem that uses the proposed template protection method to enhance the security of iris biometric systems. Overall, this study shows that the digital modulation paradigm is a promising approach for iris template protection in biometric recognition systems.",1
827,"template protection is an issue of paramount importance in the design of biometric recognition systems . in this paper we present a biometric cryptosystem applied to iris biometrics , where template security is guaranteed by means of a framework inspired by the digital modulation paradigm . specifically , the properties of modulation constellations and turbo codes with soft-decoding are exploited to design a biometric cryptosystem with high performance in terms of both verification rates and security , even while dealing with a iris biometrics characterized by a high intra-class variability such as the iris . the effectiveness of the proposed biometric cryptosystem is evaluated by performing tests on the interval subset of the casia-irisv4 database .",0
828,"This paper addresses the problem of boundary detection in natural images, where the goal is to accurately localize object boundaries. Existing single-scale approaches often suffer from poor localization due to clutter and low contrast, and rely on local boundary cues which have limited effectiveness. The proposed solution is a multi-scale approach that improves boundary detection by leveraging both large-scale and small-scale detection strategies. The effectiveness of the proposed method is evaluated using boundary and object datasets with human-marked groundtruth. Results show that the multi-scale approach outperforms single-scale approaches in terms of contrast and localization, particularly in cases of poor localization and small-scale detection. The study highlights the importance of considering relative contrast and local boundary cues in improving boundary detection performance.",1
829,"in this work we empirically study the multi-scale boundary detection problem in natural images . we utilize local boundary cues including contrast , localization and relative contrast , and train a classifier to integrate them across scales . our approach successfully combines strengths from both large-scale detection -lrb- robust but poor localization -rrb- and small-scale detection -lrb- detail-preserving but sensitive to clutter -rrb- . we carry out quantitative evaluations on a variety of boundary and object datasets with human-marked groundtruth . we show that multi-scale boundary detection problem offers large improvements , ranging from 20 % to 50 % , over single-scale approaches . this is the first time that multi-scale is demonstrated to improve boundary detection on large datasets of natural images .",0
830,"This paper presents a novel approach for non-stationary policy learning in 2-player zero sum games, which are commonly used to model adversarial relationships between agents in multiagent environments. The proposed method, called Elph, leverages on-line learning methods to construct agents that can adapt their policies over time. Elph employs an entropy pruning technique to balance exploration and exploitation, and improves the stability of the learning process. The effectiveness of Elph is evaluated on adversarial zero-sum matrix games, and compared against existing methods for non-stationary policy learning. Experimental results show that Elph achieves superior performance in terms of convergence and regret minimization. The study highlights the importance of on-line sequence learning algorithms in developing effective non-stationary policies for adversarial zero-sum games.",1
831,"a key challenge in multiagent environments is the construction of agents that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting . these domains require on-line learning methods without the benefit of repeated training examples , as well as the ability to adapt to the evolving behavior of other agents in the environment . the difficulty is further exacerbated when the agents are in an adversarial relationship , demanding that a robust -lrb- i.e. winning -rrb- non-stationary policy be rapidly learned and adapted . we propose an on-line sequence learning algorithm , elph , based on a straightforward entropy pruning technique that is able to rapidly learn and adapt to non-stationary policies . we demonstrate the performance of this on-line sequence learning algorithm in a non-stationary learning environment of adversarial zero-sum matrix games .",0
832,"This paper addresses the problem of optimal feature reduction in semi-continuous hidden Markov models, which are commonly used for time series analysis and pattern recognition tasks. The goal is to identify lower dimensional subspaces that can capture relevant information for recognition accuracy. The proposed solution involves a uniform statistical framework that incorporates linear selection methods to map features onto a reduced feature space. The optimal feature reduction is achieved by solving a maximum-likelihood estimation problem, which ensures that the selected subspace maximizes the likelihood of the observed data. The effectiveness of the proposed approach is evaluated using recognition accuracy as the metric, and compared against existing linear selection methods. Experimental results show that the proposed method outperforms existing methods in terms of recognition accuracy. The study highlights the importance of optimal feature reduction in improving the performance of semi-continuous hidden Markov models, and demonstrates the effectiveness of the proposed uniform statistical framework for solving the maximum-likelihood estimation problem.",1
833,"linear discriminant or karhunen-lo eve transforms are established techniques for mapping features into a lower dimensional subspace . this paper introduces a uniform statistical framework , where the computation of the optimal feature reduction is formalized as a maximum-likelihood estimation problem . the experimental evaluation of this suggested extension of linear selection methods shows a slight improvement of the recognition accuracy .",0
834,"This paper presents a novel approach for improving hearing aid performance by multi-microphone noise cancellation. The proposed method leverages the least mean squares algorithm to process acoustic speech and noise data captured by multiple microphones. The signal characteristics of speech signals are analyzed in wide-band signal sub-bands to identify the dominant noise sources and to apply binaural pre-processing of speech signals. The noise cancellation processing mechanism is evaluated in both simulated and real-room acoustics, using noise ratios as the metric. The effectiveness of the proposed approach is evaluated through a study involving hearing impaired volunteers. The results show that the proposed multi-microphone noise cancellation method can significantly improve the performance of linear hearing aids in various acoustic environments. The study highlights the importance of considering the signal characteristics of speech signals and the use of binaural pre-processing in designing effective hearing aid systems. The proposed method has the potential to enhance the quality of life for individuals with hearing impairments.",1
835,"a scheme for binaural pre-processing of speech signals for input to a standard linear hearing aid has been investigated . the system is based on that of toner & campbell -lsb- l -rsb- who applied the least mean squares algorithm in sub-bands to speech signals from various acoustic environments and signal to noise ratios . the processing scheme attempts to take advantage of the multiple inputs to perform noise cancellation . the use of sub-bands enables a diverse processing mechanism to be employed , where the wide-band signal is split into smaller frequency limited sub-bands , which can subsequently he processed according to their signal characteristics . the results of a large scale series of intelligibility tests are presented from experiments in which acoustic speech and noise data , generated using simulated and real-room acoustics was tested on hearing impaired volunteers .",0
836,"This paper presents an analysis methodology for determining the wordlength of a line buffer in line-based 2-D discrete wavelet transform (DWT) systems. The on-chip line buffer is a critical component in line-based 2-D DWT systems, and its wordlength affects the overflow of coefficients and round-off errors in the system. The analysis methodology proposed in this paper enables the determination of the optimal wordlength for the line buffer, which maximizes the dynamic range of the reconstructed image while minimizing the distortion caused by round-off errors. The analysis methodology is evaluated using peak signal-to-noise ratio (PSNR) as the metric, and the results show that the optimal wordlength can be determined with high accuracy using the proposed methodology. The study highlights the importance of considering the line buffer wordlength in line-based 2-D DWT systems, and demonstrates the effectiveness of the proposed analysis methodology. The proposed methodology can be applied to other image processing systems that use line buffers, and has the potential to improve the quality and accuracy of the reconstructed images.",1
837,"the on-chip line buffer dominates the total area and power of line-based 2-d dwt . therefore , the line buffer wordlength has to be carefully designed to maintain the quality level due to the dynamic range growing and the round-off errors . in this paper , a complete analysis methodology is proposed to derive the required wordlength of line buffer given the desired quality level of reconstructed image . the proposed analysis methodology can guarantee to avoid overflow of coefficients , and the difference between predicted and experimental quality level is averagely 0.06 db in terms of psnr .",0
838,"This paper presents a method for large-scale biophysical parameter estimation in single neurons using constrained linear regression. The method uses biophysically accurate multi-compartmental models to estimate the spatiotemporal pattern of synaptic input and the spatial distribution of channel densities. The input-output function of single cells is also estimated using voltage sensitive imaging techniques, and the noise level in the spatiotemporal voltage signal is taken into account. The proposed method uses constrained linear regression to estimate the reversal potentials and intercompartmental conductances in the model, and the accuracy of the estimation is evaluated using model datasets. The method is compared with hand tuning of the model parameters, and the results show that the proposed method is more accurate and efficient for large-scale parameter estimation. The study highlights the importance of considering the noise level and spatiotemporal pattern of synaptic input in biophysical parameter estimation, and demonstrates the effectiveness of the proposed method in improving the accuracy of the estimation. The proposed method has the potential to facilitate large-scale biophysical parameter estimation in single neurons, which can advance our understanding of the neural system and its function.",1
839,"our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models . the large number of parameters needing hand tuning in these biophysically accurate multi-compartmental models has , however , somewhat hampered their applicability and inter-pretability . here we propose a simple and well-founded method for automatic estimation of many of these key parameters : 1 -rrb- the spatial distribution of channel densities on the cell 's membrane ; 2 -rrb- the spatiotemporal pattern of synaptic input ; 3 -rrb- the channels ' reversal potentials ; 4 -rrb- the in-tercompartmental conductances ; and 5 -rrb- the noise level in each compartment . we assume experimental access to : a -rrb- the spatiotemporal voltage signal in the dendrite -lrb- or some contiguous subpart thereof , e.g. via voltage sensitive imaging techniques -rrb- , b -rrb- an approximate kinetic description of the channels and synapses present in each compartment , and c -rrb- the morphology of the part of the neuron under investigation . the key observation is that , given data a -rrb- - c -rrb- , all of the parameters 1 -rrb- -4 -rrb- may be simultaneously inferred by a version of constrained linear regression ; this constrained linear regression , in turn , is efficiently solved using standard algorithms , without any '' local minima '' problems despite the large number of parameters and complex dynamics . the noise level 5 -rrb- may also be estimated by standard techniques . we demonstrate the method 's accuracy on several model datasets , and describe techniques for quantifying the uncertainty in our estimates .",0
840,"This paper proposes a directionally adaptive image interpolation method using directionlets. The directionlets are a type of multiple-direction wavelet transform that provides directional features of an image. The proposed method is able to preserve edge information and sharpness of details in the interpolated images, resulting in both high numeric and visual quality. The low-resolution image is transformed using the directionlets, and then the missing high-resolution details are estimated using a constrained optimization problem. Experimental results demonstrate that the proposed method outperforms state-of-the-art interpolation methods in terms of both objective and subjective evaluations.",1
841,"we present a novel directionally adaptive image interpolation based on a multiple-direction wavelet transform , called directionlets . the directionally adaptive image interpolation uses directionlets to efficiently capture directional features and to extract edge information along different directions from the low-resolution image . then , the high-resolution image is generated using this information to preserve sharpness of details . our directionally adaptive image interpolation outperforms the state-of-the-art methods in terms of both numeric and visual quality of the interpolated image .",0
842,"This paper proposes new frame-level objective functions for deep neural network (DNN) training in automatic speech recognition (ASR), beyond the commonly used cross-entropy objective function. The goal is to improve the relative word error rate reduction of DNN-based ASR systems. The authors introduce a log posterior ratio objective function and compare it with the cross-entropy objective function on the Switchboard task. They also propose an output layer activation switch that changes the softmax activation to a linear activation. Experimental results show that the proposed objective functions and activation switch outperform the cross-entropy trained DNN system in terms of relative word error rate reduction. The study suggests that these alternatives to the cross-entropy objective function could improve DNN-based ASR systems.",1
843,"we propose two approaches for improving the objective function for the deep neural network frame-level training in large vocabulary continuous speech recognition . the large vocabulary continuous speech recognition used in large vocabulary continuous speech recognition are often constructed with an output layer with softmax activation and the cross-entropy objective function is always employed in the frame-leveling training of large vocabulary continuous speech recognition . the pairing of softmax activation and cross-entropy objective function contributes much in the success of large vocabulary continuous speech recognition . the first approach developed in this paper improves the cross-entropy objective function by boosting the importance of the frames for which the large vocabulary continuous speech recognition has low target predictions -lrb- low target posterior probabilities -rrb- and the second one considers jointly minimizing the cross-entropy and maximizing the log posterior ratio between the target senone -lrb- tied-triphone states -rrb- and the most competing one . experiments on switchboard task demonstrate that the two proposed methods can provide 3.1 % and 1.5 % relative word error rate reduction , respectively , against the already very strong conventional cross-entropy trained dnn system .",0
844,"This study explores probabilistic models to characterize human heart beat dynamics in autonomic blockade control. Dynamic respiratory sinus arrhythmia analysis and adaptive point process filtering paradigm are used to analyze the instantaneous RSA gain and the index of vagal control dynamics in human heart beat intervals. An inverse Gaussian model is used to analyze autonomic control, along with respiratory covariate measurements. The Kolmogorov-Smirnov test is employed to validate the model's performance using electrocardiogram data. The results indicate that the inverse Gaussian model outperforms other models in terms of probabilistic modeling of heart beat dynamics during autonomic blockade control.",1
845,"in this paper , we compare and validate different probabilistic models of human heart beat intervals for assessment of the electrocardiogram data recorded with varying conditions in posture and pharmacological autonomic blockade . the models are validated using the adaptive point process filtering paradigm and kolmogorov-smirnov test . the inverse gaussian model was found to achieve the overall best performance in the analysis of autonomic control . we further improve the inverse gaussian model by incorporating the respiratory covariate measurements and present dynamic respiratory sinus arrhythmia analysis . our results suggest the instantaneous rsa gain computed from our proposed inverse gaussian model as a potential index of vagal control dynamics .",0
846,"This paper proposes the use of Markov models for automated ECG interval analysis. Specifically, the authors explore the use of hidden Markov and hidden semi-Markov models to model the constituent waveform features of electrocardiogram (ECG) signals. An overcomplete representation of the signal is obtained using the undecimated wavelet transform. The proposed approach is able to model the state durations of the ECG waveform, which is critical for accurate interval analysis. The use of hidden semi-Markov models allows for more flexible state duration modeling compared to traditional hidden Markov models. Real ECG features are used to evaluate the performance of the proposed method, demonstrating its effectiveness in automated ECG interval analysis.",1
847,"we examine the use of hidden markov and hidden semi-markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features . an undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling . we show that the state durations implicit in a standard hidden markov model are ill-suited to those of real ecg features , and we investigate the use of hidden semi-markov models for improved state duration modelling .",0
848,"This paper investigates the role of the auditory efferent system in reversed speech comprehension. Specifically, the study explores the impact of the medial olivocochlear bundle functionality on degraded speech comprehension and restoration. The authors examine both high and low-level auditory mechanisms involved in speech degradation and restoration, focusing on the physiological properties of the auditory system, such as contralateral suppression of otoacoustic emissions. The study also explores the role of higher-level strategies, such as lexical benefit, and interindividual variability in pseudoword comprehension. The results suggest that the auditory efferent system plays an important role in reversed speech comprehension, particularly in degraded speech restoration, through low-level auditory mechanisms.",1
849,"in the present study we explore the implication of high and low level mechanisms in degraded speech comprehension in normal hearing subjects . in experiment 1 we compared the loss of intelligibility due to the increasing size of reversion windows in both words and pseudowords . results showed that words are generally reconstructed better than pseudowords , suggesting the existence of a lexical benefit in degraded speech restoration . moreover , there was greater variability between individuals when reconstructing pseudowords than words . in experiment 2 , we demonstrated that this interindividual variability correlated with the subjects ' medial olivocochlear bundle functionality , as measured by contralateral suppression of otoacoustic emissions . together these experiments highlight the importance of low-level auditory mechanisms in degraded speech restoration . moreover they put forward the existence of major interindividual variability in the capacity to reconstruct degraded speech , which correlates with the physiological properties of the auditory system -lrb- low-level property -rrb- . in addition , our results also suggest the existence of multiple higher-level strategies that can compensate on-line for the lack of information caused by speech degradation .",0
850,"This paper proposes an accurate positioning system based on street view recognition. The system uses a vision-based technique for dynamically recognizing shop or building signs, which is view-angle invariant. By combining this with GPS scale data, the system achieves robust and accurate position estimation. The street view recognition algorithm is capable of estimating the distance with m error and is used in path refinement to correct GPS map locations. The technique is tested with real user location data and is found to provide improved accuracy compared to using GPS alone. The paper demonstrates that combining visual recognition techniques with GPS can result in more accurate positioning systems.",1
851,"in this paper , an accurate and robust positioning system based on street view recognition is introduced . vision-based technique is employed for dynamically recognizing shop or building signs on the gps map . two mechanisms including view-angle invariant distance estimation and path refinement are proposed for robust and accurate position estimation . through the combination of visual recognition technique and gps scale data , the real user location can be accurately inferred . experimental results demonstrate that the proposed system is reliable and feasible . compared with 20m error of position estimation provided by the gps , our system only has 0.97 m error estimation .",0
852,"This paper proposes a quasi text-independent speaker-verification system based on pattern matching. The system uses phonetically matched segments and Gaussian mixture models to compute frame-level probabilities. These probabilities are then used for pattern matching against the speech signals. The proposed method shows promising results and has the potential to be applied in various fields that require speaker verification. Specifically, the use of phonetically matched segments enables the system to be quasi text-independent, which makes it more versatile in real-world applications.",1
853,we present a new approach to quasi text-independent speaker verification based on pattern matching . our method first seeks phonetically matched segments in two speech signals . for all aligned frame pairs of these segments we compute the probability that they were uttered by the same speaker . based on these frame-level probabilities we take the decision whether the two signals were spoken by the same speaker or not . our method to find phonetically matched segments does not depend on a speech recognizer . we show that our system performs better than a baseline speaker verification system based on gaussian mixture models when the signals are long enough . especially interesting is the fact that a combination of the devised system with the baseline system performs much better than either of the systems alone .,0
854,"This paper proposes a method for solving everyday physical reasoning problems using sketches and analogies. The goal is to understand common sense reasoning and solve qualitative mechanics problems, which are often found in tests such as the Bennett Mechanical Comprehension Test. The approach involves annotating sketches with conceptual quantities and modeling decisions to enable the comparison of problems through analogy. Qualitative reasoning research is used to analyze the problems and develop a comparative analysis. The physical world is a key feature in understanding common sense reasoning, and analogy is essential for solving these problems.",1
855,"understanding common sense reasoning about the physical world is one of the goals of qualitative reasoning research . this paper describes how we combine qualitative mechanics and analogy to solve everyday physical reasoning problems posed as sketches . the problems are drawn from the bennett mechanical comprehension test , which is used to evaluate technician candidates . we discuss sketch annotations , which define conceptual quantities in terms of visual measurements , how modeling decisions are made by analogy , and how analogy can be used to frame comparative analysis problems . experimental results support the plausibility of this approach .",0
856,"This paper presents a translation assistance system that provides high-quality translation suggestions by translating L1 fragments in an L2 context. The system uses statistical language modeling and contextual window to disambiguate word senses and improve translation quality. A classification-based approach is employed to distinguish between code switches and foreign language fragments. The system is designed for language learners and provides a useful tool for improving their language proficiency. The system was evaluated against word-sense disambiguation baselines and achieved promising results, indicating its effectiveness in providing accurate and relevant translation suggestions in cross-lingual contexts.",1
857,in this paper we present new research in translation assistance . we describe a system capable of translating native language fragments to foreign language fragments in an l2 context . practical applications of this research can be framed in the context of second language learning . the type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known . these code switches are subsequently translated to l2 given the l2 context . we study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense dis-ambiguation baselines . a classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contex-tual window spanning a small number of neighbouring words .,0
858,"This paper presents a corpus-based Chinese speech synthesis system that utilizes contextual dependent unit selection. The system employs a unit selection procedure to synthesize speech from a speaker's utterances, with the corpus design aimed at maximizing context similarity between synthesis units. Prosody feature modification is also used to improve the system's output quality. The paper also describes a method for predicting prosody parameters and evaluates the system's performance. Results show that the proposed system achieves high-quality synthesized speech and outperforms baselines.",1
859,"this paper describes the realization of a corpus-based chinese speech synthesis system , including the corpus design and unit selection procedure . the corpus-based chinese speech synthesis system selects the synthesis unit according to context similarity between target unit and candidate unit . neither prosody parameter prediction nor prosody feature modification is needed . the informal test shows that the synthesized speech is quite natural , and the speaking style of original speaker is preserved because units are all from the speaker 's utterances .",0
860,"In image classification tasks, accurately assessing the importance of local regions is crucial for achieving high accuracy. This paper proposes a novel Region Ranking SVM algorithm that incorporates a region evaluation function to rank the importance of local regions based on their contribution to the overall classification performance. Different pooling techniques, including average-pooling and max-pooling, are explored to aggregate the local information. Experimental results on the ILSVRC2014 dataset demonstrate that the proposed algorithm outperforms several state-of-the-art methods. Furthermore, the paper analyzes the impact of feature types and local region sizes on the classification performance. Finally, the relationship between local information and global decision making is studied, showing that the former is a critical component in achieving high accuracy.",1
861,"the success of an image classification algorithm largely depends on how it incorporates local information in the global decision . popular approaches such as average-pooling and max-pooling are suboptimal in many situations . in this paper we propose region ranking svm -lrb- rrsvm -rrb- , a novel method for pooling local information from multiple regions . rrsvm exploits the correlation of local regions in an image , and it jointly learns a region evaluation function and a scheme for integrating multiple regions . experiments on pascal voc 2007 , voc 2012 , and ilsvrc2014 datasets show that rrsvm outperforms the methods that use the same feature type and extract features from the same set of local regions . rrsvm achieves similar to or better than the state-of-the-art performance on all datasets .",0
862,"Ordinal regression is a popular approach for analyzing high-dimensional feature spaces with fixed, discrete rating scales. In this paper, we propose an ordinal regression approach based on manifold learning techniques that can capture the intrinsic geometry of high-order data. The proposed approach is particularly effective for datasets with nonlinear structures and natural tensor structures, such as ordinal regression data sets and images. By embedding the data into a low-dimensional manifold space, the proposed approach can implicitly capture the ordinal relationship between the data items, leading to improved accuracy in ordinal regression. We evaluate the proposed approach on several benchmark datasets and show that it outperforms several state-of-the-art ordinal regression methods. Our results demonstrate that manifold learning can be a valuable tool for addressing the challenges of ordinal regression in high-order data spaces. Overall, this paper presents a novel approach to ordinal regression that can achieve superior performance by exploiting the intrinsic structure of the data.",1
863,"ordinal regression is an important research topic in machine learning . it aims to automatically determine the implied rating of a data item on a fixed , discrete rating scale . in this paper , we present a novel ordinal regression approach via manifold learning , which is capable of uncovering the embedded nonlinear structure of the data set according to the observations in the high-dimensional feature space . by optimizing the order information of the observations and preserving the intrinsic geometry of the data set simultaneously , the proposed ordinal regression approach provides the faithful ordinal regression to the new coming data points . to offer more general solution to the data with natural tensor structure , we further introduce the multilinear extension of the proposed ordinal regression approach , which can support the ordinal regression of high order data like images . experiments on various data sets validate the effectiveness of the proposed ordinal regression approach as well as its extension .",0
864,"Causal structure learning from observed measurement data is a challenging problem in machine learning. Many existing algorithms rely on dynamic causal graphs to model the causal structure of time series data. However, there is often a timescale mismatch between the data and the underlying causal structure, which can lead to undersampling and other problems. In this paper, we propose a rate-agnostic causal structure learning algorithm that can handle timescale mismatches and other issues that are common in dynamic causal graphs. Our approach is based on a novel regularization term that penalizes models with high sensitivity to changes in the timescale of the data. We demonstrate the effectiveness of our method on several benchmark datasets and show that it outperforms several state-of-the-art causal structure learning algorithms. Our results suggest that rate-agnostic causal structure learning can be a valuable tool for addressing the challenges of causal structure learning in time series data. Overall, this paper presents a novel approach to causal structure learning that can handle timescale mismatches and other issues in dynamic causal graphs.",1
865,"causal structure learning from time series data is a major scientific challenge . extant algorithms assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales . in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the timescale mismatch is often unknown . this paper develops three causal structure learning algorithms , each of which discovers all dynamic causal graphs that explain the observed measurement data , perhaps given undersampling . that is , these causal structure learning algorithms all learn causal structure in a `` rate-agnostic '' manner : they do not assume any particular relation between the measurement and system timescales . we apply these causal structure learning algorithms to data from simulations to gain insight into the challenge of undersampling .",0
866,"Scalable discrete sampling is a fundamental problem in large-scale Bayesian inference, where inference over discrete random variables is often required. In this paper, we propose a novel approach to scalable discrete sampling based on the multi-armed bandit problem. Our method can handle synthetic and real-world large-scale problems by using a subsampling approach to approximate the population Monte Carlo methods used in traditional discrete random variable sampling algorithms. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms several state-of-the-art methods. Our method is particularly robust to various sources of approximation error, and we provide theoretical guarantees for its performance. Furthermore, we show that our method can be applied to graphical models and other large-scale Bayesian inference problems. Overall, this paper presents a novel approach to scalable discrete sampling that can handle large-scale inference problems and provide approximate solutions with strong theoretical guarantees.",1
867,"drawing a sample from a discrete distribution is one of the building components for monte carlo methods . like other sampling algorithms , discrete distribution also suffers from high computational burden in large-scale inference problems . we study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale bayesian inference and graphical models , and propose an efficient approximate solution with a subsampling approach . we make a novel connection between the discrete distribution and multi-armed bandits problems with a finite reward population and provide three algorithms with theoretical guarantees . empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems .",0
868,"Detecting overlapping acoustic events is a challenging problem in sound event analysis. In this paper, we propose a temporally-constrained probabilistic model for detecting overlapping acoustic events. Our approach is based on event class-wise hidden Markov models and a succession of spectral templates that capture the temporal evolution of sound events. We evaluate our method on real and synthesized monophonic datasets as well as polyphonic datasets of office sounds. We demonstrate that our approach outperforms several state-of-the-art methods based on both frame-based and event-based metrics. Additionally, we use probabilistic latent component analysis to visualize the learned templates and show that they correspond well to the underlying sound events. We also propose a novel sound event dictionary and an acoustic scene simulator that can be used to generate input time/frequency representations for testing and evaluating sound event detection algorithms. Overall, our approach provides a robust and effective solution for detecting overlapping acoustic events that can be applied to a wide range of sound event analysis applications.",1
869,"in this paper , a system for overlapping acoustic event detection is proposed , which models the temporal evolution of sound events . the system is based on probabilistic latent component analysis , supporting the use of a sound event dictionary where each exemplar consists of a succession of spectral templates . the temporal succession of the templates is controlled through event class-wise hidden markov models . as input time/frequency representation , the equivalent rectangular bandwidth spectrogram is used . experiments are carried out on polyphonic datasets of office sounds generated using an acoustic scene simulator , as well as real and synthesized monophonic datasets for comparative purposes . results show that the proposed system outperforms several state-of-the-art methods for overlapping acoustic event detection on the same task , using both frame-based and event-based metrics , and is robust to varying event density and noise levels .",0
870,"This paper proposes a bio-inspired feedforward motion processing model for action recognition, specifically focusing on the richness of center-surround interactions in the model. The model is inspired by the functional properties of MT cells in the brain, and is designed to recognize actions in complex videos using a motion pathway. The paper describes the cortical layers and cells involved in the model, as well as the foveated structure and motion contrasts used to represent motion. The proposed model achieves an average recognition rate of 92.1% on the Weizmann database, outperforming existing methods. The classification method is based on defining motion maps and using feature vectors derived from the model. The paper also discusses the neurophysiology of the velocity detectors in MT cells and how they relate to the proposed model.",1
871,"here we show that reproducing the functional properties of mt cells with various center -- surround interactions enriches motion representation and improves the action recognition performance . to do so , we propose a simplified bio -- inspired model of the motion pathway in primates : it is a feedforward model restricted to v1-mt cortical layers , cortical cells cover the visual space with a foveated structure and , more importantly , we reproduce some of the richness of center-surround interactions of mt cells . interestingly , as observed in neurophysiology , our mt cells not only behave like simple velocity detectors , but also respond to several kinds of motion contrasts . results show that this diversity of motion representation at the mt level is a major advantage for an action recognition task . defining motion maps as our feature vectors , we used a standard classification method on the weizmann database : we obtained an average recognition rate of 98.9 % , which is superior to the recent results by jhuang et al. -lrb- 2007 -rrb- . these promising results encourage us to further develop bio -- inspired models incorporating other brain mechanisms and cortical layers in order to deal with more complex videos .",0
872,"The minimax principle is a fundamental concept in game theory and game-playing programs. It is used to determine the best possible move for a player assuming that the opponent will also play optimally. Despite its widespread use, the underlying reasons for its effectiveness have not been fully understood. In this paper, we present an alternative explanation for why minimax works. We show that minimax works because it exploits a specific type of value dependence in game trees that is present in most games. We also demonstrate that the pathological behavior of minimax in certain situations is due to the presence of noise in the game tree or a poorly chosen heuristic function. Our analysis provides a deeper understanding of the minimax principle and sheds light on its limitations and strengths.",1
873,"in game-playing programs relying on the minimax principle , deeper searches generally produce better evaluations . theoretical analyses , however , suggest that in many cases minimaxing amplifies the noise introduced by the heuristic function used to evaluate the leaves of the game tree , leading to what is known as pathological behavior , where deeper searches produce worse evaluations . in most of the previous research , positions were evaluated as losses or wins . dependence between the values of positions close to each other was identified as the property of realistic game trees that eliminates the pathology and explains why minimax is successful in practice . in this paper we present an alternative explanation that does not rely on value dependence . we show that if real numbers are used for position values , position values tend to be further apart at lower levels of the game tree , which leads to a larger proportion of more extreme positions , where error is less probable . decreased probability of error in searches to greater depths is sufficient to eliminate the pathology and no additional properties of game trees are required .",0
874,"This paper proposes the use of graphical models for inference with missing data. The authors highlight the importance of formal representations in handling missingness and propose missingness graphs as a consistent estimator of missing data. They also discuss the role of causal mechanisms in modeling missing data and argue that graphical models provide an effective tool for capturing these mechanisms. Overall, the paper provides a theoretical framework for graphical models in handling missing data and shows their potential in various applications.",1
875,"we address the problem of recoverability i.e. deciding whether there exists a consistent estimator of a given relation q , when data are missing not at random . we employ a formal representation called ` missingness graphs ' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these causal mechanisms and the variables being measured . using this formal representation , we derive conditions that the graph should satisfy to ensure recoverability and devise algorithms to detect the presence of these conditions in the graph .",0
876,"This paper proposes a generalized method-of-moments algorithm for rank aggregation. The algorithm uses generalized moment conditions to find consistent and inconsistent rankings. The statistical efficiency of the algorithm is discussed, and a comparison is made with the classical minorize-maximization algorithm. The Plackett-Luce model is used for full rankings, and pairwise comparisons are used for the generalized method-of-moments algorithm. The paper provides a formal representation of the algorithm and its properties, and its effectiveness is demonstrated through simulations. Overall, the proposed algorithm provides a flexible and efficient method for rank aggregation.",1
877,"in this paper we propose a class of efficient generalized method-of-moments algorithms for computing parameters of the plackett-luce model , where the data consists of full rankings over alternatives . our generalized method-of-moments algorithms is based on breaking the full rankings into pairwise comparisons , and then computing parameters that satisfy a set of generalized moment conditions . we identify conditions for the output of generalized method-of-moments algorithms to be unique , and identify a general class of consistent and inconsistent breakings . we then show by theory and experiments that our generalized method-of-moments algorithms run significantly faster than the classical minorize-maximization algorithm , while achieving competitive statistical efficiency .",0
878,"This paper presents an entropy-based criterion for categorical clustering that is based on probabilistic clustering models. The proposed method uses entropy-type measures to evaluate the heterogeneity of clusters and dissimilarity coefficients to compare the partitions generated by different clustering algorithms. The effectiveness of the method is demonstrated on several real-world datasets of categorical data, where it is shown to outperform other state-of-the-art clustering methods. The results suggest that the entropy-based criterion can be a useful tool for clustering categorical data, especially when dealing with large and complex datasets. Overall, this paper provides valuable insights into the development of clustering methods for categorical data analysis.",1
879,entropy-type measures for the heterogeneity of clusters have been used for a long time . this paper studies the entropy-based criterion in clustering categorical data . it first shows that the entropy-based criterion can be derived in the formal framework of probabilistic clustering models and establishes the connection between the criterion and the approach based on dissimilarity co-efficients . an iterative monte-carlo procedure is then presented to search for the partitions minimizing the criterion . experiments are conducted to show the effectiveness of the proposed procedure .,0
880,"This paper proposes lookahead-based algorithms for anytime induction of decision trees. Decision trees are widely used in machine learning due to their interpretability and effectiveness in classification tasks. However, learning decision trees can be computationally expensive, and often require a significant amount of time to train. The proposed algorithms aim to address this issue by incorporating a depth-k lookahead strategy into the tree construction process. By doing so, the algorithms are able to make more informed decisions about which attributes to split on, resulting in a more accurate and compact tree. The paper provides a stochastic version of the algorithms to improve their effectiveness. Additionally, the paper proposes a time allocation strategy to control the learning time, which enables the anytime induction of decision trees. The proposed algorithms are compared to the ID3 algorithm, a commonly used decision tree learning algorithm, and are shown to achieve superior performance in terms of tree quality and learning time.",1
881,"the majority of the existing algorithms for learning decision trees are greedy -- a tree is induced top-down , making locally optimal decisions at each node . in most cases , however , the constructed tree is not globally optimal . furthermore , the greedy algorithms require a fixed amount of time and are not able to generate a better tree if additional time is available . to overcome this problem , we present two lookahead-based algorithms for anytime induction of decision trees , thus allowing tradeoff between tree quality and learning time . the first one is depth-k lookahead , where a larger time allocation permits larger k . the second algorithm uses a novel strategy for evaluating candidate splits ; a stochastic version of id3 is repeatedly invoked to estimate the size of the tree in which each split results , and the one that minimizes the expected size is preferred . experimental results indicate that for several hard concepts , our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available .",0
882,"This paper presents scalable algorithms for solving the tractable Schatten quasi-norm minimization problem. The problem involves finding the minimum of quasi-norms of factor matrices subject to rank constraints. The quasi-norms are the Frobenius and nuclear quasi-norms and their hybrid and bi-nuclear versions, also known as Schatten-p quasi-norms. The proposed algorithms are based on singular value decomposition and eigenvalue decomposition, and can efficiently solve large-scale problems involving representative matrix completion tasks using synthetic and real-world data. The paper also proves the global convergence of the proposed algorithms.",1
883,"the schatten-p quasi-norm -lrb- 0 < p < 1 -rrb- is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately . however , existing schatten-p quasi-norm minimization algorithms involve singular value decomposition or eigenvalue decomposition in each iteration , and thus may become very slow and impractical for large-scale problems . in this paper , we first define two tractable schatten quasi-norms , i.e. , the frobenius/nuclear hybrid and bi-nuclear quasi-norms , and then prove that tractable schatten quasi-norms are in essence the schatten-2 / 3 and 1/2 quasi-norms , respectively , which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices . we also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems . finally , we provide the global convergence and performance guarantees for our algorithms , which have better convergence properties than existing algorithms . experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-of-the-art methods , and are orders of magnitude faster .",0
884,"This paper proposes a novel approach for grapheme-to-phoneme conversion based on a latent analogy framework. The authors introduce a global definition of analogous events that incorporates external linguistic knowledge and local context information, allowing for the synthesis of proper names and out-of-vocabulary words. The proposed framework employs a data-driven mapping method that utilizes supervision and locally optimal sequence alignment to generate globally relevant pronunciations. The authors evaluate their approach on a phoneme transcription task and show that it outperforms existing methods. They also demonstrate the effectiveness of their approach on both synthetic and real-world data, highlighting the benefits of incorporating maximum likelihood position scoring. The proposed approach has applications in speech synthesis, language translation, and natural language processing.",1
885,"data-driven grapheme-to-phoneme conversion involves either -lrb- top-down -rrb- inductive learning or -lrb- bottom-up -rrb- pronunciation by analogy . as both approaches rely on local context information , they typically require some external linguistic knowledge , e.g. , individual grapheme/phoneme correspondences . to avoid such supervision , this paper proposes an alternative solution , dubbed pronunciation by latent analogy , which adopts a more global definition of analogous events . for each out-of-vocabulary word , a neighborhood of globally relevant pronunciations is constructed through an appropriate data-driven mapping of its graphemic form . phoneme transcription then proceeds via locally optimal sequence alignment and maximum likelihood position scoring . this method was successfully applied to the synthesis of proper names with a large diversity of origin .",0
886,"This paper proposes a new adaptation method for deep neural networks (DNN) in the context of large vocabulary continuous speech recognition (LVCSR) called Feature space Maximum A Posteriori Linear regression (fMAPLIN). The method uses a linear regression framework to adapt DNN acoustic models to new speakers, leveraging prior knowledge about speaker-dependent parameters to avoid over-fitting. The paper shows that the fMAPLIN method leads to significant relative word error rate reduction compared to other adaptation methods such as the linear input network. The adaptation process is evaluated on the Switchboard task, using speaker-independent CD-DNN-HMM systems. The paper concludes that the proposed method offers a straightforward and robust feature space adaptation method for DNNs in LVCSR, which can be extended to other related tasks.",1
887,"we propose a feature space maximum a posteriori linear regression framework to adapt parameters for context dependent deep neural network hidden markov models -lrb- cd-dnn-hmms -rrb- . due to the huge amount of parameters used in dnn acoustic models in large vocabulary continuous speech recognition , the problem of over-fitting can be severe in dnn adaptation , thus often impair the robustness of the adapted dnn adaptation . linear input network as a straightforward feature space adaptation method for dnn adaptation , similar to feature space maximum likelihood linear regression -lrb- fmllr -rrb- , can potentially suffer from the same robustness situation . the proposed straightforward feature space adaptation method is built based on map estimation of the lin parameters by incorporating prior knowledge into the adaptation process . experimental results on the switchboard task show that against the speaker independent cd-dnn-hmm systems , linear input network provides 4.28 % relative word error rate reduction and the proposed fmaplin method is able to provide further 1.15 % -lrb- totally 5.43 % -rrb- relative word error rate reduction on top of linear input network .",0
888,"This paper presents novel codec structures for noise feedback coding of speech. Noise feedback coding (NFC) is an efficient approach to reduce the bit-rate of speech codecs by exploiting the spectral characteristics of speech and noise. The proposed codec structures include vector-quantization-based NFC and scalar-quantization-based NFC, which utilize long-term and short-term noise spectral shaping and prediction techniques. The paper also discusses closed-loop VQ codebook design and VQ codebook search for these structures. The effectiveness of the proposed codec structures is demonstrated through experiments using the BroadVoice® 16 codec and comparing it with the packetcable 1.5 mandatory narrowband speech codec. The results show that the proposed NFC codec structures can achieve comparable speech quality with lower bit-rates.",1
889,"this paper presents several novel codec structures for noise feedback coding incorporating both long-term and short-term noise spectral shaping , as well as long-term and short-term prediction . in addition , the paper generalizes the conventional scalar-quantization-based nfc to vector-quantization-based nfc , and it lays the foundation for the associated efficient vq codebook search and closed-loop vq codebook design . broadvoice ® 16 , a packetcable 1.5 mandatory narrowband speech codec standardized by cablelabs ® for voice over cable in north america , is based on one of such novel nfc codec structures .",0
890,This paper proposes a method for discovering topics and roles in social networks using social network analysis and probabilistic models. The authors utilize the latent Dirichlet allocation (LDA) and author-recipient-topic (ART) models to infer the topic distributions and direction-sensitive messages in the network. They apply these models to the Enron email corpus and a researcher's email archive to extract the topic distributions and roles of the participants. The results demonstrate the usefulness of this approach for identifying the communication patterns and the role of individuals in social networks. The proposed method can be applied to various other datasets to analyze the social network structure and identify the topic distributions and roles of participants.,1
891,"previous work in social network analysis has modeled the existence of links from one entity to another , but not the language content or topics on those links . we present the author-recipient-topic model for social network analysis , which learns topic distributions based on the direction-sensitive messages sent between entities . the author-recipient-topic model builds on latent dirichlet allocation and the author-topic model , adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient -- steering the discovery of topics according to the relationships between people . we give results on both the enron email corpus and a re-searcher 's email archive , providing evidence not only that clearly relevant topics are discovered , but that the author-recipient-topic model better predicts people 's roles .",0
892,"This paper proposes a method for parallel variational optical flow computation, which can handle large-scale image processing problems and achieve real-time 2D image processing. The method is based on a variational approach and uses domain decomposition and cluster computing to achieve parallelism. The image plane is decomposed into rectangular subdomains, and a global solution is obtained by combining local solutions from each subdomain through a lower-dimensional interface. The method uses a dedicated interface and preconditioner to facilitate inter-process communication and ensure the convergence of the multi-grid iterations. The parallelization is achieved using PC-clusters and PC-hardware, and the experimental results demonstrate the effectiveness of the proposed method.",1
893,"we present an approach to parallel variational optical flow computation on standard hardware by domain decomposition . using an arbitrary partition of the image plane into rectangular subdomains , the global solution to the variational approach is obtained by iteratively combining local solutions which can be efficiently computed in parallel by separate multi-grid iterations for each subdomain . the approach is particularly suited for implementations on pc-clusters because inter-process communication between subdomains -lrb- i.e. processors -rrb- is minimized by restricting the exchange of data to a lower-dimensional interface . by applying a dedicated interface preconditioner , the necessary number of iterations between subdomains to achieve a fixed error is bounded independently of the number of subdomains . our approach provides a major step towards real-time 2d image processing using off-the-shelf pc-hardware and facilitates the efficient application of variational approaches to large-scale image processing problems .",0
894,"This paper proposes a method for emotion recognition using both cepstral and long-term speech features. The study focuses on two-class and five-class emotion detection, and uses logistic regression fusion and unweighted recall value as evaluation metrics. The proposed method uses a cepstral Gaussian mixture model fusion technique to combine the cepstral and long-term features. The results show that using both cepstral and long-term features can improve the accuracy of emotion recognition, and the proposed method achieves better performance compared to using only one type of feature.",1
895,"in this paper , we describe systems that were developed for the open performance sub-challenge of the interspeech 2009 emotion challenge . we participate in both two-class and five-class emotion detection . for the two-class and five-class emotion detection , the best performance is obtained by logistic regression fusion of three systems . these systems use short-and long-term speech features . fusion allowed to an absolute improvement of 2.6 % on the unweighted recall value compared with -lsb- 1 -rsb- . for the two-class and five-class emotion detection , we submitted two individual systems : cepstral gmm vs. long-term gmm-ubm . the best result comes from a cepstral gmm and produces an absolute improvement of 3.5 % compared to -lsb- 6 -rsb- .",0
896,"This paper investigates the application of dynamic sinusoidal models to statistical parametric speech synthesis. The study uses pitch synchronous spectral analysis and regularized cepstral coefficients to extract cepstral features from speech. A dynamic sinusoidal synthesis model is then used to generate speech from these features. The study evaluates the performance of the model using a mean opinion score test and a preference test. Results show that the dynamic sinusoidal model outperforms static amplitude statistical modeling based on a perceptual criterion. The study concludes that dynamic sinusoidal models have potential for improving the quality of statistical parametric speech synthesis, especially for producing natural-sounding speech.",1
897,"this paper applies a dynamic sinusoidal synthesis model to statistical parametric speech synthesis . for this , we utilise regularised cepstral coefficients to represent both the static amplitude and dynamic slope of selected sinusoids for statistical modelling . during synthesis , a dynamic sinusoidal synthesis model is used to reconstruct speech . a preference test is conducted to compare the selection of different sinusoids for cepstral representation . our results show that when integrated with statistical parametric speech synthesis , a relatively small number of sinusoids selected according to a perceptual criterion can produce quality comparable to using all harmonics . a mean opinion score test shows that our proposed dynamic sinusoidal synthesis model is preferred to one using mel-cepstra from pitch synchronous spectral analysis .",0
898,"This paper investigates the modeling and classification of breast tissue density in mammograms using probabilistic latent semantic analysis (PLSA) and local descriptors such as sift features. The study utilizes the MIAS and DDSM datasets for breast parenchymal tissue classification and uses compact tissue representation to describe the tissue densities. The proposed method performs classification in an unsupervised manner using a generative model and a statistical text literature approach. The study shows that combining local descriptors and PLSA can improve the classification performance, and the use of tissue distribution can help to generate a more accurate classification model. The proposed method achieves promising results for mammogram texture classification, and the evaluation is performed using a classifier and mean opinion score tests.",1
899,"we present a new approach to model and classify breast parenchymal tissue . given a mammogram , first , we will discover the distribution of the different tissue densities in an unsupervised manner , and second , we will use this tissue distribution to perform the classification . we achieve this using a classifier based on local descriptors and probabilis-tic latent semantic analysis , a generative model from the statistical text literature . we studied the influence of different descriptors like texture and sift features at the classification stage showing that textons outperform probabilis-tic latent semantic analysis in all cases . moreover we demonstrate that probabilis-tic latent semantic analysis automatically extracts meaningful latent aspects generating a compact tissue representation based on their densities , useful for discriminating on mam-mogram classification . we show the results of classification stage over the mias and ddsm datasets . we compare our method with approaches that classified these same probabilis-tic latent semantic analysis showing a better performance of our proposal .",0
900,"This paper proposes an unsupervised learning approach for the extraction of video highlights via robust recurrent auto-encoders. The temporal structure of highlight segments is learned using a robust recurrent auto-encoder, which is trained on web-crawled and downloaded edited videos. The shrinking exponential loss function is used to train the auto-encoder, and heuristic rules are applied to identify highlight segments. The proposed approach is evaluated on social media websites such as Instagram and YouTube, which are short-form video sharing platforms, and user-edited videos with noise. The results show that the proposed approach outperforms supervised techniques in the unsupervised setting, demonstrating the effectiveness of the proposed unsupervised learning approach for video highlight extraction.",1
901,"with the growing popularity of short-form video sharing platforms such as instagram and vine , there has been an increasing need for techniques that automatically extract highlights from video . whereas prior works have approached this problem with heuristic rules or supervised learning , we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as youtube . based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently , we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest . the robust recurrent auto-encoder is trained using a proposed shrinking exponential loss function that makes robust recurrent auto-encoder robust to noise in the web-crawled training data , and is configured with bidirectional long short term memory -lrb- lstm -rrb- -lsb- 5 -rsb- cells to better model the temporal structure of highlight segments . different from supervised techniques , our unsupervised learning approach can infer highlights using only a set of down-loaded edited videos , without also needing their pre-edited counterparts which are rarely available online . extensive experiments indicate the promise of our proposed unsupervised learning approach in this challenging unsupervised setting .",0
902,This paper presents a method for reconstructing high-resolution 3D visual information from low-resolution camera images. The proposed approach recovers high-resolution albedo and depth maps by exploiting Markov random fields and prior knowledge of the scene. The method uses a probabilistic framework that models the imaging process and accounts for the relative displacements of the image frames. The approach relies on statistical models and geometrical techniques to estimate the depth map and surface orientations. The reconstruction is performed using iterative algorithms that employ the expectation-maximization algorithm. Experimental results demonstrate the effectiveness of the proposed approach in generating high-quality high-resolution 3D visual information from low-resolution images.,1
903,"given a set of low resolution camera images , it is possible to reconstruct high resolution luminance and depth information , specially i f the relative displacements of the image frames are known . we have proposed iterative algorithms for recovering high resolution albedo and depth maps that require no a priori knowledge of the scene , and therefore do not depend on other methods , as regards boundary and initial conditions . the problem of surface reconstruction has been formulated as one of expectation maximization and has been tackled in a probabilistic framework as-ing markov random fields -lsb- 1 -rsb- -lsb- 3 -rsb- . as for the depth map , our iterative algorithms is directly recovering surface heights without refering to surface orientations , whale increasing the resolution by camera jittering -lsb- 2 -rsb- . conventional statistical models have been coupled with geometrical techniques to construct a general model of t.he world and the imaging process .",0
904,"This paper proposes a novel approach for event detection and domain adaptation using convolutional neural networks (CNNs). Event detection is a challenging task that involves identifying events in a given dataset. In the domain adaptation setting, the task is to detect events in a different domain with limited labeled data. This paper presents a CNN-based framework that uses rich feature sets to address this problem. The approach leverages external resources to improve the performance of the model. The proposed method outperforms traditional feature-based approaches, which require feature engineering, and exhibits better error propagation. The results demonstrate the effectiveness of the CNN-based approach for event detection and domain adaptation.",1
905,we study the event detection problem using convolutional neural networks that overcome the two fundamental limitations of the traditional feature-based approaches to this event detection problem : complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features . the experimental results show that the convolutional neural networks outper-form the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources .,0
906,"This paper proposes the use of Dual Kalman Filtering methods for nonlinear prediction, smoothing, and estimation in the context of simulations of noisy time series. The authors investigate the application of Kalman frameworks to the nonlinear case, with the goal of reducing noise in time series models. They explore the use of forward-backward filters for nonlinear prediction and smoothing, and apply these methods to noisy data in the context of signal processing. The paper also considers the linear case and proposes several approaches for reducing noise in signal processing models, including estimation of model parameters. The proposed methods are evaluated through simulations and applied to speech data. The results demonstrate the effectiveness of the Dual Kalman Filtering methods for noise reduction in both linear and nonlinear settings.",1
907,"prediction , estimation , and smoothing are fundamental to signal processing . to perform these interrelated tasks given noisy data , we form a time series model of the process that generates the data . taking noise in the system explicitly into account , maximum-likelihood and kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the underlying state of the system . we review several established methods in the linear case , and propose severa ! extensions utilizing dual kalman filters -lrb- dkf -rrb- and forward-backward filters that are applicable to neural networks . methods are compared on several simulations of noisy time series . we also include an example of nonlinear noise reduction in speech .",0
908,This paper proposes a Monte Carlo algorithm for characterizing the limit cycles of fixed-point IIR digital filters. The algorithm is based on the quantization function of the filters and is used to analyze the zero-input limit cycles of high-order filters. The paper also investigates the effect of fixed-point representation on limit cycles and presents numerical simulations to illustrate the proposed method. The results show that the proposed algorithm is effective in analyzing the limit cycles of fixed-point IIR digital filters and can be used to improve the design of these filters.,1
909,"the fixed point implementation of iir digital filters usually leads to the appearance of zero-input limit cycles , which degrade the performance of the system . in this paper , we develop an efficient monte carlo algorithm to detect and characterize limit cycles in fixed-point iir digital filters . the proposed monte carlo algorithm considers filters formulated in the state space and is valid for any fixed point representation and quantiza-tion function . numerical simulations on several high-order filters , where an exhaustive search is unfeasible , show the effectiveness of the proposed monte carlo algorithm .",0
910,"This paper presents a VLSI high level synthesis approach for fast exact least mean square (LMS) algorithms based on fast finite impulse response (FIR) filters. The proposed approach aims to reduce the computational load and memory requirements of LMS algorithms for processing and memory units such as acoustic echo cancellation. The paper discusses theoretical arithmetic reduction, different filter lengths, and algorithmic transformations to achieve a fast and exact LMS algorithm. The results show that the proposed approach outperforms the traditional FIR case LMS algorithms in terms of area, power, and delay. Overall, the paper provides a useful framework for synthesizing efficient LMS algorithms for practical applications.",1
911,"this paper relates experiences of algorithmic transformations in high level synthesis , in the area of acoustic echo cancellation . the processing and memory units are automatically designed for various equivalent lms algorithms , in the fir case , with important computational load . the results obtained with dierent lter lengths , give an accurate prototyping of new fast versions of the lms algorithms . it also show that a theoretical arithmetic reduction must be correlated to the associated increase of memory requirements .",0
912,"This paper proposes a novel method for multiple motion trajectory retrieval and classification using a view-invariant tensor null-space representation. The approach is based on the use of tensor-based null space affine invariants, which allow for the archiving and searching of motion events in a classification and retrieval system. The proposed method is capable of handling high-order data and can accommodate multidimensional affine transformations and consecutive motion events, including camera motions. The proposed approach achieves excellent performance using a linear classifier and demonstrates significant improvements over existing techniques.",1
913,"in this paper , we propose a novel general framework for ten-sor based null space affine invariants , namely , tensor null space invariants with a linear classifier for high order data classification and retrieval . we first derive tensor null space invariants , which is perfectly invariant to multidimensional affine transformations due to camera motions for multiple motion trajectories in consecutive motion events . we subsequently propose an efficient classification and retrieval system relying on tensor null space invariants for archiv-ing and searching motion events consisting of multiple motion trajectories . the simulation results demonstrate superior performance of the proposed classification and retrieval system .",0
914,"This paper presents a new method for functional magnetic resonance imaging (fMRI) activation detection using Group Markov Random Field (MRF). The proposed approach utilizes stringent one-to-one voxel correspondence and contextual image information to detect active brain regions. By incorporating both intra- and inter-subject neighbors, the method is capable of producing more accurate activation maps compared to traditional techniques. The Group MRF model is evaluated using synthetic and real fMRI data, and the results demonstrate its superior performance in detecting brain activation compared to other analysis techniques. This approach provides a promising solution for group fMRI analysis, and could potentially improve our understanding of brain function and dysfunction.",1
915,"noise confounds present serious complications to accurate data analysis in functional magnetic resonance imaging . simply relying on contextual image information often results in unsatisfactory segmentation of active brain regions . to remedy this , we propose a novel group markov random field -lrb- group mrf -rrb- that extends the neighborhood system to other subjects to incorporate group information in modeling each subject 's brain activation . our group markov random field -lrb- group mrf -rrb- has the distinct advantage of being able to regularize the states of both intra-and inter-subject neighbors without having to create a stringent one-to-one voxel correspondence as in standard fmri group analysis . also , our group markov random field -lrb- group mrf -rrb- can be efficiently implemented as a single mrf , hence enabling activation maps of a group of subjects to be simultaneously and collaboratively segmented . we validate on both synthetic and real fmri data and demonstrate superior performance over standard analysis techniques .",0
916,"This paper presents a linear universal demosaicking method for regular pattern color filter arrays (CFAs). The proposed method is applicable to various CFA patterns, including Bayer and CMY. It employs a filter weights scheme and a demosaicker to achieve high-quality demosaicking results while maintaining high speed. The method uses finite impulse response filters for noise reduction and is optimized for near Poissonian noise. The proposed technique is friendly to image signal processor design and can effectively reduce noise-induced artifacts. The method was evaluated on high sensitivity RGBW CFAs and compared with state-of-the-art demosaicking methods. The experimental results showed that the proposed method outperformed other methods in terms of speed and quality.",1
917,"we show that a recently developed universal demosaicker by the present authors greatly outperforms existing demosaickers when tested with a realistic optical pipeline . we present speed and quality optimizations of this demosaickers for the case of regular pattern color filter arrays . we implement and extensively test optimized versions for several common cfas including bayer , cmy and several rgbw patterns . these tests show that the proposed algorithms outperform other demosaickers by a substantial margin while being faster than most of them . high sensitivity rgbw cfas are shown to have better performance than bayer demosaicked with previous algorithms . the proposed universal demosaicker is a set of finite impulse response filters , which allows a single , efficient , image signal processor design to support different cfas by changing its filter weights . being linear , the demosaickers is free of noise induced arti-facts and outputs images with near poissonian noise which is noise reduction friendly .",0
918,"This paper presents a photo-real talking head system using deep bidirectional LSTM, a type of recurrent neural network architecture. The system predicts visual feature sequences from audio/visual stereo data with contextual label sequences using parallel temporal sequences. The deep bidirectional LSTM is used for regression modeling, audio/visual modeling, and for generating the photo-real talking head. The paper also introduces the use of square error for predicting visual sequences and contextual label sequences for parallel temporal sequences. The proposed system is evaluated using subjective A/B testing and objective measurements, demonstrating the effectiveness of the system in generating realistic talking heads.",1
919,"long short-term memory -lrb- lstm -rrb- is a specific recurrent neural network architecture that is designed to model temporal sequences and their long-range dependencies more accurately than conventional rnns . in this paper , we propose to use deep bidirec-tional lstm for audio/visual modeling in our photo-real talking head system . an audio/visual database of a subject 's talking is firstly recorded as our training data . the audio/visual stereo data are converted into two parallel temporal sequences , i.e. , con-textual label sequences obtained by forced aligning audio against text , and visual feature sequences by applying active-appearance-model on the lower face region among all the training image samples . the deep blstm is then trained to learn the regression model by minimizing the sum of square error of predicting visual sequence from label sequence . after testing different network topologies , we interestingly found the best network is two blstm layers sitting on top of one feed-forward layer on our datasets . compared with our previous hmm-based system , the newly proposed deep bidirec-tional lstm is better on both objective measurement and subjective a/b test .",0
920,This paper presents a novel approach for parameter estimation in signal processing for a mixture of circular and strictly non-circular signals. The proposed approach utilizes unitary ESPRIT algorithms and other ESPRIT-based parameter estimation algorithms to achieve estimation accuracy. The method also includes closed-form estimates to improve accuracy. The results show that the C-NC unitary ESPRIT algorithms outperform other NC methods in estimating the parameters for the received mixture of circular and strictly non-circular signals. This work provides an effective solution for parameter estimation in complex signal mixtures.,1
921,"recently , esprit-based parameter estimation algorithms have been developed to exploit the structure of signals from strictly second-order -lrb- so -rrb- non-circular -lrb- nc -rrb- sources . they achieve a higher estimation accuracy and can resolve up to twice as many sources . however , these nc methods assume that all the received signals are strictly non-circular . in this paper , we present the c-nc standard esprit and the c-nc unitary esprit algorithms designed for the more practical scenario of a received mixture of circular and strictly non-circular signals . assuming that the number of circular and strictly non-circular signals is known , the two proposed esprit-based parameter estimation algorithms yield closed-form estimates and c-nc unitary esprit algorithms also enables an entirely real-valued implementation . as a main result , it is shown that the estimation accuracy of the presented esprit-based parameter estimation algorithms improves with an increasing number of strictly non-circular signals among a fixed number of sources . thereby , not only the estimation accuracy of the strictly non-circular signals themselves is improved , but also the estimation accuracy of the circular signals . these results are validated by simulations .",0
922,"This paper proposes a method for predicting deep zero-shot convolutional neural networks (CNNs) using textual descriptions. The approach is evaluated on the Caltech-UCSD bird and flower datasets, where the task is to perform zero-shot learning of visual categories that have not been seen during training. The proposed method utilizes both visual category text features and textual descriptions to learn an embedding space that can be used to predict unseen categories. The embedding space is learned by mapping the semantic attributes of textual descriptions to the features extracted by CNNs. The resulting embedding space is then used to predict visual categories using a variety of evaluation metrics, including ROC and precision-recall curves. The experimental results show that the proposed method outperforms existing methods for zero-shot learning, demonstrating the effectiveness of utilizing textual descriptions to predict deep CNNs.",1
923,"one of the main challenges in zero-shot learning of visual categories is gathering semantic attributes to accompany images . recent work has shown that learning from textual descriptions , such as wikipedia articles , avoids the problem of having to explicitly define these attributes . we present a new model that can classify unseen categories from their textual description . specifically , we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neu-ral network . we take advantage of the architecture of cnns and learn features at different layers , rather than just learning an embedding space for both modalities , as is common with existing approaches . the proposed model also allows us to automatically generate a list of pseudo-attributes for each visual category consisting of words from wikipedia articles . we train our models end-to-end using the caltech-ucsd bird and flower datasets and evaluate both roc and precision-recall curves . our empirical results show that the proposed model significantly outper-forms previous methods .",0
924,This paper presents a study on acoustic equalization in reverberant environments using spherical harmonic analysis. The authors explore the equalizer robustness in non-isotropic sound fields and investigate the effects of sensor movement on equalization performance. A concise closed-form expression is derived for equalization using spherical harmonics. The study also considers the use of directional microphones in acoustic equalization and their impact on robustness. The results show that the proposed approach improves the equalization performance in reverberant rooms and provides a statistical acoustics framework for analyzing sound fields based on the wave equation.,1
925,"in this paper , we investigate the performance of acoustic equalization in reverberant environments . we first highlight an efficient general acoustic equalization of a sound field using spherical harmonics . we then use this acoustic equalization to develop a concise closed-form expression for robustness of equalization to sensor movement . this concise closed-form expression is used -lrb- i -rrb- to characterize equalization performance for a general class of non-isotropic sound fields and -lrb- ii -rrb- to quantify the improvements to equalizer robustness that can be obtained by using a directional microphone . the concise closed-form expression used here does not use any of the assumptions of statistical acoustics , but instead exploits the inherent properties of a sound field as described by the wave equation .",0
926,"This paper investigates the transmission characteristics of the outer ear canal and presents a finite element model of the canal walls to understand the behavior of sound waves in this system. The study aims to enhance hearing sensitivity by examining the properties of the external ear cavity subsystem and the elastic tympanic membrane. The model is validated using human dissections, and the eigenvalue problem is solved to understand the frequency response of the system. The results demonstrate the importance of the outer ear canal in sound transmission and provide insights into the mechanisms underlying hearing sensitivity.",1
927,the eigenvalue problem is solved on the finite element model of the external outer ear canal . the absorption of the canal walls and the interaction between external ear cavity subsystem and the elastic tympanic membrane is considered . the results of the finite element model are compared with experimental measurements on human dissections . the calculations support hypothesis of possible influence of external ear canal on the enhancement of hearing sensitivity in 2-4 khz frequency range .,0
928,"This paper proposes a novel approach for image style transformation using a coupled space learning framework. The proposed method utilizes a coupled Gaussian mixture model to embed hidden subspaces in both vector spaces and perform bidirectional portrait style transforms. By coupling the spaces and leveraging inter-space correlation information, the proposed method achieves superior performance compared to existing methods. The paper also applies the proposed approach to face super-resolution and demonstrates its effectiveness. The mixture-model architecture and inference coupling utilized in the proposed method contribute to its success in image style transformation.",1
929,"in this paper , we present a new learning framework for image style transforms . considering that the images in different style representations constitute different vector spaces , we propose a novel framework called coupled gaussian mixture model to learn the relations between different spaces and use coupled gaussian mixture model to infer the images from one style to another style . observing that for each style , only the components correlated to the space of the target style are useful for inference , we first develop the coupled gaussian mixture model to pursue the embedded hidden subspaces that best preserve the inter-space correlation information . then we develop the coupled bidirectional transform algorithm to estimate the transforms between the two embedded spaces , where the coupling between the forward transform and the backward transform is explicitly taken into account . to enhance the capability of modelling complex data , we further develop the coupled gaussian mixture model to generalize our framework to a mixture-model architecture . the effectiveness of the framework is demonstrated in the applications including face super-resolution and bidirectional portrait style transforms .",0
930,"This paper proposes a novel approach for analyzing the performance of binary classification models through Precision-Recall-Gain (PRG) curves. Unlike traditional Receiver Operating Characteristic (ROC) curves, PRG curves are able to handle incoherent scale assumptions and offer a more comprehensive accuracy-based performance assessment. PRG curves are generated by plotting the arithmetic mean of precision values against an interval of β values, which allows for the creation of a convex hull in the precision-recall space. The resulting PRG curve offers a more informative representation of classifier performance and can be used to assess model selection. Additionally, the Fβ score can be easily calculated from the PRG curve, making it a versatile precision-recall analysis tool. Overall, this paper offers a more accurate and intuitive approach to binary classification analysis through the use of PRG curves.",1
931,"precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier 's performance . perhaps inspired by the many advantages of receiver operating characteristic curves and the area under such curves for accuracy-based performance assessment , many researchers have taken to report precision-recall curves and associated areas as performance metric . we demonstrate in this paper that this practice is fraught with difficulties , mainly because of incoherent scale assumptions -- e.g. , the area under a pr curve takes the arithmetic mean of precision values whereas the f β score applies the harmonic mean . we show how to fix this by plotting pr curves in a different coordinate system , and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves . in particular , the area under precision-recall-gain curves conveys an expected f 1 score on a harmonic scale , and the convex hull of a precision-recall-gain curve allows us to calibrate the classifier 's scores so as to determine , for each operating point on the convex hull , the interval of β values for which the point optimises f β . we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected f 1 score than others , and so the use of precision-recall-gain curves will result in better model selection .",0
932,"This paper presents an efficient sparse group feature selection method based on nonconvex optimization. The proposed method addresses large-scale problems with high-dimensional data, where conventional convex methods fail to provide accurate results. The nonconvex paradigm allows for a more accurate sparse feature selection with a lower computational cost. The model is evaluated on both synthetic and real-world applications, showing promising results in terms of accuracy. The proposed method can also be used for parameter estimation, and it outperforms existing methods in terms of feature selection performance. The paper concludes that the nonconvex sparse group feature selection model can be a useful tool for high-dimensional data analysis and feature selection.",1
933,"sparse feature selection has been demonstrated to be effective in handling high-dimensional data . while promising , most of the existing works use convex methods , which may be suboptimal in terms of the accuracy of feature selection and parameter estimation . in this paper , we expand a nonconvex paradigm to sparse group feature selection , which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously . the main contributions of this article are twofold : -lrb- 1 -rrb- statistically , we introduce a nonconvex sparse group feature selection model which can reconstruct the oracle estimator . therefore , consistent feature selection and parameter estimation can be achieved ; -lrb- 2 -rrb- computationally , we propose an efficient algorithm that is applicable to large-scale problems . numerical results suggest that the proposed nonconvex sparse group feature selection model compares favorably against its competitors on synthetic data and real-world applications , thus achieving desired goal of delivering high performance .",0
934,"This paper presents an analysis of the Lombard effect, which is the phenomenon of increased speech volume and intensity in response to noise. The study focuses on the effect of different types and levels of noise on the Lombard effect and its implications for in-set speaker identification systems. The TIMIT corpus and the UT-Scope database were used to evaluate the performance of the identification system under noisy Lombard speech conditions. Test-token duration and speech characteristics were also considered. The results showed that Lombard speech significantly affects the performance of the identification system, especially in the presence of high levels of noise. The study highlights the need for further research in the area of Lombard speech and its impact on speaker identification systems.",1
935,"1 this paper presents an analysis of lombard speech produced under different types and levels of noise . the speech used for the analysis forms a part of the ut-scope database and consists of sentences from the well-known timit corpus , spoken in the presence of highway , large crowd and pink noise . differences are shown to exist in the speech characteristics under these varying noise types . the deterioration of the eer of an inset speaker identification system trained on neutral and tested with lombard speech is also illustrated . a clear demarcation between the effect of noise and lombard effect on noise is also given by testing with noisy lombard speech . the effect of test-token duration on system performance under the lombard condition is addressed . it is seen that test duration has no effect on the eer under lombard effect . the average eer for 3s test duration is 14 .",0
936,"This paper investigates the acoustic-prosodic features associated with awkward prosody in story retellings from adolescents with autism spectrum disorders (ASD). The study compares subjective perceptions of prosodic awkwardness with objective measures of intonation variability and atypical speech prosody. Diagnostic instrument algorithms are used to classify perceived awkwardness, and automated methods are applied to analyze acoustic-prosodic features. Results show that perceived awkwardness is associated with speaking rate, rhythm cues, volume, and intonation. The study suggests that the combination of objective acoustic-prosodic features and subjective perceptions could improve the diagnostic efficiency of awkward speech in individuals with ASD.",1
937,"atypical speech prosody is a primary characteristic of autism spectrum disorders , yet atypical speech prosody is often excluded from diagnostic instrument algorithms due to poor subjective reliability . robust , objective prosodic cues can enhance our understanding of those aspects which are atypical in autism . in this work , we connect objective signal-derived descriptors of prosody to subjective perceptions of prosodic awkwardness . subjectively , more awkward speech is less expressive -lrb- more monotone -rrb- and more often has perceived awkward rate/rhythm , volume , and intonation . we also find expressivity can be quantified through objective intonation variability features , and that speaking rate and rhythm cues are highly predictive of perceived awkwardness . acoustic-prosodic features are also able to significantly differentiate subjects with autism spectrum disorders from typically developing -lrb- td -rrb- subjects in a classification task , emphasizing the potential of automated methods for diagnostic efficiency and clarity .",0
938,"This paper discusses the problem of face detection, pose estimation, and landmark localization in real-world, cluttered images, which is a challenging task due to the large variability in viewpoint, lighting, and occlusion. The authors present a unified model that combines face detection and landmark estimation using tree-structured models and mixtures of trees. They also introduce a new annotated dataset called ""Wild'' which includes dense graph structures for facial landmark annotations. The proposed model is evaluated on several face benchmarks and compared to commercial systems such as Google Picasa. The experimental results show that the proposed method achieves state-of-the-art performance in face detection, pose estimation, and landmark localization, especially in cases of global elastic deformation and topological changes.",1
939,"we present a unified model for face detection , pose estimation , and landmark estimation in real-world , cluttered images . our unified model is based on a mixtures of trees with a shared pool of parts ; we unified model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint . we show that tree-structured models are surprisingly effective at capturing global elastic deformation , while being easy to optimize unlike dense graph structures . we present extensive results on standard face benchmarks , as well as a new '' in the wild '' annotated dataset , that suggests our unified model advances the state-of-the-art , sometimes considerably , for all three tasks . though our unified model is modestly trained with hundreds of faces , unified model compares favorably to commercial systems trained with billions of examples -lrb- such as google picasa and face.com -rrb- .",0
940,"This paper proposes a PAC-Bayes approach to the Set Covering Machine, which is a classification algorithm that aims to strike a balance between margin and sparsity in order to produce accurate models while avoiding overfitting. The PAC-Bayes perspective is used to derive a new learning algorithm for the Set Covering Machine that achieves this non-trivial margin-sparsity trade-off. The paper discusses the theoretical foundations of this approach and provides experimental results that demonstrate its effectiveness compared to other classifiers. Overall, this work contributes to the development of more powerful and efficient machine learning algorithms.",1
941,we design a new learning algorithm for the set covering machine from a pac-bayes perspective and propose a pac-bayes risk bound which is minimized for classifiers achieving a non trivial margin-sparsity trade-off .,0
942,"This paper presents a method for top-down induction of clustering trees, which combines decision tree induction with clustering. The approach generates a hierarchical clustering tree from a set of instances, where each node in the tree represents a cluster. The method uses a top-down approach to recursively partition the data, guided by a decision tree. This approach allows for the discovery of complex cluster structures and the generation of a logical decision tree representation. The method is implemented in an inductive logic programming system and can be applied to propositional and relational domains. Experimental results show that the method outperforms instance-based learning and first-order clustering on several datasets. Overall, this paper presents a promising approach for clustering data using decision tree induction.",1
943,"an approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering . to this aim , it employs the principles of instance based learning . the resulting methodology is implemented in the top-down induction of decision trees method -lrb- top down induction of clustering trees -rrb- system for first order clustering . the top-down induction of decision trees method employs the first order logical decision tree representation of the inductive logic programming system tilde . various experiments with top-down induction of decision trees method are presented , in both propositional and re-lational domains .",0
944,"This paper presents a Syntax-based Automatic Crossword puzzle Resolution sYstem (SACRY) for solving crossword puzzles automatically. The system utilizes syntactic structures to extract clues and answers from a given puzzle and uses aggregated information and minimum features to rank potential answers. The system also includes a clue reranking module to improve accuracy, and it is evaluated on a CP resolution tasks dataset. Results show that SACRY achieves high accuracy, and its performance is comparable to state-of-the-art automatic crossword puzzle resolution systems.",1
945,"in this paper , we present our crossword puzzle resolution system , which exploits syntactic structures for clue reranking and answer extraction . crossword puzzle resolution system uses a database -lrb- db -rrb- containing previously solved cps in order to generate the list of candidate answers . additionally , crossword puzzle resolution system uses innovative features , such as the answer position in the rank and aggregated information such as the min , max and average clue reranking scores . our crossword puzzle resolution system is based on webcrow , one of the most advanced systems for automatic crossword puzzle resolution . our extensive experiments over our two million clue dataset show that our crossword puzzle resolution system highly improves the quality of the answer list , enabling the achievement of unprecedented results on the complete cp resolution tasks , i.e. , accuracy of 99.17 % .",0
946,"This paper focuses on incomplete preference profiles in single-peaked electorates. Incomplete preferences can arise in real-world scenarios, and determining single-peakedness in such profiles is a challenging problem. The paper presents polynomial-time algorithms for determining single-peakedness in incomplete preference profiles, and discusses how these algorithms can be used for preference aggregation in voting systems. The authors show that their algorithms can efficiently determine single-peakedness even when a large portion of the preference profile is incomplete. The paper highlights the importance of considering incomplete preferences in voting systems, and provides practical solutions for handling such scenarios.",1
947,"incomplete preferences are likely to arise in real-world preference aggregation and voting systems . this paper deals with determining whether an incomplete preference profile is single-peaked . this is essential information since many intractable voting problems become tractable for single-peaked profiles . we prove that for incomplete profiles the problem of determining single-peakedness is np-complete . despite this computational hardness result , we find two polynomial-time algorithms for reasonably restricted settings .",0
948,"This paper proposes a method for constructing optimal sub-graphical models in a graph G by finding a subgraph H of G that satisfies certain separation properties. The authors show that finding such a subgraph H is an NP-hard problem, and propose a combinatorial optimization algorithm for solving it. They then demonstrate how to use H to construct a sub-graphical model of G, which has minimal separators and can be represented as a junction-tree. The authors also provide complexity bounds and show that the KL-divergence between the original graphical model and the sub-graphical model is bounded by a constant. Overall, this work provides a framework for constructing efficient sub-graphical models that can be used in a variety of applications.",1
949,"we investigate the problem of reducing the complexity of a graphical model -lrb- g , p g -rrb- by finding a subgraph h of g , chosen from a class of subgraphs h , such that h is optimal with respect to kl-divergence . we do this by first defining a decomposition tree representation for g , which is closely related to the junction-tree representation for g . we then give an algorithm which uses this decomposition tree representation to compute the optimal h ∈ h. gavril -lsb- 2 -rsb- and tarjan -lsb- 3 -rsb- have used graph separation properties to solve several combinatorial optimization problems when the size of the minimal separators in the g is bounded . we present an extension of this technique which applies to some important choices of h even when the size of the minimal separators of g are arbitrarily large . in particular , this applies to problems such as finding an optimal subgraphical model over a -lrb- k − 1 -rrb- - tree of a graphical model over a k-tree -lrb- for arbitrary k -rrb- and selecting an optimal subgraphical model with -lrb- a constant -rrb- d fewer edges with respect to kl-divergence can be solved in time polynomial in | v -lrb- g -rrb- | using this formulation .",0
950,"This paper presents a method for finding ideographic representations of Japanese names written in Latin script. The approach involves a three-tier filtering process that utilizes bilingual lexicons, English-to-Japanese back-transliteration, and language-specific mappings to identify Japanese names in Latin script. Corpus validation is performed to ensure the accuracy of the method. The process involves string copying and attested bigrams to identify the most likely Japanese representation of a given Latin-scripted name. The method achieves high average precisions on a dataset of Latin-scripted Japanese names, demonstrating its effectiveness for multilingual applications. The approach is also applicable to other Asian languages written in Latin script, such as Chinese. The paper highlights the importance of developing computational tools for ideographic representations in Latin-scripted languages, and the potential applications of such tools in various domains, including the web and Chinese name computation.",1
951,"multilingual applications frequently involve dealing with proper names , but names are often missing in bilingual lexicons . this multilingual applications is exacerbated for applications involving translation between latin-scripted languages and asian languages such as chinese , japanese and korean -lrb- cjk -rrb- where simple string copying is not a solution . we present a novel approach for generating the ideographic representations of a cjk name written in a latin script . the proposed approach involves first identifying the origin of the name , and then back-transliterating the name to all possible chinese characters using language-specific mappings . to reduce the massive number of possibilities for computation , we apply a three-tier filtering process by filtering first through a set of attested bigrams , then through a set of attested terms , and lastly through the www for a final validation . we illustrate the approach with english-to-japanese back-transliteration . against test sets of japanese given names and surnames , we have achieved average precisions of 73 % and 90 % , respectively .",0
952,"This paper investigates further on the problem of EMG-to-speech conversion. EMG signals are mapped to speech signals using a Gaussian mixture model. The surface EMG signals are obtained from facial muscles and are used in speech-to-text systems. The study also involves the comparison of whispered speech and spoken speech, and the electrode repositioning is also explored. A spectral distortion measure is used to evaluate the quality of the mapping, and the human articulatory apparatus is also considered. The study contributes to the development of a silent speech interface.",1
953,"our study deals with a silent speech interface based on mapping surface electromyographic signals to speech waveforms . electromyographic signals recorded from the facial muscles capture the activity of the human articulatory apparatus and therefore allow to retrace speech , even when no audible signal is produced . the mapping of emg signals to speech is done via a gaussian mixture model - based conversion technique . in this paper , we follow the lead of emg-based speech-to-text systems and apply two major recent technological advances to our system , namely , we consider emg-based speech-to-text systems , which are robust against electrode repositioning , and we show that mapping the emg signal to whispered speech creates a better speech signal than a mapping to normally spoken speech . we objectively evaluate the performance of our systems using a spectral distortion measure .",0
954,"This paper presents a large-scale sparse clustering algorithm for both synthetic and real-world datasets. Large-scale data clustering is a challenging task due to the high dimensionality of the data and the presence of noise. This work proposes a two-step optimization strategy that incorporates dimension reduction techniques and a sparse coding algorithm for large-scale clustering. The algorithm is based on nonlinear approximation, and it is able to efficiently cluster large-scale sparse data. The proposed method is evaluated on both synthetic and real-world datasets, and it outperforms other large-scale clustering methods in terms of accuracy and speed. The refinement of the clustering results is also discussed, showing how the proposed algorithm can effectively deal with noise in the data. Overall, this work demonstrates the potential of large-scale sparse clustering for a wide range of applications.",1
955,"large-scale clustering has found wide applications in many fields and received much attention in recent years . however , most existing large-scale clustering methods can only achieve mediocre performance , because large-scale clustering methods are sensitive to the unavoidable presence of noise in the large-scale data . to address this challenging problem , we thus propose a large-scale sparse clustering algorithm . in this paper , we choose a two-step optimization strategy for large-scale sparse clustering : 1 -rrb- k-means clustering over the large-scale data to obtain the initial clustering results ; 2 -rrb- clustering refinement over the initial results by developing a spare coding algorithm . to guarantee the scalabil-ity of the second step for large-scale data , we also utilize nonlinear approximation and dimension reduction techniques to speed up the spare coding algorithm . experimental results on both synthetic and real-world datasets demonstrate the promising performance of our large-scale sparse clustering algorithm .",0
956,The paper proposes an efficient reactive planner for synthesizing reactive plans that satisfy safety and liveness rules. The focus of the paper is on comparing nonlinear forward-search method and linear methods for this task. The authors argue that the proposed nonlinear forward-search method outperforms linear methods in terms of efficiency and optimality. The results are evaluated on a range of benchmarks and demonstrate the effectiveness of the proposed approach.,1
957,we present a nonlinear forward-search method suitable for planning the reactions of an agent operating in a highly unpredictable environment . we show that this nonlinear forward-search method is more eecient than existing linear methods . we then introduce the notion of safety and liveness rules . this makes possible a sharper exploitation of the information retrieved when exploring the future of the agent .,0
958,"This paper presents a fast algorithm for the design of two-dimensional (2D) linear-phase finite impulse response (FIR) filters using the weighted least squares method. The proposed method is based on the orthonormal basis of the subspace spanned by the filter coefficients and the desired frequency response. The algorithm supports both quadrantally-symmetric and centro-symmetric filter designs, and it achieves a significant reduction in computational complexity compared to existing methods. The paper also provides experimental results that demonstrate the efficiency and accuracy of the proposed algorithm.",1
959,"in this paper , we develop a new method for weighted least squares 2d linear-phase fir filter design . it poses the problem of filter design as the problem of projecting the desired frequency response onto the subspace spanned by an appropriate orthonormal basis . we show how to compute the orthonormal basis efficiently in the cases of quadrantally-symmetric filter design and centro-symmetric filter design . the design examples show that the proposed method is faster than a conventional weighted least squares filter design method . also , the amount of storage required to compute the filter coefficients is greatly reduced .",0
960,"This paper proposes the use of Generalized Latent Factor Models (GLFM) for social network analysis. GLFM is a multiplicative latent factor model that can capture the stochastic equivalence of directed links and homophily, which are two important features of real-world networks. The paper presents a minorization-maximization algorithm to estimate the model parameters, which achieves linear-time complexity and convergence guarantee. The effectiveness of the proposed method is demonstrated on several real-world networks. The results show that the GLFM can successfully model network structure and homophily. This paper provides a new tool for social network analysis that can capture the complex dependencies in real-world networks.",1
961,"homophily and stochastic equivalence are two primary features of interest in social networks . recently , the multiplicative latent factor model is proposed to model social networks with directed links . although multiplicative latent factor model can capture stochastic equivalence , it can not model well ho-mophily in network structure . however , many real-world networks exhibit homophily or both homophily and stochastic equivalence , and hence the network structure of these network structure can not be mod-eled well by multiplicative latent factor model . in this paper , we propose a novel model , called generalized latent factor model -lrb- glfm -rrb- , for social network analysis by enhancing homophily modeling in multiplicative latent factor model . we devise a minorization-maximization algorithm with linear-time complexity and convergence guarantee to learn the model parameters . extensive experiments on some real-world networks show that glfm can effectively model homophily to dramatically outperform state-of-the-art methods .",0
962,"In this paper, a spectrally efficient nonorthogonal amplify-and-forward (NAF) protocol is proposed for cooperative wireless networks. The protocol allows for higher spectral efficiency compared to traditional orthogonal amplify-and-forward and decode-and-forward protocols. The NAF protocol is designed to reduce the error rate by utilizing nonorthogonal multiple access techniques. The proposed protocol is evaluated using wireless systems and compared to traditional cooperative protocols, demonstrating superior spectral efficiency. Overall, this work presents a novel protocol for improving the performance of cooperative wireless networks.",1
963,"in wireless systems where half duplex transceivers are employed , most existing practical cooperative protocols achieve a spectral efficiency of 0.5 symbols per channel use -lrb- pcu -rrb- . recently a decode-and-forward protocol was developed to achieve a spectral efficiency of 2/3 symbols pcu . but there is no practical amplify-and-forward protocol that can achieve a spectral efficiency higher than 0.5 symbols pcu . in this paper , we develop a nonorthogonal af protocol which achieves a spectral efficiency of 2/3 symbols pcu and provides almost the same bit error rate as the traditional orthogonal af which has a spectral efficiency of 0.5 symbols pcu .",0
964,"This paper presents a new approach for large-scale machine learning applications using the proximal average, which provides better approximation and faster algorithms than previous methods. The proposed method applies the proximal gradient algorithm to nonsmooth losses/regularizers and uses the overlapping group lasso to handle the computational challenges. The paper also highlights the benefits of using convex analysis tools and the proxi-mal map to handle nonsmooth approximations. Furthermore, the paper discusses the trade-off between smoothing overhead and the quality of the approximation. Overall, the proposed method offers a significant improvement in computational efficiency and accuracy for large-scale machine learning problems.",1
965,"it is a common practice to approximate '' complicated '' functions with more friendly ones . in large-scale machine learning applications , nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions . we reexamine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proxi-mal map . the new approximation is justified using a recent convex analysis tool -- proximal average , and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing , without incurring any extra overhead . numerical experiments conducted on two important applications , overlapping group lasso and graph-guided fused lasso , corroborate the theoretical claims .",0
966,"This paper presents a methodological contribution to multifractal analysis and α-stable processes. The study explores wavelet coefficients, partition functions, and the Legendre multifractal spectrum to estimate multifractal time for both fractional Brownian motion and self-similar-stable process. The proposed estimation procedure for partition functions and wavelet coefficients allows for a better understanding of the multifractal spectrum. The paper also discusses the use of α-stable processes in multifractal analysis and its potential for applications in various fields such as physics, finance, and geology. Overall, this work contributes to the development of new tools and techniques for the analysis of multifractal time series.",1
967,"this work is a contribution to the analysis of the procedure , based on wavelet coeecient partition functions , commonly used to estimate the legendre multifractal spectrum . the procedure is applied to two examples , a fractional brownian motion in multifractal time and a self-similar-stable process , whose sample paths exhibit irregularities that by eye appear very close . we observe that , for the second example , this analysis results in a qualitatively inaccurate estimation of its multifractal spectrum , and a related masking of the-stable nature of the process . we explain the origin of this error through a detailed analysis of the partition functions of the self-similar-stable process . such a study is made possible by the speciic properties of the wavelet co-eecients of such processes . we indicate how the estimation procedure might be modiied to avoid such errors .",0
968,"This paper proposes a dynamic programming approach for fast and robust object pose recognition from range images. Object recognition and pose estimation are essential tasks in computer vision, robotics, and automated manufacturing environments. The proposed approach is based on joint object recognition and pose estimation and uses range images obtained from commodity depth sensors. The approach is designed to handle computationally expensive training phases and clear outliers in real sequences. The method integrates local belief propagation and a random sampling-based approach to efficiently estimate the object pose. Additionally, color information is used to improve recognition accuracy. The proposed approach is evaluated on several datasets, and the results show that it outperforms other state-of-the-art methods in terms of speed and robustness.",1
969,"joint object recognition and pose estimation solely from range images is an important task e.g. in robotics applications and in automated manufacturing environments . the lack of color information and limitations of current commodity depth sensors make this task a challenging computer vision problem , and a standard random sampling based approach is prohibitively time-consuming . we propose to address this difficult problem by generating promising inlier sets for pose estimation by early rejection of clear outliers with the help of local belief propagation -lrb- or dynamic programming -rrb- . by exploiting data-parallelism our method is fast , and we also do not rely on a computationally expensive training phase . we demonstrate state-of-the art performance on a standard dataset and illustrate our approach on challenging real sequences .",0
970,"This paper presents an approach for real-time tracking using adaptive linear predictors. The proposed method aims to improve the efficiency of template tracking by dynamically modifying pre-learned linear predictors. The algorithm avoids computationally expensive full matrix inversion by iteratively updating the linear predictors in an online manner. The effectiveness of the approach is demonstrated through experiments that show its ability to adapt to changes in template size and shape. Additionally, the proposed adaptive linear predictors are compared to traditional linear predictors and learning approaches, and are shown to outperform them in terms of tracking accuracy and efficiency. Overall, the approach offers a promising solution for real-time tracking in various computer vision applications.",1
971,"enlarging or reducing the template size by adding new parts , or removing parts of the template , according to their suitability for tracking , requires the ability to deal with the variation of the template size . for instance , real-time template tracking using linear predictors , although fast and reliable , requires using templates of fixed size and does not allow on-line modification of the predictor . to solve this problem we propose the adaptive linear predictors which enable fast online modifications of pre-learned linear predictors . instead of applying a full matrix inversion for every modification of the template shape as standard approaches to learning linear predictors do , we just perform a fast update of this inverse . this allows us to learn the adaptive linear predictors in a much shorter time than standard learning approaches while performing equally well . we performed exhaustive evaluation of our adaptive linear predictors and compared adaptive linear predictors to standard linear predictors and other state of the art approaches .",0
972,"This paper presents a novel method for inferring white matter geometry from diffusion tensor magnetic resonance imaging (DT-MRI) and its application to connectivity mapping. The method uses a level set formulation scheme and a numerical scheme to compute geodesics of the diffusion tensor MRI (DT-MRI) on a Riemannian manifold. The proposed method is based on an exponential map and can be used for non-invasive characterization of anisotropic diffusion of water molecules in biological tissues. The method is evaluated on synthetic and real DT-MRI datasets and is shown to produce consistent neural fibers reconstruction. The proposed method provides a direct mapping from the complex diffusion profiles of DT-MRI to a distance function on a manifold, which can be used for global modelization of white matter geometry. The paper also discusses the use of radial brownian motion and Laplace-Beltrami operator for diffusion paths of water molecules and clear outliers. This work demonstrates the potential of DT-MRI for connectivity mapping and provides a powerful tool for investigating the structure of the human brain.",1
973,"we introduce a novel approach to the cerebral white matter connectivity mapping from diffusion tensor mri . dt-mri is the unique non-invasive technique capable of probing and quantifying the anisotropic diffusion of water molecules in biological tissues . we address the problem of consistent neural fibers reconstruction in areas of complex diffusion profiles with potentially multiple fibers orientations . our method relies on a global modelization of the acquired mri volume as a riemannian manifold m and proceeds in 4 majors steps : first , we establish the link between brownian motion and diffusion mri by using the laplace-beltrami operator on m . we then expose how the sole knowledge of the diffusion properties of water molecules on m is sufficient to infer its geometry . there exists a direct mapping between the diffusion tensor and the metric of m. next , having access to that metric , we propose a novel level set formulation scheme to approximate the distance function related to a radial brownian motion on m. finally , a rigorous numerical scheme using the exponential map is derived to estimate the geodesics of m , seen as the diffusion paths of water molecules . numerical experimentations conducted on synthetic and real diffusion mri datasets illustrate the potentialities of this level set formulation scheme .",0
974,"This paper presents a Bayesian inference approach to mixed-type anomaly detection using a generalized linear model with a student-t distribution to model high-dimensional latent variables. The proposed approach utilizes an error buffering component to handle mixed-type data and attributes. Laplace approximation is used for computational optimization. The method is evaluated on mixed-type datasets and compared to existing approaches, demonstrating its effectiveness in detecting anomalies. The proposed method, named ""BuffDetect,"" can be applied in various machine learning fields where mixed-type data is prevalent.",1
975,"anomaly detection for mixed-type data is an important problem that has not been well addressed in the machine learning field . there are two challenging issues for mixed-type datasets , namely modeling mutual correlations between mixed-type attributes and capturing large variations due to anomalies . this paper presents buffdetect , a robust error buffering approach for anomaly detection in mixed-type datasets . a new variant of the generalized linear model is proposed to generalized linear model the dependency between mixed-type attributes . the generalized linear model incorporates an error buffering component based on student-t distribution to absorb the variations caused by anomalies . however , because of the non-gaussian design , the problem becomes analytically intractable . we propose a novel bayesian inference approach , which integrates laplace approximation and several computational optimizations , and is able to efficiently approximate the posterior of high dimensional latent variables by iteratively updating the latent variables in groups . extensive experimental evaluations based on 13 benchmark datasets demonstrate the effectiveness and efficiency of buffdetect .",0
976,"This paper presents a novel approach for modeling and understanding phonological processes in Italian speech using the CLIPS corpus. Specifically, we focus on learning probabilistic phonological pronunciation rules and replacement rules from the phonetic surface form of spontaneous sentences spoken with regional Italian accents. To achieve this, we introduce an Italian speech alignment technique and a machine-learning algorithm that utilizes conditional probabilities to generate a probabilistic rule set. Our technical analysis reveals that our approach successfully captures the phonological form of spontaneous sentences and phonological concepts, leading to a better understanding of Italian phonology. Additionally, we provide a web-interface rule for easy access to our model and results. Our research provides a significant contribution to the field of phonology and machine learning, demonstrating the efficacy of our approach for learning probabilistic phonological rules.",1
977,a blending of phonological concepts and technical analysis is proposed to yield a better modeling and understanding of phonological processes . based on the manual segmentation and labeling of the italian clips corpus we automatically derive a probabilistic set of phonological pronunciation rules : a new alignment technique is used to map the phonological form of spontaneous sentences onto the phonetic surface form . a machine-learning algorithm then calculates a set of phonologi-cal replacement rules together with their conditional probabilities . a critical analysis of the resulting probabilistic rule set is presented and discussed with regard to regional italian accents . the rule set presented here is also applied in the newly published web-service webmaus that allows a user to segment and phonetically label italian speech via a simple web-interface .,0
978,"This paper presents a mean square analysis of a fast filtered-x affine projection algorithm for active noise control applications. The filtered-x affine projection algorithm is a computationally demanding method used to enhance the performance of the affine projection algorithm in single-channel ANC systems. To address this issue, we propose a modified filtered-x scheme that improves energy conservation arguments and steady-state behavior of the filtered-x scheme. The theoretical expressions for the mean square error of the proposed algorithm are derived and compared to those of the original filtered-x scheme. Our analysis shows that the proposed algorithm provides better performance than the original scheme, with lower mean square error and faster convergence rates. We also investigate the signal distribution in the system and show that the proposed algorithm can effectively suppress noise while preserving the desired signal. Our results demonstrate the effectiveness of the proposed fast filtered-x affine projection algorithm in active noise control applications and its potential for further improvement.",1
979,"this paper provides an analysis of the steady-state behavior of the filtered-x affine projection algorithm . this efficient affine projection algorithm for active noise control applications is based on the filtered-x scheme , unlike most ap algorithms based on the more computationally demanding modified filtered-x scheme . this study depends on energy conservation arguments and does not require an specific signal distribution . the theoretical expressions derived for the mean square error allowed to accurately predict the steady-state performance of the affine projection algorithm for meaningful practical cases . simulation results of a single-channel anc system validate the analysis and the theoretical expressions derived .",0
980,"This paper presents a web-driven, case-based approach to comprehending and generating apt metaphors in figurative language. We focus on hard cases, which include non-explicit metaphors, marked-ness of similes, cryptic allusions, and crossword puzzles. To address these challenges, we propose a computational agent that utilizes a metaphor case-base, which includes categorizations of property-attribution metaphors, illustrative examples, and tacit knowledge. The agent leverages both the case-base and WordNet to comprehend and generate apt metaphors in a variety of contexts. Our approach is demonstrated using several examples of figurative language, including similes. Our results show that the proposed approach is effective in comprehending and generating apt metaphors in various contexts, including those that are difficult to understand for humans. Additionally, we demonstrate the importance of hand-crafted resources in building a high-quality case-base for metaphor comprehension and generation. Our research contributes to the field of figurative language processing by providing a new approach to comprehending and generating apt metaphors in a variety of contexts.",1
981,"examples of figurative language can range from the explicit and the obvious to the implicit and downright enigmatic . some simpler forms , like simile , often wear their meanings on their sleeve , while more challenging forms , like metaphor , can make cryptic allusions more akin to those of riddles or crossword puzzles . in this paper we argue that because the same concepts and properties are described in either case , a computational agent can learn from the easy cases -lrb- explicit similes -rrb- how to comprehend and generate the hard cases -lrb- non-explicit metaphors -rrb- . we demonstrate that the marked-ness of similes allows for a large case-base of illustrative examples to be easily acquired from the web , and present a system , called sardonicus , that uses this case-base both to understand property-attribution metaphors and to generate apt metaphors for a given target on demand . in each case , we show how the text of the web is used as a source of tacit knowledge about what cate-gorizations are allowable and what properties are most contextually appropriate . overall , we demonstrate that by using the web as a primary knowledge source , a system can achieve a robust and scalable competence with metaphor while minimizing the need for hand-crafted resources like wordnet .",0
982,"This paper addresses the co-localization problem in real-world images, where objects of interest may have significant intra-class variation and inter-class diversity. We propose a joint image-box formulation and a convex quadratic program to discover objects in images and co-localize them with weakly supervised localization. Our approach leverages ground-truth annotations from object discovery datasets and addresses annotation noise in real-world settings. We also explore co-segmentation techniques to further improve our results. We evaluate our method on several datasets, including ImageNet, and show that our approach outperforms state-of-the-art methods in co-localization accuracy. Our results demonstrate the importance of joint image-box formulation and the effectiveness of our proposed convex quadratic program for co-localization in real-world images. Our research contributes to the field of computer vision by providing a new approach to co-localization that addresses the challenges of intra-class variation and inter-class diversity in real-world settings.",1
983,"in this paper , we tackle the problem of co-localization in real-world images . co-localization is the problem of simultaneously localizing -lrb- with bounding boxes -rrb- objects of the same class across a set of distinct images . although similar problems such as co-segmentation and weakly supervised localization have been previously studied , we focus on being able to perform co-localization in real-world settings , which are typically characterized by large amounts of intra-class variation , inter-class diversity , and annotation noise . to address these issues , we present a joint image-box formulation for solving the co-localization problem , and show how joint image-box formulation can be relaxed to a convex quadratic program which can be efficiently solved . we perform an extensive evaluation of our joint image-box formulation compared to previous state-of-the-art approaches on the challenging pascal voc 2007 and object discovery datasets . in addition , we also present a large-scale study of co-localization on imagenet , involving ground-truth annotations for 3,624 classes and approximately 1 million images .",0
984,"This paper proposes a discriminative adaptive training method based on factor analysis for noise robust speech recognition using the Vocal Tract Length Normalization (VTS) technique. The proposed method addresses the problem of diverse noise-degraded training data by employing a diagonal loading matrix, which is optimized using a Jacobi iteration procedure. The method uses in-car collected data and the Aurora4 task to evaluate the effectiveness of discriminative VTS. We also explore the use of canonical models and EM-based approaches to further improve the performance of the proposed method. Our experimental results demonstrate the superiority of our method over the traditional VTS method and other state-of-the-art methods in terms of recognition accuracy under various noise conditions. Our research contributes to the field of speech recognition by providing a discriminative approach to VTS that is robust to noise and able to handle diverse training data.",1
985,"vector taylor series -lrb- vts -rrb- model based compensation is a powerful approach for noise robust speech recognition . an important extension to this approach is vts adaptive training , which allows canonical models to be estimated on diverse noise-degraded training data . these canonical models can be estimated using em-based approaches , allowing simple extensions to discriminative vat . however to ensure a diagonal corrupted speech covariance matrix the jaco-bian -lrb- loading matrix -rrb- relating the noise and clean speech is diagonalised . in this work an approach for yielding optimal diagonal loading matrices based on minimising the expected kl-divergence between the diagonal loading matrix and '' correct '' distributions is proposed . the performance of discriminative vat using the standard and optimal diagonalisation was evaluated on both in-car collected data and the aurora4 task .",0
986,"This paper presents a data-driven approach to estimate 2-D human pose using belief propagation (BP) and Monte Carlo algorithms. The proposed method leverages bottom-up reasoning mechanisms with hand-labeled images to capture the human body configuration. The method combines bottom-up visual cues, importance sampling functions, and probabilistic inference techniques to solve the 2-D human pose estimation problem. Specifically, low-dimensional representations, statistical formulations, and Markov networks are used to model the relationship between shape, appearance, and edge information. The proposed approach demonstrates superior performance compared to state-of-the-art methods in 2-D human pose estimation using single images. The results suggest that the proposed approach has the potential to solve complex inference tasks related to human pose estimation in various applications.",1
987,"we propose a statistical formulation for 2-d human pose estimation from single images . the human body configuration is modeled by a markov network and the estimation problem is to infer pose parameters from image cues such as appearance , shape , edge , and color . from a set of hand labeled images , we accumulate prior knowledge of 2-d body shapes by learning their low-dimensional representations for inference of pose parameters . a data driven belief propagation monte carlo algorithm , utilizing importance sampling functions built from bottom-up visual cues , is proposed for efficient probabilistic inference . contrasted to the few sequential statistical formulations in the literature , our statistical formulation integrates both top-down as well as bottom-up reasoning mechanisms , and can carry out the inference tasks in parallel . experimental results demonstrate the potency and effectiveness of the proposed statistical formulation in estimating 2-d human pose from single images .",0
988,"This paper proposes a Dimensional Contextual Semantic Model (DCSM) for music description and retrieval. The DCSM is a novel approach that combines categorical and dimensional models to generate high-level music descriptions that can be used in a semantic music search engine. The model is based on graded descriptions that capture the complexity and richness of musical content, while addressing the issue of polysemy. The DCSM utilizes semantic relations between musical content and context-aware fashion to generate semantic descriptions that can be used for music retrieval. The proposed model is evaluated on a large music dataset, and the results demonstrate its effectiveness in generating high-quality music descriptions and improving the accuracy of music retrieval. The proposed DCSM represents a significant improvement over traditional categorical and dimensional models for music description and retrieval, and has the potential to enable more accurate and efficient music retrieval in a variety of applications.",1
989,"several paradigms for high-level music descriptions have been proposed to develop effective system for browsing and retrieving musical content in large repositories . such paradigms are based on either categorical or dimensional models . the interest in dimensional models has recently grown a great deal , as they define a semantic relation between concepts through graded descriptions . one problem that affects semantic descriptions is the ambiguity that often arises from using the same descriptor in different contexts . in order to overcome this difficulty , it is important to dimensional contextual semantic model and address polysemy , which is the property of words to take on different meanings depending on the use-context . in this paper we propose a dimensional contextual semantic model for defining semantic relations among descriptors in a context-aware fashion . this dimensional contextual semantic model is here used for developing a semantic music search engine . in order to evaluate the effectiveness of our dimensional contextual semantic model , we compare this dimensional contextual semantic model with two systems that are based on different description models .",0
990,"This paper proposes an approach for automatic lecture transcription by utilizing presentation slide information to adapt a language model. The proposed method employs a robust adaptation scheme that leverages global and local slide information, keyword and topic information, and a cache model to improve the recognition accuracy of content keywords. Specifically, the probabilistic latent semantic analysis (PLSA) is used to detect the keywords and global topic adaptation is applied to the language model for adaptation. The experimental results show that the proposed approach achieves a higher detection rate of content keywords and improves the recognition accuracy of keywords compared to other methods. Moreover, the use of global and local slide information and topic information further enhances the robustness of the adaptation scheme. The proposed method can be applied to real lectures and web text with both global and local preference, demonstrating its potential in automatic lecture transcription.",1
991,"the paper addresses language model adaptation for automatic lecture transcription by fully exploiting presentation slide information used in the lecture . as the text in the presentation slides is small in its size and fragmentary in its content , a robust adaptation scheme is addressed by focusing on the keyword and topic information . several methods are investigated and combined ; first , global topic adaptation is conducted based on plsa -lrb- probabilistic latent semantic analysis -rrb- using keywords appearing in all slides . web text is also retrieved to enhance the relevant text . then , local preference of the keywords are reflected with a cache model by referring to the slide used during each utterance . experimental evaluations on real lectures show that the proposed method combining the global and local slide information achieves a significant improvement of recognition accuracy , especially in the detection rate of content keywords .",0
992,"This paper presents a method for large vocabulary continuous speech recognition (LVCSR) using context-expanded region-dependent linear transforms that are trained in a tied-state based discriminative approach. The proposed method utilizes long-span features and contextual weight expansion to improve the performance of the lattice-based discriminative training. The maximum mutual information criterion is used to train the context-expanded region-dependent linear transforms, which are then used for acoustic modeling with hidden Markov models (HMMs). The discriminative feature weight expansion and the lattice-free, boosted MMI training are also employed to further enhance the performance of the system. The proposed approach is evaluated on the Switchboard-1 conversational telephone speech transcription task and achieves a significant relative word error rate reduction compared to other methods. The results demonstrate that the use of long-span features and context-expanded region-dependent linear transforms can improve the performance of LVCSR systems. Moreover, the proposed method provides a more robust and effective approach for acoustic modeling with HMMs.",1
993,"we present a new discriminative feature transform approach to large vocabulary continuous speech recognition using gaussian mixture density hidden markov models -lrb- gmm-hmms -rrb- for acoustic modeling . the feature transform is formulated with a set of context-expanded region-dependent linear transforms utilizing both long-span features and contextual weight expansion . the context-expanded region-dependent linear transforms are estimated by lattice-free , tied-state based discriminative training using maximum mutual information criterion , while the gmm-hmms are trained by conventional lattice-based , boosted mmi training . compared with two baseline systems , which use context-expanded region-dependent linear transforms with either long-span features or weight expansion only and are trained using the conventional lattice-based discriminative training for both context-expanded region-dependent linear transforms and hmms , the proposed approach achieves a relative word error rate reduction of 10 % and 6 % respectively on switchboard-1 conversational telephone speech transcription task .",0
994,"This paper proposes a closed form recursive solution for the Maximum Correntropy criterion, which is a non-Gaussian, robust alternative to the traditional Mean Square Error criterion for adaptive systems training. The Maximum Correntropy criterion is based on the cross-correntropy of the error pdf and the weighted least squares pdf, and has been shown to be effective in dealing with outliers and non-Gaussian noise in adaptive filters. The proposed closed form recursive solution for the Maximum Correntropy criterion is derived from the recursive least squares (RLS) algorithm, and provides a computationally efficient and accurate solution for adaptive filter weights. The proposed method is compared to gradient-based training and the RLS algorithm, and is shown to provide superior performance in terms of convergence rate and robustness to outliers. The experimental results demonstrate the effectiveness and efficiency of the proposed closed form recursive solution for the Maximum Correntropy criterion, and its potential for applications in various adaptive systems training tasks.",1
995,"this paper presents a closed form recursive solution for training adaptive filters using the maximum correntropy criterion . correntropy has been recently proposed as a robust similarity measure between two random variables or signals , when the pdf s involved are heavy tailed and non-gaussian . maximizing the cross-correntropy between the output of an adaptive filters and the desired response leads to the maximum correntropy criterion for adaptive systems training . we show that a closed form , closed form recursive solution of the filter weights using this closed form yields a simple weighted least squares like formulation . our simulations show that training the filter weights using this closed form recursive solution is much faster than gradient based training , and more accurate than the rls algorithm in cases where the error pdf is non-gaussian and heavy tailed .",0
996,"This paper addresses the problem of pursuit and capture in polygonal environments with obstacles. Given a simply-connected polygon of n vertices and a set of o (log n) pursuers, the goal is to capture an evader that moves at maximum speed in a deterministic fashion. The problem is approached using a visibility-based pursuit-evasion strategy that relies on the line-of-sight detection and a discrete-time step framework. The pursuit strategy assumes that the evader has a minimum feature size property, and the search strategy is deterministic. The proposed method is evaluated in arbitrary polygonal environments, and the experimental results show that the visibility-based approach is effective in capturing the evader in various scenarios. The proposed method provides a practical solution to the pursuit and capture problem in polygonal environments with obstacles, and can be applied to various real-world scenarios, such as surveillance and security systems. Moreover, the study of the line-of-sight detection and its relationship with the number of pursuers sheds light on the fundamental aspects of the pursuit and capture problem.",1
997,"we resolve a several-years old open question in visibility-based pursuit evasion : how many pursuers are needed to capture an evader in an arbitrary polygonal environment with obstacles ? the evader is assumed to be adversarial , moves with the same maximum speed as pursuers , and is '' sensed '' by a pursuer only when it lies in line-of-sight of that pursuer . the players move in discrete time steps , and the capture occurs when a pursuer reaches the position of the evader on its move . our main result is that o -lrb- √ h + log n -rrb- pursuers can always win the game with a deterministic search strategy in any polygon with n vertices and h obstacles -lrb- holes -rrb- . in order to achieve this bound , however , we argue that the environment must satisfy a minimum feature size property , which essentially requires the minimum distance between any two vertices to be of the same order as the speed of the players . without the minimum feature size assumption , we show that ω -lrb- n / log n -rrb- pursuers are needed in the worst-case even for simply-connected polygons of n vertices ! this reveals an unexpected subtlety that seems to have been overlooked in previous work claiming that o -lrb- log n -rrb- pursuers can always win in simply-connected n-gons . our lower bound also shows that capturing an evader is inherently more difficult than just '' seeing '' it because o -lrb- log n -rrb- pursuers are prov-ably sufficient for line-of-sight detection even against an arbitrarily fast evader in simple n-gons .",0
998,"This paper proposes an approach to improve grapheme-to-phoneme (G2P) conversion using a WFST-based G2P framework with alignment constraints and RNNLM n-best rescoring. The authors use an EM-driven alignment algorithm to incorporate structural constraints in the conversion process and then apply an open-source toolkit for G2P conversion on various datasets. To further enhance the accuracy of G2P conversion, the authors introduce an n-best rescoring mechanism using a recurrent neural network language model. Experimental results demonstrate that the proposed approach achieves a significant improvement in word accuracy compared to the traditional WFST-based G2P conversion method.",1
999,"this work introduces a modified wfst-based multiple to multiple em-driven alignment algorithm for grapheme-to-phoneme conversion , and preliminary experimental results applying a recurrent neural network language model as an n-best rescoring mechanism for g2p conversion . the alignment algorithm leverages the wfst-based g2p framework and introduces several simple structural constraints which yield a small but consistent improvement in word accuracy on a selection of standard base-lines . the recurrent neural network language model further extends these gains and achieves state-of-the-art performance on four standard g2p datasets . the system is also shown to be significantly faster than existing solutions . finally , the complete wfst-based g2p framework is provided as an open-source toolkit .",0
1000,"This paper presents an efficient approach to solve the minimal labeling problem of temporal and spatial qualitative constraints. The problem arises in the context of qualitative constraint networks, which are used to represent qualitative temporal and topological relations between objects. The region connection calculus (RCC-8) is a commonly used formalism for this purpose. The paper proposes a new approach based on the ◆-G-consistency property and partial consistency techniques. The approach is shown to be effective for solving the minimal labeling problem in chordal QCNS, which are a class of RCC-8 networks with certain structural constraints. The paper also discusses the use of interval algebra and region connection calculus in artificial intelligence approaches.",1
1001,"the interval algebra and a subset of the region connection calculus , namely rcc-8 , are the dominant artificial intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively . such qualitative temporal and topological relations can be formulated as a qualitative constraint network . in this paper , we focus on the minimal labeling problem and we propose an algorithm to efficiently derive all the feasible base relations of a qualitative constraint network . our algorithm considers chordal qcns and a new form of partial consistency which we define as ◆ g-consistency . further , the proposed algorithm uses tractable subclasses of relations having a specific patchwork property for which-consistency implies the consistency of the input qualitative constraint network . experi-mentations with qualitative constraint network of interval algebra and rcc-8 show the importance and efficiency of this new approach .",0
1002,"This paper presents a novel optical flow framework that combines multiple data models to estimate optical flow via locally adaptive fusion of complementary data costs. By incorporating a minimum description length constraint, an energy model is formulated that incorporates both data and regularization terms. The proposed method is evaluated on the Middlebury optical flow benchmark and outperforms state-of-the-art methods in terms of accuracy and robustness to matching ambiguity. The locally varying data term allows for the effective handling of ill-posed problems, making the method suitable for a wide range of optical flow estimation applications.",1
1003,"many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems . in this paper , in contrast to the conventional optical flow framework that uses a single or fixed data model , we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models . the locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models . the optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint . from these chosen data models , a new optical flow estimation energy model is designed with the weighted sum of the multiple data models , and a convex optimization-based highly effective and practical solution that finds the optical flow , as well as the weights is proposed . comparative experimental results on the middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-of-the art methods .",0
1004,"This paper proposes a method to extract the semantic orientations of words using the Spin Model. The semantic orientations of words are important in natural language processing tasks such as sentiment analysis. However, the task is intractable due to the large number of possible word combinations. The Spin Model is a physics-inspired approach that models the orientations of spins of electrons to compute the actual probability function. The paper presents an approximate probability function for semantic orientations based on the Spin Model and uses a mean field approximation to make the task computationally feasible. The proposed method is evaluated on the English lexicon and seed words to show its accuracy in extracting semantic orientations. The paper also discusses the selection of parameters for the Spin Model to improve the accuracy of the method. Overall, the paper presents a promising approach for extracting semantic orientations of words using the Spin Model and mean field approximation.",1
1005,"we propose a method for extracting semantic orientations of words : desirable or undesirable . regarding semantic ori-entations as spins of electrons , we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function . we also propose a criterion for parameter selection on the basis of magnetization . given only a small number of seed words , the proposed method extracts semantic orienta-tions with high accuracy in the experiments on english lexicon . the result is comparable to the best value ever reported .",0
1006,"This paper investigates the value of pairwise constraints in classification and consistency using both real-world and simulated datasets. The authors explore the impact of pairwise examples on optimal linear decision boundaries and asymptotic variance. The study shows that incorporating pairwise constraints in classification can improve the decision boundary and lead to better accuracy. Furthermore, the use of pairwise constraints can increase consistency in the labeling of data points. These findings demonstrate the potential benefits of incorporating pairwise constraints in binary variable classification tasks.",1
1007,"in this paper we consider the problem of classification in the presence of pairwise constraints , which consist of pairs of examples as well as a binary variable indicating whether they belong to the same class or not . we propose a method which can effectively utilize pairwise constraints to construct an estimator of the decision boundary , and we show that the resulting estimator is sign-insensitive consistent with respect to the optimal linear decision boundary . we also study the asymptotic variance of the estimator and extend the method to handle both labeled and pairwise examples in a natural way . several experiments on simulated datasets and real world classification datasets are conducted . the results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications .",0
1008,"This paper presents an approach to large-scale syntactic language modeling using treelets. The method uses overlapping windows of tree context to build a generative, syntactic language model, which is compared to n-gram language models using estimation techniques on automatically parsed text. Positive data grammaticality tasks are used to evaluate the discriminative models against the generative baselines. Results show that the generative, syntactic language model outperforms n-gram models, and that discriminative models perform well on grammaticality tasks. The study suggests that using overlapping windows of tree context can lead to improved language models for large-scale syntactic modeling.",1
1009,"we propose a simple generative , syntactic language model that conditions on overlapping windows of tree context -lrb- or treelets -rrb- in the same way that n-gram language models condition on overlapping windows of linear context . we estimate the parameters of our generative , syntactic language model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques , allowing us to train a generative , syntactic language model on over one billion tokens of data using a single machine in a matter of hours . we evaluate on perplexity and a range of grammaticality tasks , and find that we perform as well or better than n-gram models and other generative baselines . our generative , syntactic language model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks , despite training on positive data alone . we also show fluency improvements in a preliminary machine translation experiment .",0
1010,"This paper presents a method to cope with the problem of imbalanced prosodic unit boundary detection, by using linguistically motivated prosodic features and machine learning techniques. Prosodic boundaries are important cues in speech recognition systems, but detecting them accurately can be challenging due to imbalanced class distributions. The proposed approach uses prosodically defined units and decision tree classifier to classify them. Linguistically motivated prosodic features are extracted from the audio data to improve the detection accuracy. The results of experiments conducted on the BMPM dataset show that the proposed approach outperforms the state-of-the-art methods and can be effective for ASR processing with minority class. The findings also suggest that the C4.5 algorithm is a suitable machine learning technique for imbalanced prosodic unit boundary detection.",1
1011,"continuous speech input for asr processing is usually pre-segmented into speech stretches by pauses . in this paper , we propose that smaller , prosodically defined units can be identified by tackling the problem on imbalanced prosodic unit boundary detection using five machine learning techniques . a parsimonious set of linguistically motivated prosodic features has been proven to be useful to characterize prosodic boundary information . furthermore , bmpm is prone to have true positive rate on the minority class , i.e. the defined prosodic units . as a whole , the decision tree classifier , c4 .5 , reaches a more stable performance than the other algorithms .",0
1012,"This paper proposes an approach to automatically build a kinematic chain from feature trajectories of articulated objects. The method involves non-rigid articulated parts segmentation, human motion tracking, and data set generation. The feature trajectories are used to construct a graph structure, which is then analyzed using minimum spanning tree and spectral clustering techniques. Affine projections are applied to the segmented motion subspaces, and local sampling is performed to extract the kinematic chain from the articulated object. The proposed method is evaluated on different data sets of articulated objects, and the results show that the approach is effective in building the kinematic chain from feature trajectories of articulated objects.",1
1013,"we investigate the problem of learning the structure of an articulated object , i.e. its kinematic chain , from feature tra-jectories under affine projections . we demonstrate this possibility by proposing an algorithm which first segments the trajectories by local sampling and spectral clustering , then builds the kinematic chain as a minimum spanning tree of a graph constructed from the segmented motion subspaces . we test our method in challenging data sets and demonstrate the ability to automatically build the kinematic chain of an articulated object from feature trajectories . the algorithm also works when there are multiple articulated objects in the scene . furthermore , we take into account non-rigid articulated parts that exist in human motions . we believe this advance will have impact on articulated object tracking and dynamical structure from motion .",0
1014,"This paper proposes a new approach for object detection using context-sensitive decision forests. The decision forest framework is extended to take into account contextual information and context-based decision criteria, leading to improved performance. An intermediate prediction step is introduced to refine the output and regression mode selection is used to optimize the split criterion. Tree-structured classifiers are used for the final classification step. The proposed method is evaluated on the TUD dataset for pedestrian detection, and results show improved performance compared to traditional decision forests. The paper presents a new perspective on context-sensitive decision forests and demonstrates their effectiveness in improving object detection tasks.",1
1015,"in this paper we introduce context-sensitive decision forests-a new perspective to exploit contextual information in the popular decision forest framework for the object detection problem . they are tree-structured classifiers with the ability to access intermediate prediction -lrb- here : classification and regression -rrb- information during training and inference time . this intermediate prediction is available for each sample and allows us to develop context-based decision criteria , used for refining the decision forest framework . in addition , we introduce a novel split criterion which in combination with a priority based way of constructing the trees , allows more accurate regression mode selection and hence improves the current context information . in our experiments , we demonstrate improved results for the task of pedestrian detection on the challenging tud data set when compared to state-of-the-art methods .",0
1016,"This paper proposes a new approach to Statistical Machine Translation (SMT) using a Bilingual Correspondence Recursive Autoencoder (BCRA) that learns bilingual phrase representations with semantic and structural similarity features. The BCRA employs a neural network model with a joint objective to learn semantic relations between bilingual phrases and to align them with tree structures. The BCRA uses cross-lingual reconstruction error and alignment-consistent phrase structures to achieve structural alignment consistency. Experimental results on the NIST Chinese-English test sets show that the proposed model outperforms state-of-the-art SMT systems in terms of BLEU score. The proposed approach also enables the learning of semantic representations, which can be used to improve other natural language processing tasks.",1
1017,"learning semantic representations and tree structures of bilingual phrases is beneficial for statistical machine translation . in this paper , we propose a new neu-ral network model called bilingual correspondence recursive autoencoder to model bilingual phrases in translation . we incorporate word alignments into bilingual correspondence recursive autoencoder to allow neu-ral network model freely access bilingual constraints at different levels . bilingual correspondence recursive autoencoder minimizes a joint objective on the combination of a recursive au-toencoder reconstruction error , a structural alignment consistency error and a cross-lingual reconstruction error so as to not only generate alignment-consistent phrase structures , but also capture different levels of semantic relations within bilingual phrases . in order to examine the effectiveness of bilingual correspondence recursive autoencoder , we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by bilingual correspondence recursive autoencoder into a state-of-the-art smt system . experiments on nist chinese-english test sets show that our neu-ral network model achieves a substantial improvement of up to 1.55 bleu points over the baseline .",0
1018,"This paper proposes the use of graphical models for mixed-initiative dialog management systems with real-time policies. The focus is on improving the dialog modeling and management in systems such as air traveling and information systems that require natural spoken dialogs between the user and the system. The paper highlights the need for error recognition capabilities and system flexibility in such systems. Statistical dialog models are used to improve speech recognition and sensitivity rates, while also increasing the specificity rate. The paper evaluates the proposed approach based on the average dialog length and its applicability to a reference system. The results demonstrate the effectiveness of the proposed approach in improving the overall performance of the system.",1
1019,"in this paper , we present a novel approach for dialog modeling , which extends the idea underlying the partially observable markov decision processes -lrb- pomdps -rrb- , i. e. it allows for calculating the dialog modeling in real-time and thereby increases the system flexibility . the use of statistical dialog models is particularly advantageous to react adequately to common errors of speech recognition systems . comparing our results to the reference system , we achieve a relative reduction of 31.6 % of the average dialog length . furthermore , the proposed system shows a relative enhancement of 64.4 % of the sensitivity rate in the error recognition capabilities using the same specifity rate in both systems . the achieved results are based on the air travelling information system with 21 650 user utterances in 1 585 natural spoken dialogs .",0
1020,"This paper proposes a novel approach for large vocabulary continuous speech recognition using structured data and a weighted finite state transducer (WFST)-based linear classifier. The system employs a discriminative approach with a distributed perceptron algorithm for training the large-scale linear classifier. The WFST-based decoding process uses information source models, including hidden Markov models and n-gram models, to create a decoding graph that is used for the linear classification process. The proposed system achieves state-of-the-art results on several benchmark datasets and outperforms traditional speech recognition systems that use acoustic models. The results demonstrate that the use of structured data and a linear classifier can significantly improve the accuracy and efficiency of large vocabulary continuous speech recognition tasks.",1
1021,"this paper describes a discriminative approach that further advances the framework for weighted finite state transducer based decoding . the discriminative approach introduces additional linear models for adjusting the scores of a decoding graph composed of conventional information source models -lrb- e.g. , hidden markov models and n-gram models -rrb- , and reviews the wfst-based decoding process as a linear classifier for structured data -lrb- e.g. , sequential multiclass data -rrb- . the difficulty with the discriminative approach is that the number of dimensions of the additional linear models becomes very large in proportion to the number of arcs in a weighted finite state transducer , and our previous study only applied it to a small task -lrb- timit phoneme recognition -rrb- . this paper proposes a training method for a large-scale linear classifier employed in wfst-based decoding by using a distributed perceptron algorithm . the experimental results show that the proposed discriminative approach was successfully applied to a large vocabulary continuous speech recognition task , and achieved an improvement compared with the performance of the minimum phone error based discrimina-tive training of acoustic models .",0
1022,The paper proposes a method for detecting email batches in a streaming data scenario using supervised clustering. The approach leverages the collective information of jointly generated messages and their collective attributes to identify batches of emails. The proposed technique uses a sequential decoding procedure that operates in linear time to cluster the emails. The paper addresses the decoding problem of detecting spam emails and identifying email batches by proposing effective decoding procedures. The proposed method aims to improve the accuracy of batch detection in email streams.,1
1023,"we address the problem of detecting batches of emails that have been created according to the same template . this problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages . the application matches the problem setting of supervised clustering , because examples of correct clusterings can be collected . known decoding procedures for supervised clustering are cubic in the number of instances . when decisions can not be reconsidered once they have been made -- owing to the streaming nature of the data -- then the decoding problem can be solved in linear time . we devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering . we study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails .",0
1024,"This paper proposes a solution to the low-power programmable DSP problem by utilizing dynamic reconfiguration of hardware modules. The authors argue that the traditional instruction-based engines do not provide the necessary power reduction required for wireless communication components. By adopting a reconfigurable processing approach, the power dissipation can be reduced significantly. The paper presents an application-specific approach to designing a programmable environment that can dynamically reconfigure its hardware modules. The proposed approach is evaluated using wireless communication components, and the results show a significant reduction in power dissipation. Overall, this paper presents a promising solution for achieving low-power programmable DSP using a reconfigurable processing approach.",1
1025,"one of the most compelling issues in the design of wireless communication components is to keep power dissipation between bounds . while low-power solutions are readily achieved in an application-specific approach , doing so in a programmable environment is a substantially harder problem . this paper presents an approach to low-power programmable dsp that is based on the dynamic reconfigura-tion of hardware modules . this technique has shown to yield at least an order of magnitude of power reduction compared to traditional instruction-based engines for problems in the area of wireless communication .",0
1026,"This paper proposes a novel approach for modeling and equalizing audio systems using Kautz filters. The aim is to achieve better auditory frequency resolution and loudspeaker response equalization through frequency warping and related techniques. The paper presents a detailed analysis of the transfer function modeling and its application in audio signal processing. The proposed method utilizes Kautz filters, which are known for their frequency warping-related resolution and are applied in conjunction with Laguerre filters and allpass structures. The method is evaluated on various audio systems, including guitar body modeling, and the results demonstrate the effectiveness of the proposed approach in achieving accurate and efficient equalization of audio systems.",1
1027,frequency warping using allpass structures or laguerre filters has found increasingly applications in audio signal processing due to good match with the auditory frequency resolution . kautz filters are an extension where the frequency warping and related resolution can have more freedom . in this paper we discuss the properties of kautz filters and how kautz filters meet typical requirements found in modeling and equalization of audio systems . case studies include transfer function modeling of the guitar body and loudspeaker response equalization .,0
1028,"This paper proposes a novel approach to improve automatic speech recognition (ASR) using tangent distance. The proposed method is based on a probabilistic framework and uses Gaussian mixture densities (GMD) to model the observation vectors. The approach is evaluated using telephone line recorded German digit strings and the Sietill corpus. The results show that the use of tangent distance in combination with GMD improves the ASR performance. Tangent distance is able to capture the local geometry of the high dimensional feature space and allows for more accurate modelling of the prototype vectors and variances. The proposed method can also be used in other areas, such as image and object recognition, where high dimensional feature spaces are present.",1
1029,"in this paper we present a new approach to variance modelling in automatic speech recognition that is based on tangent distance . using tangent distance , classifiers can be made invariant w.r.t. small classifiers of the data . such classifiers generate a manifold in a high dimensional feature space when applied to an observation vector . while conventional classifiers determine the distance between an observation and a prototype vector , tangent distance approximates the minimum distance between their manifolds , resulting in classification that is invariant w.r.t. the underlying transformation . recently , this approach was successfully applied in image object recognition . in this paper we describe how tangent distance can be incorporated into automatic speech recognition based on gaussian mixture densities . the proposed method is embedded into a probabilistic framework . experiments performed on the sietill corpus for telephone line recorded german digit strings show a significant improvement in comparison with a conventional gmd approach using a comparable amount of model parameters .",0
1030,"The paper describes the I4U system that participated in the NIST 2008 speaker recognition evaluation. The I4U system is a speaker recognition system that uses cepstral features and classifiers. The paper focuses on the use of cepstral features and their role in the performance of the system. It also discusses the classifiers used in the I4U system and their contribution to the system's overall performance. Overall, the paper provides insights into the I4U system and its performance in the NIST 2008 speaker recognition evaluation.",1
1031,"this paper describes the performance of the i4u speaker recognition system in the nist 2008 speaker recognition evaluation . the i4u speaker recognition system consists of seven subsystems , each with different cepstral features and classifiers . we describe the i4u speaker recognition system and report on its core test results as they were submitted , which were among the best-performing submissions . the i4u effort was led by the",0
1032,"This paper proposes a learning scheme for generating expressive music performances of jazz standards. The system involves an induced expressive transformation model that transforms inexpressive melody descriptions into expressive ones. The system also includes a machine learning component and a melodic transcription component that extracts acoustic features. The melodic transcription component and machine learning component work together to learn the expressive audio from monophonic recordings. The proposed system shows promising results in generating expressive jazz performances, which could have implications for music production and education.",1
1033,"we describe our approach for generating expressive music performances of monophonic jazz melodies . it consists of three components : -lrb- a -rrb- a melodic transcription component which extracts a set of acoustic features from monophonic recordings , -lrb- b -rrb- a machine learning component which induces an expressive transformation model from the set of extracted acoustic features , and -lrb- c -rrb- a melody synthesis component which generates expressive mono-phonic output -lrb- midi or audio -rrb- from inexpressive melody descriptions using the induced expressive transformation model . in this paper we concentrate on the machine learning component , in particular , on the learning scheme we use for generating expressive audio from a score .",0
1034,"This paper proposes a method for opponent modeling in deep reinforcement learning (RL). The approach utilizes a mixture-of-experts architecture to model the opponent's behavior in multi-agent settings, such as in simulated soccer games or trivia games. By explicitly modeling secondary agents, the proposed method improves the performance of deep Q-networks in opponent-rich environments. The opponent modeling is done through parameterized strategies, which are learned by neural-based models. The paper presents several experiments that demonstrate the effectiveness of the approach, and shows that it can improve the policy and multitasking capabilities of deep RL agents. The proposed method provides a new direction for incorporating opponent modeling into deep RL and can be extended to other domains where explicit modeling of opponents is important.",1
1035,"opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies , yet it remains challenging because strategies interact with each other and change . most previous work focuses on developing probabilistic models or parameterized strategies for specific applications . inspired by the recent success of deep reinforcement learning , we present neural-based models that jointly learn a policy and the behavior of opponents . instead of explicitly predicting the opponent 's action , we encode observation of the opponents into a deep q-network ; however , we retain explicit modeling -lrb- if desired -rrb- using multitasking . by using a mixture-of-experts architecture , our model automatically discovers different strategy patterns of opponents without extra supervision . we evaluate our models on a simulated soccer game and a popular trivia game , showing superior performance over deep q-network and its variants .",0
1036,"This paper compares parametric and non-parametric approaches for automatic speech recognition (ASR) using both structure-based and template-based methods. The study examines algorithms that go beyond hidden Markov model (HMM) frameworks to incorporate phonetic detail in ASR. Specifically, the focus is on acoustic modeling, speech features, and HMMs. The results provide insights into the relative performance of parametric and non-parametric approaches in ASR, and highlight the importance of incorporating phonetic detail in ASR systems.",1
1037,"this paper provides an introductory tutorial for the interspeech07 special session on '' structure-based and template-based automatic speech recognition '' . the purpose of the special session is to bring together researchers who have special interest in novel techniques that are aimed at overcoming weaknesses of hmms for acoustic modeling in speech recognition . numerous such approaches have been taken over the past dozen years , which can be broadly classified into structured-based -lrb- parametric -rrb- and template-based -lrb- non-parametric -rrb- ones . in this paper , we will provide an overview of both approaches , focusing on the incorporation of long-range temporal dependencies of the speech features and phonetic detail in speech recognition algorithms . we will provide a high-level survey on major existing work and systems using these two types of '' beyond-hmm '' frameworks . the contributed papers in this special session will elaborate further on the related topics .",0
1038,"This paper presents three generative, lexicalised models for statistical parsing. The models utilize a probabilistic treatment of sub-categorization and wh-movement and are evaluated using the Wall Street Journal text corpus. The paper focuses on the constituent precision/recall of the statistical parsing model and how it is affected by the different generative models presented. The models are based on lexicalized context-free grammars and aim to improve the accuracy of statistical parsing.",1
1039,"in this paper we first propose a new statistical parsing model , which is a genera-tive model of lexicalised context-free grammar . we then extend the statistical parsing model to include a probabilistic treatment of both sub-categorisation and wh-movement . results on wall street journal text show that the statistical parsing model performs at 88.1 / 87.5 % constituent precision/recall , an average improvement of 2.3 % over -lrb- collins 96 -rrb- .",0
1040,"This paper proposes a method for efficient computation of squared curvature, which is used in the analysis of elongated structures in computer vision. The method involves using submodular and super-modular pairwise potentials, and a length-based regularization, to efficiently compute the squared curvature in both high and low angular resolutions. The proposed method is based on the trust region framework and integral geometry, and is shown to be more efficient than existing methods. The paper demonstrates the effectiveness of the proposed method in the analysis of elongated structures, which is important in many applications in computer vision.",1
1041,"curvature has received increasing attention as an important alternative to length based regularization in computer vision . in contrast to length , it preserves elongated structures and fine details . existing approaches are either inefficient , or have low angular resolution and yield results with strong block artifacts . we derive a new model for computing squared curvature based on integral geometry . the model counts responses of straight line triple cliques . the corresponding energy decomposes into submodular and super-modular pairwise potentials . we show that this energy can be efficiently minimized even for high angular resolutions using the trust region framework . our results confirm that we obtain accurate and visually pleasing solutions without strong artifacts at reasonable runtimes .",0
1042,"This paper proposes a game-theoretic machine learning approach for revenue maximization in sponsored search, a popular monetization channel for search engines. The approach uses an empirical revenue maximization framework within a bilevel optimization framework and a genetic programming algorithm to predict bid sequences. The auction mechanism is modeled using game theory and Markov models. The proposed approach is evaluated on historical data and outperforms existing methods in terms of revenue maximization. The results demonstrate the effectiveness of the proposed approach in this highly competitive and dynamic domain.",1
1043,"sponsored search is an important monetization channel for search engines , in which an auction mechanism is used to select the ads shown to users and determine the prices charged from advertisers . there have been several pieces of work in the literature that investigate how to design an auction mechanism in order to optimize the revenue of the search engines . however , due to some unrealistic assumptions used , the practical values of these studies are not very clear . in this paper , we propose a novel game-theoretic machine learning approach , which naturally combines machine learning and game theory , and learns the auction mechanism using a bilevel optimization framework . in particular , we first learn a markov model from historical data to describe how advertisers change their bids in response to an auction mechanism , and then for any given auction mechanism , we use the learnt markov model to predict its corresponding future bid sequences . next we learn the auction mechanism through empirical revenue maximization on the predicted bid sequences . we show that the empirical revenue will converge when the prediction period approaches infinity , and a genetic programming algorithm can effectively optimize this empirical revenue . our experiments indicate that the proposed game-theoretic machine learning approach is able to produce a much more effective auction mechanism than several baselines .",0
1044,"This paper proposes an integrated approach for robust speaker recognition by combining feature normalization and enhancement techniques using Acoustic Factor Analysis (AFA). The method is based on a speaker/utterance dependent Gaussian Mixture Model (GMM) that employs transformation signal sub-space-based speech enhancement schemes and low-rank covariance structure of cepstral features. The proposed approach uses factor analysis-based channel compensation methods, mixture-dependent feature transformation, probabilistic mixture alignment, and eigenvector directions to reduce the dimensionality of the feature space. The AFA parameter model is used to estimate the speaker-specific acoustic feature space, and the resulting super-vector domain is used for de-correlation and variance normalization. The i-vector system is used for speaker recognition in the enhanced feature space, which yields improved performance compared to the baseline. Experimental results on various datasets show the effectiveness of the proposed approach in terms of speaker recognition accuracy.",1
1045,"state-of-the-art factor analysis based channel compensation methods for speaker recognition are based on the assumption that speaker/utterance dependent gaussian mixture model mean super-vectors can be constrained to lie in a lower dimensional subspace , which does not consider the fact that conventional acoustic features may also be constrained in a similar way in the feature space . in this study , motivated by the low-rank covariance structure of cepstral features , we propose a factor analysis model in the acoustic feature space instead of the super-vector domain and derive a mixture dependent feature transformation . we demonstrate that , the proposed acoustic factor analysis transformation performs feature dimensionality reduction , de-correlation , variance normalization and enhancement at the same time . the transform applies a square-root wiener gain on the acoustic feature eigenvector directions , and is similar to the signal sub-space based speech enhancement schemes . we also propose several methods of adaptively selecting the afa parameter for each mixture . the proposed feature transform is applied using a probabilistic mixture alignment , and is integrated with a conventional i-vector system . experimental results on the telephone trials of the nist sre 2010 demonstrate the effectiveness of the proposed factor analysis model .",0
1046,"This paper presents a method for verifying knowledge-based programs over description logic actions. Knowledge-based programs incorporate general domain knowledge and an agent's knowledge, allowing them to handle complex procedures. The paper proposes a DL-based action language for programming constructs and primitive actions, which can be used to describe physical and sensing actions. The verification of knowledge-based programs is based on test conditions and a restricted fragment of epistemic DL. The method is demonstrated through examples and experiments, showing its effectiveness in verifying knowledge-based programs over DL actions.",1
1047,"a knowledge-based program defines the behavior of an agent by combining primitive actions , programming constructs and test conditions that make explicit reference to the agent 's knowledge . in this paper we consider a setting where an agent is equipped with a description logic knowledge base providing general domain knowledge and an incomplete description of the initial situation . we introduce a corresponding new dl-based action language that allows for representing both physical and sensing actions , and that we then use to build knowledge-based programs with test conditions expressed in the epistemic dl . after proving undecidability for the general case , we then discuss a restricted fragment where verification becomes decidable . the provided proof is constructive and comes with an upper bound on the proce-dure 's complexity .",0
1048,"This paper presents an evaluation of a spoken language model based on a filler prediction model in the context of speech recognition. The study focuses on the Japanese National Diet Record, a transcribed corpus of domain-relevant topics, and investigates the performance of the language model in predicting fillers in the speech. The results of the evaluation demonstrate the effectiveness of the filler prediction model in improving the accuracy of the language model for filler recognition in the speech. Overall, the study highlights the potential benefits of incorporating domain-specific filler prediction models in speech recognition systems to improve their performance in real-world applications.",1
1049,"we propose a method that uses a filler prediction model for building a language model that includes fillers from a corpus without fillers . in our method , a filler prediction model is trained from a corpus that does not cover domain-relevant topics . it recovers fillers in inexact transcribed corpora in the target domain , and then a language model that includes fillers is built from the corpora . the results of an evaluation of the japanese national diet record showed that a model using our method achieves higher recognition performance than conventional ones .",0
1050,"This paper proposes a collective information extraction approach using Relational Markov Networks (RMNs) for improving the accuracy of information extraction systems. RMNs are an extension of Conditional Random Fields (CRFs) that allow modeling of arbitrary dependencies between variables, which is particularly useful in domains such as biomedical text where there are complex relationships between entities. The paper shows that RMNs outperform CRFs in terms of overall accuracy and provide better generalization by exploiting the inherent structure in the data. The proposed method is evaluated on a dataset of protein name extraction from biomedical text, and the results demonstrate its effectiveness in improving the performance of the information extraction system.",1
1051,"most information extraction systems treat separate potential extractions as independent . however , in many cases , considering influences between different potential extractions could improve overall accuracy . statistical methods based on undirected graphical models , such as conditional random fields , have been shown to be an effective approach to learning accurate ie systems . we present a new ie method that employs relational markov networks -lrb- a generalization of crfs -rrb- , which can represent arbitrary dependencies between extractions . this allows for '' collective information extraction '' that exploits the mutual influence between possible extractions . experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach .",0
1052,"This paper proposes a hybrid speech recognition system that combines Hidden Markov Models (HMMs) and Polynomial Classification for conversational speech recognition tasks. The proposed system models the speech features using a polynomial classifier, which estimates the density values in the feature space using a polynomial of Gaussian distributions. The emission probabilities are then calculated using both the HMMs and the polynomial classifier. Experimental results demonstrate that the hybrid approach achieves better performance than using either HMMs or polynomial classifiers alone.",1
1053,"in this paper , we present a hybrid speech recognizer combining hidden markov models and a polynomial classifier . in our hybrid speech recognizer the emission probabilities are not modeled as a mixture of gaus-sians but are calculated by the polynomial classifier . however , we do not apply the classifier directly to the feature vector but we make use of the density values of cents gaussians clustering the feature space . that means we model the emission probability as a polynomial of gaussian distributions of # - th degree . as most of these density values are approximately zero for a single feature vector the calculation of a polynomial can be done very efficiently . the usefulness of this hybrid speech recognizer was successfully tested on a large conversational speech recognition task .",0
1054,"This paper proposes a graphical model approach to pitch tracking using a probabilistic framework. The proposed approach involves a graphical model toolkit that enables pitch tracking parameters to be estimated using maximum likelihood sense. The model framework incorporates transition cost functions and probabilistic dependencies, allowing for accurate voicing decisions and pitch estimation. The approach is implemented using dynamic programming, and pitch trackers are tuned using the proposed graphical model framework. Experimental results demonstrate the effectiveness of the proposed approach for pitch tracking in various contexts, indicating its potential for use in real-world applications.",1
1055,"many pitch trackers based on dynamic programming require meticulous design of local cost and transition cost functions . the forms of these functions are often empirically determined and their parameters are tuned accordingly . parameter tuning usually requires great effort without a guarantee of optimal performance . this work presents a graphical model framework to automatically optimize pitch tracking parameters in the maximum likelihood sense . therein , probabilistic dependencies between pitch , pitch transition and acoustical observations are expressed using the language of graphical models , and probabilistic inference is accomplished using the graphi-cal model toolkit . experiments show that this graphical model framework not only expedites the design of a pitch trackers , but also yields remarkably good performance for both pitch estimation and voicing decision .",0
1056,"This paper presents a technique for learning informative and condensed feature representations from large unsupervised data sets, which can be used for supervised learning. The proposed approach uses a semi-supervised learning technique to learn a dense and low-dimensional feature space that captures the most important information in the data. The resulting feature representations can be used as input to a wide range of natural language processing (NLP) systems, including dependency parsing and named entity recognition. The approach is evaluated on several datasets, including the CoNLL-2003 NER dataset and the Penn Treebank III dataset, and shows significant improvements in supervised learning tasks when compared to standard feature representations. The results demonstrate the effectiveness of the proposed approach for learning useful representations from large amounts of unlabeled data.",1
1057,"this paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning . we use unsupervised data to generate informative ` condensed feature represen-tations ' from the original feature set used in supervised nlp systems . the main contribution of our method is that it can offer dense and low-dimensional feature spaces for nlp tasks while maintaining the state-of-the-art performance provided by the recently developed high-performance semi-supervised learning technique . our method matches the results of current state-of-the-art systems with very few features , i.e. , f-score 90.72 with 344 features for conll-2003 ner data , and uas 93.55 with 12.5 k features for dependency parsing data derived from ptb-iii .",0
1058,"This paper introduces a new approach to linear modeling that avoids the need for penalties or tears. The traditional approach of penalization-based methods is replaced by a three-step algorithm that uses ordinary least squares fitting, hard thresholding, and ridge regression. The authors demonstrate that their method, which they call ""No penalty no tears,"" is a powerful alternative to traditional approaches, particularly in high-dimensional linear models. They provide numerical exercises to illustrate their technique and compare it to other methods. The generalized version of the method is shown to be particularly effective in situations where the sample size is small and the dimensionality is high. This paper concludes that the ""No penalty no tears"" approach is a valuable tool for researchers seeking to improve their linear modeling capabilities without resorting to penalties or tears.",1
1059,"ordinary least squares -lrb- ordinary least squares -rrb- is the default method for fitting linear models , but is not applicable for problems with dimensionality larger than the sample size . for these problems , we advocate the use of a generalized version of ordinary least squares motivated by ridge regression , and propose two novel three-step algorithms involving least squares fitting and hard thresholding . the three-step algorithms are methodologically simple to understand intuitively , computationally easy to implement efficiently , and theoretically appealing for choosing models consistently . numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed three-step algorithms .",0
1060,"This paper investigates the impact of corpus size on the effectiveness of supervised and unsupervised learning for disambiguation. Specifically, the study focuses on prepositional phrase attachment, relative clause attachment, and attachment decisions using Collins' parser and lexical statistics. The experiments were conducted on an unannotated corpus of newswire text, which was used for unsupervised learning and the supervised component utilized the Wall Street Journal. The results show that combining supervised and unsupervised learning improves disambiguation performance, and larger corpus sizes generally yield better results. The study suggests that corpus size is an important factor to consider when combining supervised and unsupervised learning techniques for disambiguation.",1
1061,"we investigate the effect of corpus size in combining supervised and unsuper-vised learning for two types of attachment decisions : relative clause attachment and prepositional phrase attachment . the supervised component is collins ' parser , trained on the wall street journal . the unsupervised component gathers lexical statistics from an unannotated corpus of newswire text . we find that the combined system only improves the performance of the collins ' parser for small training sets . surprisingly , the size of the unannotated corpus has little effect due to the noisi-ness of the lexical statistics acquired by unsupervised learning .",0
1062,"This paper presents statistical methods for automatic topic segmentation in multimedia archival and retrieval systems using English and Mandarin TDT3 corpora. The paper explores the use of machine learning and statistical natural language processing techniques to perform automatic topic segmentation. The proposed methods are evaluated using a manually segmented corpus and the NIST evaluation metric. The results show that the statistical methods outperform other methods for topic segmentation, and have potential for use in information retrieval techniques.",1
1063,"automatic topic segmentation is an important t e c hnology for multimedia archival and retrieval systems . in this paper we present an algorithm for topic segmentation which uses a combination of machine learning , statistical natural language processing , and information retrieval techniques . the performance of this algorithm is measured by considering the misses and false alarms on a manually segmented corpus . we present our results on the widely used tdt2 and tdt3 corpora provided by nist . most of the techniques described are independent of the source language . we demonstrate this by applying the algorithm on both the english and mandarin tdt3 corpora with only minor changes .",0
1064,"This paper presents the Materials in Context Database, a large-scale, open dataset of materials aimed at enabling real-world material recognition. The dataset contains a rich surface texture and pixel geometry, and was designed to be well-sampled to improve material recognition. Convolutional neural networks (CNN) classifiers were trained on the dataset to recognize materials in full images and patch-based classification. The mean class accuracy was found to be high for simultaneous material recognition of real-world images. The paper also discusses the challenges of material recognition in cluttered scenes and under varying lighting conditions. Overall, the Materials in Context Database provides a valuable resource for recognizing materials in the wild and advancing research in this field.",1
1065,"recognizing materials in real-world images is a challenging task . real-world materials have rich surface texture , geometry , lighting conditions , and clutter , which combine to make the problem particularly difficult . in this paper , we introduce a new , large-scale , open dataset of materials in the wild , the materials in context database , and combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild . materials in context database is an order of magnitude larger than previous material databases , while being more diverse and well-sampled across its 23 categories . using materials in context database , we train convolu-tional neural networks -lrb- cnns -rrb- for two tasks : classifying materials from patches , and simultaneous material recognition and segmentation in full images . for patch-based classification on materials in context database we found that the best performing materials in context database can achieve 85.2 % mean class accuracy . we convert these trained cnn classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field -lrb- crf -rrb- to predict the material at every pixel in an image , achieving 73.1 % mean class accuracy . our experiments demonstrate that having a large , well-sampled dataset such as materials in context database is crucial for real-world material recognition and segmentation .",0
1066,"This paper presents a data-driven approach for estimating the time-frequency binary mask, which is a widely used tool in audio and speech processing for separating speech signals from background noise. The proposed approach uses a localized Bayes risk framework and a decision-directed approach to estimate the binary mask. The approach also considers false alarm rates and uses a sensitivity metric to evaluate the performance. The binary mask estimator is evaluated using priori and posteriori SNR, as well as instantaneous SNR. The proposed data-driven approach outperforms existing approaches and provides improved classification accuracy.",1
1067,"the ideal binary mask , often used in robust speech recognition applications , requires an estimate of the local snr in each time-frequency unit . a data-driven approach is proposed for estimating the instantaneous snr of each t-f unit . by assuming that the a priori snr and a posteriori snr are uniformly distributed within a small region , the instantaneous snr is estimated by minimizing the localized bayes risk . the binary mask estimator derived by the proposed data-driven approach is evaluated in terms of hit and false alarm rates . compared to the binary mask estimator that uses the decision-directed approach to compute the snr , the proposed data-driven approach yielded substantial improvements -lrb- up to 40 % -rrb- in classification performance , when assessed in terms of a sensitivity metric which is based on the difference between the hit and false alarm rates .",0
1068,"This paper proposes a method for extracting and verifying KO-OU expressions from a large-scale electronic corpus of Japanese language text. KO-OU expressions consist of a KO element and an OU element, which are grammatical forms that relate to each other in Japanese language. The method presented in this paper involves analyzing the data and using a Japanese concord to identify and extract the KO-OU expressions. The extracted expressions are then verified based on their grammatical form and KO-OU relation. This approach provides a way to effectively extract and verify KO-OU expressions from large corpora, which can be used in various natural language processing tasks.",1
1069,"in the japanese language , as a predicate is placed at the end of a sentence , the content of a sentence can not be inferred until reaching the end . however , when the content is complicated and the sentence is long , people want to know at an earlier stage in the sentence whether the content is negative , affirmative , or interrogative . in japanese , the grammatical form called the ko-ou relation exists . the ko-ou relation is a kind of concord . if a ko element appears , then an ou element appears in the latter part of a sentence . it is being pointed out that the ko-ou relation gives advance notice to the element that appears in the latter part of a sentence . in this paper , we present the method of extracting automatically the ko-ou expression data from large-scale electronic corpus and verify the usefulness of the ko-ou expression data .",0
1070,"This paper presents an approach for solving the zero-anaphora resolution problem by exploiting syntactic patterns as clues. The zero-anaphora resolution problem arises when a pronoun is omitted in a sentence, and the task is to determine the referent of the pronoun. The proposed method uses rich syntactic pattern features to train a learning-based anaphora resolution model for intra-sentential zero-anaphora resolution. The model is evaluated on a Japanese corpus containing zero-pronouns, and achieves high accuracy. The approach is effective in utilizing syntactic patterns as clues in solving the zero-anaphora resolution problem, and can be extended to inter-sentential zero-anaphora resolution.",1
1071,"we approach the zero-anaphora resolution problem by decomposing zero-anaphora resolution problem into intra-sentential and inter-sentential zero-anaphora resolution . for the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues . taking japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zero-anaphora resolution .",0
1072,"This paper presents an accelerometer-based gesture recognition system that utilizes dynamic time warping (DTW), affinity propagation (AP) algorithms, and compressive sensing to achieve high user-independent recognition accuracy. The system employs a 3-axis accelerometer and a dictionary of gestures to recognize acceleration-based gestures. The recognition process uses DTW to measure the similarity between the input gesture and the dictionary of gestures, followed by AP clustering to group similar gestures and reduce noise. Compressive sensing is then applied to further enhance the recognition accuracy. The system is compared to existing statistical methods and is shown to achieve better user-independent recognition accuracy, while also supporting user-dependent recognition. The proposed approach demonstrates promising results based on published studies, showing the potential for practical applications in gesture sequence recognition.",1
1073,"we propose a gesture recognition system based primarily on a single 3-axis accelerometer . the gesture recognition system employs dynamic time warping and affinity propagation algorithms for training and utilizes the sparse nature of the gesture sequence by implementing compressive sensing for gesture recognition . a dictionary of 18 gestures or classes is defined and a database of over 3,700 repetitions is created from 7 users . our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition , to the best of our knowledge . the proposed gesture recognition system achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature .",0
1074,"This paper proposes an optimal pricing strategy for improving the efficiency of taxi systems. The study focuses on the problem of taxi drivers' congestion costs and their profit-driven decisions, which are affected by the taxi fare structure and scheduling constraints. The authors introduce the atom schedule method, which provides a strategy space for taxi drivers to make optimal decisions based on market variance. They argue that the traditional pure strategies for pricing and scheduling are infeasible due to computational intensiveness. Instead, they propose an optimal pricing scheme that takes into account the taxi drivers' scheduling constraints and the market variance. The study shows that the proposed approach can lead to significant improvements in the efficiency of taxi systems, with a higher profit for taxi drivers and lower congestion costs for passengers.",1
1075,"in beijing , most taxi drivers intentionally avoid working during peak hours despite of the huge customer demand within these peak periods . this dilemma is mainly due to the fact that taxi drivers ' congestion costs are not reflected in the current taxi fare structure . to resolve this problem , we propose a new atom schedule method to provide taxi drivers with extra incentives to work during peak hours . this differs from previous studies of taxi market by considering market variance over multiple periods , taxi drivers ' profit-driven decisions , and their scheduling constraints regarding the interdependence among different periods . the major challenge of this research is the computational inten-siveness to identify optimal strategy due to the exponentially large size of a taxi driver 's strategy space and the scheduling constraints . we develop an atom schedule method to overcome these issues . it reduces the magnitude of the problem while satisfying the constraints to filter out infeasible pure strategies . simulation results based on real data show the effectiveness of the proposed atom schedule method , which opens up a new door to improving the efficiency of taxi market in megacities -lrb- e.g. , beijing -rrb- .",0
1076,"This paper presents Radiobot-CFF, a spoken dialogue system designed for military training that allows soldiers to practice requesting artillery fire missions via call for fire radio dialogues. The system is built using an information-state dialogue manager that maintains a representation of the current state of the conversation, and interactive information components that allow the user to provide and receive information from the system. The paper discusses the design and implementation of the system, as well as the evaluation results which show that Radiobot-CFF is effective in training soldiers to perform call for fire radio dialogues. The system's ability to generate realistic and complex dialogues, along with its capacity to respond to incomplete and ambiguous information, are highlighted as its key strengths.",1
1077,"we describe a spoken dialogue system which can engage in call for fire radio dialogues to help train soldiers in proper procedures for requesting artillery fire missions . we describe the domain , an information-state dialogue manager with a novel system of interactive information components , and provide evaluation results .",0
1078,This paper presents a study of sibilant fricatives production in post-glossectomy speakers using cine magnetic resonance imaging (MRI). The goal of this study is to provide insight into the precise tongue control and air flow mechanisms used in the production of sibilant fricatives. The study includes both acoustic and articulatory data and uses dimensional vocal tract reconstructions to analyze missing unilateral tongue tissue. The results show that post-glossectomy speakers use an air flow bypass to compensate for missing tongue tissue and are able to achieve primary closure and constriction locations similar to those of non-surgical speakers. This study highlights the potential of cine MRI as a tool for investigating speech production in individuals with structural abnormalities.,1
1079,"post-glossectomy patients -lrb- t2 tumors resected with primary closure , the red lines indicate the midline of the tongues -rrb- abstract glossectomy changes properties of the tongue and negatively affects patients ' speech production . among the most difficult consonants to produce in the post-glossectomy speakers , the sibilant fricatives / s / and / sh / are often problematic . to better understand these problems in production , this study analyzed acoustic and articulatory data of / s / and / sh / from three subjects : one normal speaker and two post-glossectomy speakers with abnormal / s / or / sh . based on cine magnetic resonance images , three dimensional vocal tract reconstructions , tongue surface shapes behind constrictions , and area functions were analyzed . our results show that in each patient , contrary to normal , / s / and / sh / were quite similar in acoustic spectra , tongue surface shapes , and constriction locations . in the abnormal / s / , the missing unilateral tongue tissue created an air flow bypass which made the constriction further backward . the abnormal / sh / may be explained by the lack of precise tongue control after surgery . in addition , the tongue surfaces in the patients were more asymmetric in the back and were not grooved for / s / anterior to the constriction .",0
1080,"This paper presents the Java Visual Speech Components (JVSC) framework, which provides a set of standard components for rapid development of graphical user interface (GUI) based speech processing applications. The framework includes components for audio recording and playback, speech signal analysis, transcription and spectrogram visualization. The framework is built on an object-oriented design and allows for easy integration of external pitch values. The power plot component is introduced as a novel tool for visualizing the energy distribution of speech signals in real time. The standard components are fully customizable and extendable, providing flexibility for developers to tailor the components to their specific needs. The paper demonstrates the use of JVSC in developing a GUI application for speech signal processing, with examples of speech file analysis, transcription and spectrogram display.",1
1081,"in this paper , we describe a new java framework for an easy and efficient way of developing new gui based speech processing applications . standard components are provided to display the speech signal , the power plot , and the spectrogram . furthermore , a component to create a new transcription and to display and manipulate an existing transcription is provided , as well as a component to display and manually correct external pitch values . these standard components can be easily embedded into own java programs . standard components can be synchronized to display the same region of the speech file . the object-oriented design provides base classes for rapid development of own components .",0
1082,"This paper proposes a method for network topology discovery using finite mixture models. The proposed approach involves the use of unicast end-to-end packet pair delay measurements and unsupervised learning algorithms to estimate the network topology. A hierarchical topology construction algorithm is used to construct the network tree from the estimated topology. The network tree consists of leaf nodes that represent the delay co-variances between leaf pairs, and mixture components that represent the mixture models of the delay distributions. The proposed approach uses the map criterion to estimate the optimal number of mixture components. The experimental results demonstrate the effectiveness of the proposed approach in discovering the network topology.",1
1083,in this article we propose a network topology estimation strategy using unicast end-to-end packet pair delay measurements that is based on mixture models for the delay co-variances . an unsupervised learning algorithms is applied to estimate the number of mixture components and delay covariances . the leaf pairs are clustered by a map criterion and passed to a hierarchical topology construction algorithm to rebuild the tree . results from an ns simulation show that our network topology estimation strategy can identify a network tree with 8 leaf nodes .,0
1084,"This paper proposes a novel approach for endpoint detection in noisy environments using a Poincare recurrence metric. The method is designed to address the challenges of nonstationary and transient time series, and can detect state transitions using the recurrence point variability algorithm. The proposed approach utilizes the fractal structure of the recurrence points and recurrence time statistics to identify the stationarity change in the time series. The Poincare recurrence metric is used to detect endpoints in low signal-to-noise ratio speech signals. The experimental results demonstrate the effectiveness of the proposed method in detecting endpoints in noisy environments, and show that it outperforms existing methods in terms of accuracy and robustness. The proposed method has potential applications in speech recognition and other fields that require endpoint detection in noisy environments.",1
1085,"speech speech recognition continues to be a challenging problem particularly for speech recognition in noisy environments . in this paper , we address this problem from the point of view of fractals and chaos . by studying recurrence time statistics for chaotic systems , we find the nonstationarity and transience in a time series are due to non-recurrence and lack of fractal structure in the signal . a poincaré recurrence metric is designed to determine the stationarity change for speech recognition . we consider the small area of beginning and ending of an utterance as transient . for nonstationary and transient time series , we expect the average number of poincaré recurrence points for each given small block will be different for different blocks of data subsets . however , the average number of recurrence points will stay nearly constant . the resulting recurrence point variability algorithm is shown to be well suited for the detection of state transitions in a time series and is very robust for different types of noise , especially for low snr .",0
1086,"This paper proposes a method for solving Distributed Constraint Optimization Problems (DCOP) using Logic Programming. DCOP is a class of problems where a group of agents work together to optimize a common objective function subject to constraints. Memory limitations make traditional DCOP algorithms infeasible for large-scale problems. The proposed method uses Answer Set Programming (ASP) to solve DCOPs, which can handle complex constraints and preferences. The paper also introduces a new algorithm called ASP-DPOP, which combines ASP with the popular DCOP algorithm DPOP. Experimental results demonstrate the effectiveness of the proposed method in solving large-scale DCOP problems with multiple agents.",1
1087,"this paper explores the use of answer set programming in solving distributed constraint optimization problems . it makes the following contributions : -lrb- i -rrb- it shows how one can formulate answer set programming as logic programs ; -lrb- ii -rrb- it introduces asp-dpop , the first dcop algorithm that is based on logic programming ; -lrb- iii -rrb- it experimentally shows that asp-dpop can be up to two orders of magnitude faster than answer set programming -lrb- its imperative-programming counterpart -rrb- as well as solve some problems that answer set programming fails to solve due to memory limitations ; and -lrb- iv -rrb- it demonstrates the applicability of answer set programming in the wide array of multi-agent problems currently modeled as answer set programming .",0
1088,"This paper introduces the concept of boosting decision trees to enhance the accuracy of data mining problems. Decision trees have been widely used in knowledge discovery due to their simplicity and interpretability. However, decision trees have limitations in handling complex data and suffer from overfitting issues. Boosting decision trees aim to improve their performance by combining multiple weak decision trees to create a strong ensemble. The paper discusses the boosting algorithm and its implementation using Quinlan's C4.5 algorithm. The boosting algorithm uses the information ratio criterion to generate probability distribution and improve the accuracy of decision trees. The paper concludes that boosting decision trees can be an effective method for solving data mining problems.",1
1089,"a new boosting algorithm of freund and schapire is used to improve the performance of decision trees which are constructed usin : the information ratio criterion of quinlan 's c4 .5 algorithm . this boosting algorithm iteratively constructs a series of decision trees , each decision tree being trained and pruned on examples that have been filtered by previously trained trees . examples that have been incorrectly classified by the previous trees in the ensemble are resampled with higher probability to give a new probability distribution for the next ace in the ensemble to tnin on . results from optical cha-xc : er reco ~ tion -lrb- ocr -rrb- , and knowledge discovery and data mining problems show that in comparison to single trees , or to trees trained independenrly _ or to trees trained on subsets of the feature space , the boosring ensemble is much better .",0
1090,"This paper proposes a subspace co-regularized multi-view learning method for cross language text classification tasks. Cross language text classification is a challenging problem in which a classifier must learn to classify documents written in a language different from the language in which it was trained. The proposed method uses parallel corpora and machine translation to build a subspace that captures the shared structure of the different languages. The method also incorporates domain adaptation methods and label knowledge to improve classification accuracy. The effectiveness of the proposed method is evaluated on multilingual text classification problems and compared with inductive methods. Results show that the proposed method outperforms other state-of-the-art multi-view learning methods, and it is effective for cross language text classification.",1
1091,"in many multilingual text classification problems , the documents in different languages often share the same set of categories . to reduce the labeling cost of training a classification model for each individual language , it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification . in this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification . this subspace co-regularized multi-view learning method is built on parallel corpora produced by machine translation . subspace co-regularized multi-view learning method jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents . our empirical study on a large set of cross language text classification tasks shows the proposed subspace co-regularized multi-view learning method consistently outperforms a number of inductive methods , domain adaptation methods , and multi-view learning methods .",0
1092,"This paper proposes a semi-supervised dimensionality reduction method for label propagation on facial images that have multiple representations. The method incorporates state-of-the-art techniques such as must-link and cannot-link constraints, priori pairwise information, and linear combination of data representations to learn a projection matrix. The proposed method uses locality information and label information to propagate labels on facial images. The approach is tested on stereo movies and achieves promising results compared to other state-of-the-art methods.",1
1093,"in this paper a novel method is introduced for semi-supervised dimensionality reduction on facial images extracted from stereo videos . it operates on image data with multiple representations and calculates a projection matrix that preserves locality information and a priori pairwise information , in the form of must-link and cannot-link constraints between the various data representations , as well as label information for a percentage of the data . the final data representations is a linear combination of the projections of all data representations . the performance of the proposed semi-supervised multiple locality preserving projections method was evaluated in person identity label propagation on facial images extracted from stereo movies . experimental results showed that the proposed method outperforms state of the art methods .",0
1094,"This paper proposes a deep learning approach to unsupervised ensemble learning using Restricted Boltzmann Machines (RBMs) and deep neural networks. RBMs are used to model the probability distribution over a set of observed and hidden variables. The authors show that RBM-based models can be used for unsupervised ensemble learning on both simulated and real-world datasets. The paper also explores the use of posterior probabilities and hidden node classifiers to improve the performance of the ensemble models. Additionally, the authors investigate the use of crowdsourcing to label training data for unsupervised learning tasks. Overall, the results suggest that deep learning methods can be an effective approach for unsupervised ensemble learning.",1
1095,"we show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning . first , we prove that the popular model of dawid and skene , which assumes that all classifiers are conditionally independent , is equivalent to a restricted boltzmann machine with a single hidden node . hence , under this model , the posterior probabilities of the true labels can be instead estimated via a trained restricted boltzmann machine . next , to address the more general case , where classifiers may strongly violate the conditional independence assumption , we propose to apply rbm-based deep neural net . experimental results on various simulated and real-world datasets demonstrate that our proposed dnn approach outperforms other state-of-the-art methods , in particular when the data violates the conditional independence assumption .",0
1096,"This paper proposes a variable projection technique for sparse seismic imaging, where the signal of interest is separated from the sparse noise through sparsity-promoting optimization. The method uses sparse signal and auxiliary parameters to deconvolve large-scale seismic data and reconstruct the sparse Green's function. The approach is demonstrated using simulated and real-world seismic experimental data and shows improved results compared to existing deconvolution techniques. The paper emphasizes the usefulness of incorporating auxiliary information such as the source signature into the sparsity optimization process. The results indicate that the variable projection technique is a promising approach for sparse signal processing problems.",1
1097,"we consider an important class of signal processing problems where the signal of interest is known to be sparse , and can be recovered from data given auxiliary information about how this data was generated . for example , a sparse green 's function may be recovered from seismic experimental data using sparsity optimization when the source signature is known . unfortunately , in practice this information is often missing , and must be recovered from data along with the signal using deconvolution techniques . in this paper , we present a novel methodology to simultaneously solve for the sparse signal and auxiliary parameters using a recently proposed variable projection technique . our main contribution is to combine variable projection with spar-sity promoting optimization , obtaining an efficient algorithm for large-scale sparse deconvolution problems . we demonstrate the algorithm on a seismic imaging example .",0
1098,"The paper presents a novel method for signal recovery in shift-invariant spaces from partial frequency data. Specifically, the focus is on reconstructing the missing frequency content of signals that are band-limited and shift-invariant. The paper highlights the reconstruction ability of the proposed method and demonstrates its effectiveness in comparison to existing techniques. Additionally, the paper discusses the importance of pre-processing techniques in improving the reconstruction accuracy. Overall, the results suggest that the proposed method is a promising approach for signal recovery in shift-invariant spaces, particularly when the signal bandwidth is limited and only partial frequency data is available.",1
1099,"this paper studies conditions under which a signal can be reconstructed from partial frequency content . we focus on signals in shift-invariant spaces generated by multiple generators . for these signals , we derive a lower bound on the necessary signal bandwidth as well as sufficient conditions on the generators such that signal recovery is possible . when the available frequency content is not sufficient to recover the signal , we propose appropriate pre-processing that can improve the reconstruction ability .",0
1100,"This paper presents a waiting cycle analysis of an H.246 decoder running on the PAC Duo platform in multi-core scenarios. The study investigates inter-core synchronization and resource contention issues that affect execution speed. Dual-core decoders are particularly prone to cache misses, which can be mitigated by data and function partition. The analysis shows that function partitioning is a crucial factor in improving the performance of the dual-core decoder, and that inter-core synchronization and resource contention must be carefully managed to avoid delays. The study provides valuable insights into the design of multi-core systems and highlights the importance of data partitioning, function partitioning, and inter-core synchronization in improving system performance.",1
1101,"two approaches for parallelization of h. 264 decoder , data partition and function partition , are realized on a pac duo platform , which contains two parallel architecture core digital signal processors -lrb- pacdsp 's -rrb- . eight baseline cif sequences are decoded and their execution cycles and waiting cycles are examined . there are three roots hindering the performance of dual-core decoders : inter-core synchronization , resource contention , and cache miss . through the waiting cycle analysis , the major reasons causing the degradation of dual core h. 246 decoders are found . the inter core synchronization and resource contention principally slow down the execution speed of the dual core with function partition and dual core data partition , respectively . the precious experience and analysis will help the software and hardware designers explore the mechanisms to improve performance of the multi core scenarios .",0
1102,"This paper proposes an asymmetric sparse kernel partial least squares classifier for automatic intelligibility detection of pathological speech. The intelligibility detection of pathological speech is important as it can help to diagnose voice and/or voice disorders, which may be symptoms of various diseases or illnesses. The paper discusses the use of un-weighted accuracy as a metric for evaluating the proposed classifier's performance. The study highlights the need to address pathological speech intelligibility detection through effective feature extraction and classification techniques, taking into account the underlying articulatory mechanisms that cause physical problems in speech production. Furthermore, the paper suggests that unhealthy social behavior can contribute to voice abuse and, therefore, affect speech intelligibility. Overall, the proposed method shows promising results for detecting intelligibility in pathological speech.",1
1103,"pathological speech usually refers to the voice disorders resulting from atypicalities in voice and/or in the articulatory mechanisms due to disease , illness or other physical problem in the speech production system . it may increase unhealthy social behavior and voice abuse , and dramatically affect the patients ' quality of life . therefore , automatic intel-ligibility detection of pathological speech has an important role in the opportune treatment of pathological voices . this paper proposes to use asymmetric sparse kernel partial least squares classifier -lrb- askplsc -rrb- for intelligibility detection of pathological speech . the proposed approach achieves an un-weighted accuracy of 74.0 % , which is 7.34 % relative improvement of baseline system of an un-weighted accuracy of 68.90 % for the pathology sub-challenge of interspeech 2012 speaker trait challenge .",0
1104,"This paper proposes a method for dialog act tagging using both text and acoustic features. The approach combines sparse high-dimensional text features and dense low-dimensional acoustic features to train linear support vector machines and hidden Markov models for sequence labeling. The HCRC MapTask corpus is used as the dataset for evaluation, and the posterior probabilities generated by the models are compared. The results show that the proposed method outperforms other state-of-the-art approaches for dialog act tagging. The combination of text and acoustic features along with the use of both SVMs and HMMs prove to be effective for this task.",1
1105,"we use a combination of linear support vector machines and hidden markov models for dialog act tagging in the hcrc maptask corpus , and obtain better results than those previously reported . support vector machines allow easy integration of sparse high-dimensional text features and dense low-dimensional acoustic features , and produce posterior probabilities usable by sequence labelling algorithms . the relative contribution of text and acoustic features for each class of dialog act is analyzed .",0
1106,"This paper presents a method for efficient classification and detection in computer vision using sparse kernel approximations. The proposed method leverages low-dimensional and dense features and high-dimensional and sparse ones, and uses bundle optimization methods to optimize the arbitrary additive kernels. The approach is applied to the PASCAL VOC dataset, and experiments show that it outperforms previous methods in terms of memory use and classification accuracy. The method also incorporates sparse feature encoding, Fisher kernels, and non-linear kernels to improve image classification and object detection. The paper concludes that the proposed method is effective for learning and using sparse features with arbitrary kernels, and is a promising direction for future research in computer vision.",1
1107,"efficient learning with non-linear kernels is often based on extracting features from the data that '' linearise '' the kernel . while most constructions aim at obtaining low-dimensional and dense features , in this work we explore high-dimensional and sparse ones . we give a method to compute sparse features for arbitrary kernels , re-deriving as a special case a popular map for the intersection kernel and extending it to arbitrary additive kernels . we show that bundle optimisation methods can handle efficiently these sparse features in learning . as an application , we show that product quantisation can be interpreted as a sparse feature encoding , and use this to significantly accelerate learning with this technique . we demonstrate these ideas on image classification with fisher kernels and object detection with deformable part models on the challenging pascal voc data , obtaining five to tenfold speed-ups as well as reducing memory use by an order of magnitude .",0
1108,This paper proposes a game theoretic approach for jointly optimizing linear precoders and selecting base stations in an uplink MIMO network. The authors formulate a weighted sum rate optimization problem and model the interactions between the base stations as a noncooperative game. They show that the game admits a stationary solution and develop a distributed algorithm to find it. The proposed approach achieves user fairness and alleviates base station congestion in congested scenarios. The performance of the algorithm is evaluated through simulations using a realistic system model.,1
1109,"we consider the weighted sum rate optimization of weighted sum rate optimization in a mimo interfering multiple access channel . we propose to jointly optimize the users ' linear procoders as well as their base station -lrb- bs -rrb- associations . this approach enables the users to avoid congested bss and can improve system performance as well as user fairness . we formulate the weighted sum rate optimization into a noncooperative game , and develop an algorithm that allows the players to distributedly reach the nash equilibrium of the game . we show that every ne of the game is a stationary solution of the weighted sum rate optimization problem , and propose an algorithm to compute the ne of the game . simulation results show that the proposed algorithm performs well in the presence of bs congestion .",0
1110,"This paper presents a method for elaborating object descriptions through examples in instructional or explanatory contexts. The authors propose a framework for generating elaborated descriptions of objects by using example sentences. They demonstrate that this framework can produce richer and more informative object descriptions, improving the overall understanding of the object being described. The approach is evaluated using a dataset of object descriptions and example sentences, and the results show that the proposed method outperforms several baseline approaches. The findings suggest that using example sentences can help to provide more detailed and accurate descriptions of objects in instructional or explanatory contexts.",1
1111,"examples are often used along with textual descriptions to help convey particular ideas-especially in instructional or explanatory contexts . these accompanying examples reflect information in the surrounding text , and in turn , also influence the text . sometimes , examples replace possible -lrb- textual -rrb- elaborations in the description . it is thus clear that if object descriptions are to be generated , the system must incorporate strategies to handle examples . in this work , we shall investigate some of these issues in the generation of object descriptions .",0
1112,"This paper proposes a method for enabling scalable stochastic gradient-based inference for Gaussian processes using the Unbiased LInear System SolvEr (ULISSE). The stochastic gradient langevin dynamics algorithm is used for unbiased estimation of gradients, and parallelizable covariance matrix-vector products are used to compute posterior distributions. Gaussian process regression is used for quantification of uncertainty and estimation of marginal likelihood. The ULISSE algorithm enables efficient computation of covariance parameters with negligible bias and enables scalable inference for large datasets. The results show that the proposed method outperforms existing methods in terms of computation time and accuracy.",1
1113,"in applications of gaussian processes where quantification of uncertainty is of primary interest , it is necessary to accurately characterize the posterior distribution over covariance parameters . this paper proposes an adaptation of the stochastic gradient langevin dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood . in gaussian process regression , this has the enormous advantage that stochastic gradients can be computed by solving linear systems only . a novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients . the results demonstrate the possibility to enable scal-able and exact -lrb- in a monte carlo sense -rrb- quantifi-cation of uncertainty in gaussian processes without imposing any special structure on the covari-ance or reducing the number of input vectors .",0
1114,"This paper investigates the expressive completeness of existential rule languages for ontology-based query answering. The authors focus on a class of disjunctive tuple-generating dependencies and a class of embedded dependencies, called disjunctive embedded dependencies, which have been shown to be useful in modeling certain types of ontologies. The paper provides a semantic definition of ontology-based query answering and shows that the expressive completeness of a language can be characterized by its ability to express all queries that can be answered using this semantic definition. The authors prove that a language based on built-in linear order and existential rules is expressively complete for ontology-based conjunctive query answering, and present a sound and complete algorithm for answering such queries. The results are important for the design of ontology languages and for the development of efficient algorithms for ontology-based query answering in large-scale databases with complex data dependencies.",1
1115,"existential rules , also known as data dependencies in databases , have been recently rediscovered as a promising family of languages for ontology-based query answering . in this paper , we prove that disjunctive embedded dependencies exactly capture the class of recursively enumerable ontologies in ontology-based conjunctive query answering . our expressive completeness result does not rely on any built-in linear order on the database . to establish the expressive completeness , we introduce a novel semantic definition for ontology-based conjunctive query answering . we also show that neither the class of disjunc-tive tuple-generating dependencies nor the class of embedded dependencies is expressively complete for recursively enumerable ocqa ontologies .",0
1116,"This paper discusses the use of array processing techniques and shape reconstruction in tomography. Tomography is a mathematical inversion problem that aims to reconstruct an object's internal structure from external measurements. This paper explores the application of classical antenna array processing techniques and a Bayesian estimation approach to solve the inverse problem and reconstruct the object's shape. The proposed methods are evaluated on X-ray tomography data and compared with existing methods. The paper also introduces the concept of a compact object, which is represented by a contour polygon. Overall, the paper presents promising results for the use of array processing techniques and shape reconstruction in tomography.",1
1117,"we consider the problem of the shape reconstruction of a compact object in x ray tomography when the contour of the object is modeled by a polygon . the problem is then to estimate the vertices of that polygon from a limited number of projections . the main objectives of this paper are : -lcb- to show how this shape reconstruction becomes equivalent to a generic mathematical inversion problem which arises also in linear antenna array processing ; -lcb- to evaluate the performances of the classical ap techniques to handle with this shape reconstruction , and , -lcb- to propose a new method based on bayesian estimation approach for the resolution of this inverse problem .",0
1118,"This paper presents a method for generating 3D scenes from natural language descriptions using rich lexical grounding. The goal is to bridge the gap between manually specified object categories and natural language descriptions in the task of 3D scene generation. The proposed approach uses 3D geometric representations and a grounding approach that links textual descriptions to physical objects through concrete referents and lexical terms. The method is evaluated using human judgments and compared to rule-based methods. The results show that the proposed approach outperforms other methods in terms of generating more accurate and realistic 3D scenes. This work has implications for the fields of art, robotics, and education.",1
1119,"the ability to map descriptions of scenes to 3d geometric representations has many applications in areas such as art , education , and robotics . however , prior work on the text to 3d scene generation task has used manually specified object categories and language that identifies them . we introduce a dataset of 3d scenes annotated with natural language descriptions and learn from this data how to ground tex-tual descriptions to physical objects . our method successfully grounds a variety of lexical terms to concrete referents , and we show quantitatively that our method improves 3d scene generation over previous work using purely rule-based methods . we evaluate the fidelity and plau-sibility of 3d scenes generated with our grounding approach through human judgments . to ease evaluation on this task , we also introduce an grounding approach that strongly correlates with human judgments .",0
1120,"This paper proposes a probabilistic graphical model for layered image motion estimation that incorporates explicit occlusions, temporal consistency, and depth ordering. The model uses a spatial prior for robust segmentation of meaningful scene regions and an image-dependent hidden field prior to detect occlusion regions. The layered model is compared to non-layered methods using the Middlebury benchmark and is shown to achieve high accuracy for optical flow in natural scenes. The paper also presents a parametric model for smooth deviation in optical flow estimation to handle occlusions and disocclusions. Overall, the proposed approach provides a robust framework for layered image motion estimation with explicit occlusions and temporal consistency.",1
1121,"layered layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other . for image motion estimation , such layered models have a long history but have not achieved the wide use or accuracy of non-layered methods . we present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches . in particular , we define a probabilistic graphical model that explicitly captures : 1 -rrb- occlusions and disocclusions ; 2 -rrb- depth ordering of the layers ; 3 -rrb- temporal consistency of the layer segmentation . additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an mrf with a robust spatial prior ; the resulting probabilistic graphical model allows roughness in layers . finally , a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent layered models for static scene segmentation . the probabilistic graphical model achieves state-of-the-art results on the middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions .",0
1122,"This paper proposes a feature study for classification-based speech separation under very low signal-to-noise ratios (SNRs). The study investigates the robustness of multi-resolution cochleagram features for speech separation under SNR level of -5 dB, a condition where speech recognition is challenging. A neural network classifier is trained to solve the classification problem of separating speech from background noise. A post-processing technique is applied to refine the separation result. The study shows that the proposed approach outperforms monaural features and other state-of-the-art methods in terms of separation accuracy and robustness to non-stationary noises. The proposed method is promising for speech separation in real-world scenarios where low SNRs and complex noise environments are common.",1
1123,"speech separation is a challenging problem at low signal-to-noise ratios . separation can be formulated as a classification problem . in this study , we focus on the snr level of-5 db in which speech is generally dominated by background noise . in such a low snr condition , extracting robust features from a noisy mixture is crucial for successful classification . using a common neural network classifier , we systematically compare separation performance of many monaural features . in addition , we propose a new multi-resolution cochleagram called multi-resolution cochleagram , which is extracted from four cochlea-grams of different resolutions to capture both local information and spectrotemporal context . comparisons using two non-stationary noises show a range of feature robustness for speech separation with the proposed multi-resolution cochleagram performing the best . we also find that multi-resolution cochleagram , a post-processing technique previously used for robust speech recognition , improves speech separation performance by smoothing the temporal trajectories of feature dimensions .",0
1124,"This paper proposes a method for recognizing handwritten digits using mixtures of linear models. The approach involves representing the pixel-based images of digits as locally linear generative models and estimating the sample covariance matrices of the tangent vectors. The proposed EM-based algorithm then computes the mixture coefficients and the model parameters in an iterative manner. The method is shown to be effective in dealing with local deformations in the digits, and achieves high recognition accuracy on benchmark datasets. The use of locally linear generative models and tangent vectors is found to be particularly effective for capturing the underlying structure of the data. Overall, the results demonstrate the effectiveness of the proposed method for recognizing handwritten digits.",1
1125,"we construct a mixture of locally linear generative models of a collection of pixel-based images of digits , and use locally linear generative models for recognition . different models of a given digit are used to capture different styles of writing , and new images are classified by evaluating their log-likelihoods under each locally linear generative models . we use an em-based algorithm in which the m-step is computationally straightforward principal components analysis -lrb- m-step -rrb- . incorporating tangent-plane information -lsb- 12 -rsb- about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the m-step , and it demonstrably improves performance .",0
1126,"This paper compares Context-Free Grammar (CFG) filtering techniques for two different grammar formalisms, Lexicalized Tree Adjoining Grammar (LTAG) and Head-driven Phrase Structure Grammar (HPSG). CFG filtering is a process that restricts the set of trees generated by the grammar, which can lead to more efficient parsing. The paper explores various CFG filtering techniques and evaluates their effectiveness on LTAG and HPSG. The results show that some techniques are more effective than others and that the effectiveness can vary depending on the grammar formalism. The paper concludes with recommendations for which filtering techniques to use for each formalism.",1
1127,an empirical comparison of cfg filtering techniques for ltag and hpsg is presented . we demonstrate that an approximation of hpsg produces a more effective cfg filter than that of ltag . we also investigate the reason for that difference .,0
1128,"This paper describes experiments in mining named entity transliteration pairs from comparable corpora, with a focus on Indian language transliteration pairs between Tamil and English. The authors use linguistic tools to align articles in comparable corpora and extract parallel named entity pairs. They then develop a mining methodology to identify transliteration pairs using a well-trained linear classifier. The effectiveness of the methodology is demonstrated through experiments on the extracted named entity pairs. The results show that the proposed methodology outperforms baseline methods and can be applied to other languages and tasks in natural language processing.",1
1129,"parallel named entity parallel named entity pairs are important resources in several nlp tasks , such as , clir and mt systems . further , such parallel named entity pairs may also be used for training transliteration systems , if parallel named entity pairs are transliterations of each other . in this paper , we profile the performance of a mining methodology in mining parallel named entity transliteration pairs in english and an indian language , tamil , leveraging linguistic tools in english , and article-aligned comparable corpora in the two languages . we adopt a mining methodology parallel to that of -lsb- klementiev and roth , 2006 -rsb- , but we focus instead on mining parallel named entity transliteration pairs , using a well-trained linear classifier to identify transliteration pairs . we profile the performance at several operating parameters of our mining methodology and present the results that show the potential of the mining methodology in mining transliterations pairs ; in addition , we uncover a host of issues that need to be resolved , for effective mining of parallel named entity transliteration pairs .",0
1130,"The paper proposes a computationally-efficient method called Fastmap for estimating unknown parameters in Bayesian estimation problems. The Fastmap approach is an approximate maximum a posteriori probability parameter estimator that reduces the number of computationally-intensive integrations required in the estimation process. The paper demonstrates the effectiveness of Fastmap for source localization in matched-field processing, where nuisance parameters can cause significant computational challenges for parameter estimation. The proposed approach provides a fast and accurate solution to parameter estimation problems that involve unknown parameters and nuisance parameters.",1
1131,"in many estimation problems , the set of unknown parameters can be divided into a subset of desired parameters and a subset of nuisance parameters . using a maximum a pos-teriori approach to parameter estimation , these nuisance parameters are integrated out in the estimation process . this can result in an extremely computationally-intensive esti-mator . this paper proposes a method by which computationally-intensive i n tegrations over the nuisance parameters required in bayesian estimation may be avoided under certain conditions . the propsed method is an approximate map estimator which is much more compu-tationally ecient than direct , or even monte carlo , integration of the joint posteriori distribution of the desired and nuisance parameters . as an example of its eciency , we apply the fast algorithm to matched-eld source localiza-tion in an uncertain environment .",0
1132,"This paper presents a closed-loop feedback cancellation approach for hearing aids, utilizing two microphones and transform domain processing. The proposed method employs discrete cosine transform (DCT) to estimate the incoming signal and undesired signal correlation, and uses an adaptive filter to cancel the acoustic feedback. The approach also incorporates orthogonal transforms to improve convergence rates and reduce the adaptation process complexity. Real measured feedback paths are used to evaluate the performance of the proposed method, and the results show effective feedback cancellation and improved speech signal quality.",1
1133,"in this paper we are studying the use of two microphones for acoustic feedback cancellation in hearing aids . with the two microphones approach , an additional microphone is employed to provide added information about the signals which is then utilized to obtain an incoming signal estimate . this incoming signal estimate is removed from the error signal prior to adapting the canceler , thus removing the undesired signal correlation . in this paper , we propose to use orthogonal transforms with the two microphones approach . the discrete fourier transform and the discrete cosine transform are implemented to transform the adaptive filter signals . also , a bank of adaptive filters is employed , each adapting to different portions of the spectrum for a finer control of the adaptation process . simulation results based on real measured feedback paths and speech signals show improved convergence rates and stable solutions .",0
1134,"This paper proposes a novel approach for goal recognition design in the presence of stochastic action outcomes of an agent. The approach is based on Markov decision process-based algorithms and worst-case distinctiveness measure to address real-world problems such as wheel slippage. The proposed method utilizes the stochasticity of the action outcomes to enhance the performance of the goal recognition design problems. The paper presents a comprehensive analysis of the proposed approach and its effectiveness in tackling stochastic GRD problems. The results show that the proposed method outperforms the existing methods in terms of accuracy and efficiency. The proposed approach has wide-ranging applications in various domains, including robotics, autonomous systems, and decision-making processes.",1
1135,"goal recognition design -lrb- grd -rrb- problems involve identifying the best ways to modify the underlying environment that the agents operate in , typically by making a subset of feasible actions infeasible , in such a way that agents are forced to reveal their goals as early as possible . thus far , existing work assumes that the outcomes of the actions of the agents are deterministic , which might be unrealistic in real-world problems . for example , wheel slippage in robots cause the outcomes of their movements to be stochastic . in this paper , we generalize the goal recognition design problems to stochastic grd problems , which handle stochastic action outcomes . we also generalize the worst-case distinctiveness measure , which measures the goodness of a solution , to take stochastic-ity into account . finally , we introduce markov decision process based algorithms to compute the wcd and minimize markov decision process based algorithms by making up to k actions infeasible .",0
1136,"This paper presents a novel algorithm, SDG Cut, for the 3D reconstruction of non-lambertian objects using iterative graph cuts based on surface distance grids. The algorithm utilizes a signed distance transform and a volumetric graph cut to handle non-lambertian photo-consistency measures. The algorithm also incorporates a minimal surface bias and surface extrusions cost function to improve the 3D reconstruction. The paper evaluates the effectiveness of the algorithm for the 3D reconstruction of non-lambertian objects and shows that the surface distance grid is an effective tool for the iterative graph cuts-based algorithm. Additionally, the paper highlights the importance of minimizing both surface and discretization biases in the 3D reconstruction process.",1
1137,"we show that the approaches to 3d reconstruction that use volumetric graph cuts to minimize a cost function over the object surface have two types of biases , the minimal surface bias and the discretization bias . these biases make it difficult to recover surface extrusions and other details , especially when a non-lambertian photo-consistency measure is used . to reduce these biases , we propose a new iterative graph cuts based algorithm that operates on the surface distance grid , which is a special discretization of the 3d space , constructed using a signed distance transform of the current surface estimate . it can be shown that surface distance grid significantly reduces the minimal surface bias , and transforms the discretization bias into a controllable degree of surface smoothness . experiments on 3d reconstruction of non-lambertian objects confirm the effectiveness of our iterative graph cuts based algorithm over previous methods .",0
1138,"This paper presents Co-Simmate, a method for quickly retrieving all pairwise co-simrank scores for a given graph structure. Co-simrank is a simrank-like measure of similarity between nodes in a graph, based on matrix decomposition. The method utilizes the singular graphs and pagerank vectors to efficiently calculate the co-simrank score for all pairs of nodes in the graph. The proposed approach can be used for a variety of tasks that require pairwise node similarity scores. Experimental results demonstrate the effectiveness and efficiency of Co-Simmate compared to existing methods for co-simrank computation. Overall, the paper provides a valuable contribution to the field of graph analysis and similarity measures.",1
1139,"co-simrank is a useful simrank-like measure of similarity based on graph structure . the existing method iteratively computes each pair of co-simrank score from a dot product of two pagerank vectors , entailing o -lrb- log -lrb- 1 / ǫ -rrb- n 3 -rrb- time to compute all pairs of co-simrank score in a graph with n nodes , to attain a desired accuracy ǫ . in this study , we devise a model , co-simrank , to speed up the retrieval of all pairs of co-simrank score to o -lrb- log 2 -lrb- log -lrb- 1 / ǫ -rrb- -rrb- n 3 -rrb- time . moreover , we show the optimality of co-simrank among other hop - -lrb- u k -rrb- variations , and integrate it with a matrix decomposition based method on singular graphs to attain higher efficiency . the viable experiments verify the superiority of co-simrank to others .",0
1140,"This paper proposes a model for on-line following of musical performances, specifically for automated musical accompaniment of human performers. The proposed model is based on a melodic corpus of 98 jazz melodies, which exhibits large-scale structural variation in terms of score form. The paper presents a sequence-based score follower algorithm that utilizes a Markov model to follow the performer in real-time. The model is trained on the written score and its branching structure to accurately track the performer's movements during the performance. The proposed approach is evaluated using sequence-based score followers, and the results show that the model can accurately follow the performer and generate appropriate accompaniment. Overall, the paper provides a valuable contribution to the field of automated musical accompaniment and real-time performance tracking.",1
1141,"automated musical accompaniment of human performers often requires an agent be able to follow a musical score with similar facility to that of a human performer . systems described in the literature represent musical scores in a way that assumes no large-scale structural variation of the piece during performance . if the performer deviates from the expected path by skipping or repeating a section , the system may become lost . we describe a way to automatically generate a markov model from a written score that models the score form , and an on-line algorithm to align a performance to a score . the resulting system can follow performances that take alternate paths through the score without losing its place . we compare the performance of our system to that of sequence-based score followers on a melodic corpus of 98 jazz melodies . results show that explicitly representing the branching structure of a score significantly improves score following when the branch a performer may take is unknown beforehand .",0
1142,"This paper introduces a Rao-Blackwellised Gibbs sampling approach for estimating the parameters of switching linear dynamical systems (SLDS). SLDS are commonly used for modeling time-series data, but their parameters are difficult to estimate due to the intractability of the model. The proposed approach combines hidden Markov models with SLDS and utilizes a stochastic segment model to assume independent segments in the data. The paper presents a proposal mechanism for the algorithm and shows how it can be applied to speech recognition tasks. The proposed approach is evaluated using the ARPA Resource Management Task dataset and the results demonstrate its effectiveness in inference and parameter estimation compared to other methods. Overall, the paper provides a valuable contribution to the field of SLDS modeling and inference, particularly in speech recognition tasks.",1
1143,"this paper describes the application of rao-blackwellised gibbs sampling to speech recognition using switching linear dy-namical systems -lrb- rao-blackwellised gibbs sampling -rrb- . the rao-blackwellised gibbs sampling is a hybrid of standard hidden markov models and linear dynamical systems . rao-blackwellised gibbs sampling is an extension of the stochastic segment model as rao-blackwellised gibbs sampling relaxes the assumption of independent segments . rao-blackwellised gibbs sampling explicitly take into account the strong co-articulation present in speech . unfortunately , inference in rao-blackwellised gibbs sampling is intractable unless the discrete state sequence is known . rao-blackwellised gibbs sampling is one approach that may be applied for both improved training and decoding for this form of intractable model . the theory of rao-blackwellised gibbs sampling and rao-blackwellised gibbs sampling is described , along with an efficient proposal mechanism . the performance of the rao-blackwellised gibbs sampling using rao-blackwellised gibbs sampling for training and inference is evaluated on the arpa resource management task .",0
1144,"This paper proposes a matched filtering assisted energy detection approach for spectrum sensing in cognitive radio networks. The proposed approach aims to sense weak primary user signals by utilizing secondary user (SU) signals and reducing SU interference. The paper presents an energy detector model for detecting the presence of primary user signals and shows how matched filtering can improve the performance of the detector. The proposed approach is evaluated using simulations and the results demonstrate its effectiveness in reducing SU interference and detecting weak primary user signals. Overall, the paper provides a valuable contribution to the field of energy detection and spectrum sensing in cognitive radio networks, particularly for sensing weak primary user signals.",1
1145,"energy detection is widely used by cognitive radios for spectrum sensing . during a silent period , secondary users are kept silent so that the energy detector does not confuse su signals for primary user -lrb- pu -rrb- signals . due to imperfect coordination , an secondary users may transmit during a silent period and cause possible false alarms . we propose to leverage matched filters that already exist in many sus to alleviate the impact of such su interference by combining the matched filtering result and the energy detection result . the analysis shows that for practical purposes , our algorithm virtually eliminates all of the negative impact of su interference with only negligible penalty in delay and energy consumption .",0
1146,"This paper proposes a block and group regularized sparse modeling approach for dictionary learning. The proposed approach utilizes block/group sparse coding schemes and dictionary learning methods to improve the learning process. The paper presents a dictionary learning framework that incorporates block and group structures, and optimizes intra-block coherence to achieve better performance. The proposed approach is evaluated using well-known datasets and the results demonstrate its effectiveness in improving the quality of the learned dictionary compared to other methods. The paper also presents optimization algorithms based on block-gradient descent and sparse coding techniques to solve the optimization problems involved in the learning process. Overall, the paper provides a valuable contribution to the field of dictionary learning and sparse modeling, particularly for input data with block/group structure.",1
1147,"this paper proposes a dictionary learning framework that combines the proposed block/group -lrb- bgsc -rrb- or reconstructed block/group sparse coding schemes with the novel intra-block coherence suppression dictionary learning -lrb- ics-dl -rrb- algorithm . an important and distinguishing dictionary blocks of the proposed dictionary learning framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective . we provide both empirical evidence and heuristic support for this dictionary blocks that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process . the optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent , and the details of the optimization algorithms are presented . we evaluate the proposed methods using well-known datasets , and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed dictionary learning framework .",0
1148,"This paper proposes single-link diffusion strategies over adaptive networks with limited communication overhead. The paper focuses on developing adaptive diffusion strategies that can be implemented with low communication overhead and achieve a steady-state mean-square-deviation. The paper presents a maximal-ratio-combining rule that combines the combination coefficients of the interacting nodes to achieve better performance. The proposed approach is evaluated using simulations and the results demonstrate its effectiveness in achieving a steady-state mean-square-deviation while keeping communication overhead low. Overall, the paper provides a valuable contribution to the field of adaptive diffusion strategies and communication overhead in networks, particularly for limited communication scenarios.",1
1149,"we propose an adaptive diffusion strategy with limited communication overhead by cutting off all links but one for each node in the network . we keep the '' best '' neighbor that has the smallest estimated variance-product measure and ignore the other neighbors . the combination coefficients for the interacting nodes are calculated via a maximal-ratio-combining rule to minimize the steady-state mean-square-deviation . simulation results illustrate that , with less communication overhead and less computations , the proposed adaptive diffusion strategy performs well and outperforms other related methods with similar overheads .",0
1150,"This paper presents a novel space-time video montage technique that utilizes the spatial and temporal information distribution of a video sequence to create a compact summary. The proposed method builds on the state-of-the-art graph cut optimization techniques and the3rst-jt algorithm to perform space-time video summarization. The paper describes a packing process that uses volumetric layers to represent the visual information of the video sequence along the time axis. By leveraging empty space and layering, the proposed technique produces a small output video volume that can be used as a video trailer or movie trailer. The paper compares the proposed method with existing video summarization methods, and the results demonstrate its effectiveness in generating high-quality space-time video montages. Overall, this paper makes a valuable contribution to the field of space-time video summarization methods and provides a useful technique for creating video summaries.",1
1151,"conventional video summarization methods j2 -rrb- cus predominantly on summarizing videos along the time axis , such as building a movie trailer : the resulting video trailer tends to retain much empty spuce in the background of the video ji-ames while discarding much informulive video content due lo size limit . in this pupes we propose a novel space-time video summarization method which we call space-time video montage . the space-time video summarization method simultaneously analyzes both the spatial and temporal injbrmation distribution in a video sequence , and exlructs the visually informative space-time portions of the input videos . the informative video porlions are represented in volumetric la.yers . the layers are then puckrd together in a smull ouzput video volume such tlzar the total amount of visual information in the video volume is maximized . to achieve the packing process , we develop a new space-time video summarization method based upon the3rst-jt und graph cut optimization techniques . since our space-time video summarization method is uble to cul qfr spatially und temporally less informative portions , it is uble to generute much more compuct yet highly informative output videos . the effecliveness -lrb- $ our space-time video summarization method is validated by extensive experiments over a wide variety c ~ videos .",0
1152,"This paper presents an efficient search algorithm for transformation-based inference. The proposed algorithm employs a local-lookahead node expansion method and a gradient-style evaluation function to efficiently search the space of inference-preserving transformations. The search problem is a key challenge in textual inference and the proposed algorithmic components improve the proof quality of an open-source system, Biutee. The efficiency of the proposed algorithm is demonstrated through experiments on various textual inference tasks.",1
1153,"this paper addresses the search problem in textual inference , where systems need to infer one piece of text from another . a prominent approach to this search problem is attempts to transform one text into the other through a sequence of inference-preserving transformations , a.k.a. a proof , while estimating the proof 's validity . this raises a search challenge of finding the best possible proof . we explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference : a gradient-style evaluation function , and a local-lookahead node expansion method . evaluations , using the open-source system , biutee , show the contribution of these ideas to search efficiency and proof quality .",0
1154,"I'm sorry, but the graph provided does not contain enough information to generate a coherent abstract. Please provide a more complete graph.",1
1155,"this paper presents a new analytical model for the normalized least mean square -lrb- nlms -rrb- adaptive algorithm . the new analytical model is derived using a stochastic differential equation approach . an accurate estimate of the steady-state weight-error correlations is also derived , which leads to an improved analytical model performance for medium and large step sizes . numerical simulations compare the new analytical model with existing analytical model and show better agreement with monte carlo simulations .",0
1156,"This paper proposes an ultrafast Monte Carlo method for statistical summations. The method utilizes a multi-stage stratified Monte Carlo approach, with probabilistic relative error control, to handle nested summations over datasets. The proposed technique addresses computational bottlenecks and provides speedups using multi-tree methods. The authors provide theoretical sample complexity analysis and demonstrate the scalability of the proposed method through numerical simulations. The results show that the method is effective in handling large dataset sizes and provides accurate results with improved efficiency compared to traditional Monte Carlo methods. The proposed technique can be useful in machine learning applications where efficient and accurate summation of large datasets is required.",1
1157,"machine learning contains many computational bottlenecks in the form of nested summations over datasets . computation of these machine learning is typically o -lrb- n 2 -rrb- or higher , which severely limits application to large datasets . we present a multi-stage stratified monte carlo method for approximating such machine learning with probabilistic relative error control . the essential idea is fast approximation by sampling in trees . this multi-stage stratified monte carlo method differs from many previous scalability techniques -lrb- such as multi-tree methods -rrb- in that its error is stochastic , but we derive conditions for error control and demonstrate that they work . further , we give a theoretical sample complexity for the multi-stage stratified monte carlo method that is independent of dataset size , and show that this appears to hold in experiments , where speedups reach as high as 10 14 , many orders of magnitude beyond the previous state of the art .",0
1158,"This paper proposes a novel type of auto-encoder, called a ""Contractive Auto-Encoder"" (CAE), that is able to extract features that are explicitly invariant to small deformations in the input space. Unlike traditional auto-encoders, the CAE includes a regularization term that encourages the activations of the encoder to be locally contractive, effectively constraining the local directions of variation of the encoded features. The CAE is able to reconstruct input data from its encoded representation, with the reconstruction error being measured by a classical cost function, such as the Frobenius norm. The proposed CAE is compared to other types of auto-encoders, such as deterministic and non-deterministic, and it is shown to perform better in terms of feature extraction and reconstruction quality. The paper also discusses the relationship between the CAE and other methods for pre-training deep neural networks, such as denoising auto-encoders, and shows how the CAE can be used to learn a lower-dimensional, non-linear manifold of the input space.",1
1159,"we present in this paper a novel approach for training deterministic auto-encoders . we show that by adding a well chosen penalty term to the classical reconstruction cost function , we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets . this penalty term corresponds to the frobenius norm of the jacobian matrix of the encoder activations with respect to the input . we show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer . furthermore , we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders . we find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data , corresponding to a lower-dimensional non-linear manifold , while being more invariant to the vast majority of directions orthogonal to the manifold . finally , we show that by using the learned features to initialize a mlp , we achieve state of the art classification error on a range of datasets , surpassing other methods of pre-training .",0
1160,"This paper examines the relationship between eye movements and spoken language comprehension. Specifically, it focuses on how rapid mental processes involved in linguistic processing can be measured by tracking saccadic eye movements. The paper explores how eye movements can provide insights into syntactic ambiguity resolution and reference resolution, and how they can be used to investigate the interplay between linguistic input and non-linguistic information in the visual workspace. Through a review of existing literature and empirical evidence, the paper highlights the potential of eye-tracking techniques for investigating spoken language comprehension and understanding the cognitive processes involved in interpreting spoken instructions.",1
1161,"we present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace . subjects naturally make saccadic eye-movements to objects that are closely time-locked to relevant information in the instruction . thus the eye-movements provide a window into the rapid mental processes that underlie spoken language comprehension . we review studies of reference resolution , word recognition , and pragmatic effects on syntactic ambiguity resolution . our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing . moreover , referentially relevant non-linguistic information immediately affects how the linguistic input is initially structured .",0
1162,"This paper proposes a new matrix factorization technique called Locality Preserving Nonnegative Matrix Factorization (LP-NMF) that aims to capture the intrinsic geometric structure of high-dimensional databases. By leveraging geodesics of the data manifold, LP-NMF is able to find a low-dimensional compact representation of feature values. This technique is particularly useful for information processing tasks such as hidden topic discovery, where the goal is to extract latent topics from large datasets. LP-NMF is also motivated by the way the human brain processes information, and it provides a new geometric perspective on non-negative matrix factorization. The effectiveness of LP-NMF is demonstrated through experiments on synthetic and real-world datasets, where it outperforms existing matrix factorization techniques in terms of accuracy and efficiency.",1
1163,"matrix factorization techniques have been frequently applied in information processing tasks . among them , non-negative matrix factorization have received considerable attentions due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in human brain . on the other hand , from geometric perspective the data is usually sampled from a low dimensional manifold embedded in high dimensional ambient space . one hopes then to find a compact representation which uncovers the hidden topics and simultaneously respects the intrinsic geometric structure . in this paper , we propose a novel algorithm , called locality preserving non-negative matrix factorization -lrb- lpnmf -rrb- , for this purpose . for two data points , we use kl-divergence to evaluate their similarity on the hidden topics . the optimal maps are obtained such that the feature values on hidden topics are restricted to be non-negative and vary smoothly along the geodesics of the data manifold . our empirical study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art algorithms on two large high-dimensional databases .",0
1164,"This paper introduces a theory of learning with similarity functions, where natural pairwise similarity functions are used to define implicit spaces. The paper explores the properties of these spaces and how they can be used in machine learning. Specifically, the paper focuses on kernel functions, which are a type of similarity function that have positive semi-definiteness. The theory presented in this paper provides a framework for understanding how kernel functions can be used to solve various learning problems, and how their properties can be used to design efficient algorithms for these problems. Overall, the paper presents a novel approach to learning with similarity functions that has the potential to improve the performance of machine learning algorithms in a variety of applications.",1
1165,"kernel functions have become an extremely popular tool in machine learning , with an attractive theory as well . this theory views a kernel as implicitly mapping data points into a possibly very high dimensional space , and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space . however , while quite elegant , this theory does not directly correspond to one 's intuition of a good kernel as a good similarity function . furthermore , it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning problem at hand since the implicit mapping may not be easy to calculate . finally , the requirement of positive semi-definiteness may rule out the most natural pairwise similarity functions for the given problem domain.in this work we develop an alternative , more general theory of learning with similarity functions -lrb- i.e. , sufficient conditions for a similarity function to allow one to learn well -rrb- that does not require reference to implicit spaces , and does not require the function to be positive semi-definite -lrb- or even symmetric -rrb- . our results also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be a good similarity function under our definition -lrb- though with some loss in the parameters -rrb- . in this way , we provide the first steps towards a theory of kernels that describes the effectiveness of a given kernel function in terms of natural similarity-based properties .",0
1166,"This paper proposes a method for instantaneous frequency estimation using discrete evolutionary transform (DET) for jammer excision in direct sequence spread spectrum communication. The method involves a recursive non-linear correction procedure that utilizes the time-frequency kernel obtained from the evolutionary kernel of DET. The procedure is applied to both noiseless and noisy situations to excise jammers from non-stationary signals. The paper presents results of numerical simulations showing the effectiveness of the proposed method in estimating instantaneous frequency and excising jammers. Additionally, the paper discusses the representation of instantaneous frequency using time-frequency analysis and the masking effect that occurs when the signal is corrupted by jamming.",1
1167,"in this paper , we propose a method -- based on the discrete evolutionary transform -- to estimate the instantaneous frequency of a signal embedded in noise or noise-like signals . the discrete evolutionary transform provides a representation for non-stationary signals and a time-frequency kernel that permit us to obtain the time-dependent spectrum of the signal . we will show the instantaneous phase and the corresponding instantaneous frequency can also be computed from the evolutionary kernel . estimation of instantaneous frequency is of general interest in time-frequency analysis , and of special interest in the excision of jammers in direct sequence spread spectrum . implementation of the instantaneous frequency is done by masking and a recursive non-linear correction procedure . the proposed instantaneous frequency is valid for monocompo-nent as well as multicomponent signals in the noiseless and noisy situations . its application to jammer excision in direct sequence spread spectrum communication is considered as an important application . the instantaneous frequency procedure is illustrated with several examples .",0
1168,"This paper proposes a tensor decomposition algorithm for efficient parsing with latent-variable probabilistic context-free grammars (PCFGs) in natural language processing. The authors demonstrate how tensor decomposition, a powerful tool from multilinear algebra literature, can be used to speed up inference in PCFG parsing. By formulating the parsing problem as a tensor decomposition problem, the authors show that it is possible to achieve significant speed-ups without sacrificing accuracy. The proposed method is evaluated on real-world natural language parsing data, and the results show that the tensor decomposition approach outperforms previous state-of-the-art methods. This paper contributes to the literature on natural language parsing by introducing a new approach that leverages the power of tensor decomposition to speed up the inference process.",1
1169,"we describe an approach to speed-up inference with latent-variable pcfgs , which have been shown to be highly effective for natural language parsing . our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable pcfgs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature . we also describe an error bound for this approximation , which gives guarantees showing that if the underlying tensors are well approximated , then the probability distribution over trees will also be well approximated . empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance .",0
1170,"This paper presents a sparse probabilistic regression approach for activity-independent human pose inference. The proposed method maps visual observations to articulated body configurations in pose space using a local regression scheme. The approach uses a probabilistic framework with gaussian processes and hyperparameters to model the underlying probability distribution over the pose space. The online regression scheme updates the model in a computationally efficient way, and the approach is shown to outperform global regression and discriminative approaches on real pose databases. The proposed approach is also able to prune irrelevant features and take into account local neighborhoods of the data, resulting in a faster and more accurate pose inference method.",1
1171,"discriminative approaches to human pose inference involve mapping visual observations to articulated body configurations . current probabilistic approaches to learn this mapping visual observations have been limited in their ability to handle domains with a large number of activities that require very large training sets . we propose an online probabilistic regression scheme for efficient inference of complex , high-dimensional , and multimodal mappings . our online probabilistic regression scheme is based on a local mixture of gaussian processes , where locality is defined based on both appearance and pose , and where the mapping hyperparameters can vary across local neighborhoods to better adapt to specific regions in the pose space . the mapping hyperparameters are defined online in very small neighborhoods , so learning and inference is extremely efficient . when the mapping visual observations is one-to-one , we derive a bound on the approximation error of local regression -lrb- vs. global regression -rrb- for monotonically decreasing co-variance functions . our online probabilistic regression scheme can determine when training examples are redundant given the rest of the database , and use this criteria for pruning . we report results on synthetic -lrb- poser -rrb- and real pose databases , obtaining fast and accurate pose estimates using training set sizes up to 10 5 .",0
1172,"This paper proposes a method for globally optimizing neural network models through a sequential sampling framework using importance resampling algorithms. The proposed method iteratively samples from a probability distribution of the network weights, evaluates the performance of the network on a validation set, and updates the probability distribution to bias future samples towards promising regions of the weight space. This approach allows for the efficient exploration of a large search space and can improve the generalization performance of the network. The paper demonstrates the effectiveness of the proposed method on several benchmark datasets and compares it to other global optimization methods.",1
1173,"we propose a novel strategy for training neural networks using sequential sampling-importance resampling algorithms . this global optimisation strategy allows us to learn the probability distribution of the network weights in a sequential framework . it is well suited to applications involving on-line , nonlinear , non-gaussian or non-stationary signal processing .",0
1174,"This paper presents a novel approach for statistical machine translation, called Hierarchical Incremental Adaptation (HIA). HIA introduces a flexible hierarchical domain structure, allowing for the adaptation of the translation model to different domains. The approach uses a stream of post-edits to improve translation quality, and takes into account local context to ensure consistent model rules. The incremental adaptation approach also provides a way to control the granularity of the adaptation process. Experiments show that HIA outperforms existing methods in terms of translation quality.",1
1175,"we present an incremental adaptation approach for statistical machine translation that maintains a flexible hierarchical domain structure within a single consistent model . both weights and rules are updated incrementally on a stream of post-edits . our multi-level domain hierarchy allows the incremental adaptation approach to adapt simultaneously towards local context at different levels of granularity , including genres and individual documents . our experiments show consistent improvements in translation quality from all components of our incremental adaptation approach .",0
1176,This paper proposes a deep random field model for image segmentation based on pairwise random field approaches. The proposed model utilizes cooperative cuts and coupling graph edges to segment images into instances. It also incorporates multi-layered hidden units to improve the accuracy of random field map inference. The paper presents an exact inference algorithm for the proposed model and shows that it outperforms existing image segmentation methods by addressing the short-boundary bias problem inherent in pairwise random field based approaches. The experimental results demonstrate the effectiveness of the proposed approach in achieving accurate image segmentation.,1
1177,"we discuss a model for image segmentation that is able to overcome the short-boundary bias observed in standard pairwise random field based approaches . to wit , we show that a random field with multi-layered hidden units can encode boundary preserving higher order potentials such as the ones used in the cooperative cuts model of -lsb- 12 -rsb- while still allowing for fast and exact map inference . exact inference allows our model to outperform previous image seg-mentation methods , and to see the true effect of coupling graph edges . finally , our model can be easily extended to handle segmentation instances with multiple labels , for which it yields promising results .",0
1178,"This paper presents a novel particle filtering algorithm for tracking in infinite or large-dimensional state spaces. The proposed algorithm uses a basis change detection technique and weaker assumptions on the noise distribution to adaptively re-estimate the particle weights, improving the accuracy of the tracking. The algorithm is shown to be asymptotically stable, and its performance is compared to other particle filtering algorithms. The results demonstrate the effectiveness of the proposed algorithm in tracking applications.",1
1179,"we study particle filtering algorithms for tracking on infinite -lrb- in practice , large -rrb- dimensional state spaces . particle filtering -lrb- monte carlo sampling -rrb- from a large dimensional system noise distribution is computationally expensive . but , in most large dim tracking applications , it is fair to assume that '' most of the state change '' occurs in a small dimensional basis and the basis itself may be slowly time varying -lrb- approximated as piecewise constant -rrb- . we have proposed a pf algorithm with basis change detection and re-estimation steps that uses this idea . the implicit assumptions in defining this pf algorithm are very strong . we study here the implications of weaker assumptions and how to handle them . we propose to use a simple modification of the asymptotically stable adaptive particle filter to handle errors in estimating the basis dimension .",0
1180,"This paper presents a study on spectral and temporal modulation features for phonetic recognition in automatic speech recognition systems. The authors analyze the log magnitude spectrum and the modulation spectrum of speech signals, and propose new features based on spectral and modulation information. The features are extracted using a discrete cosine transform analysis and provide both spectral and temporal resolution. The authors compare the new features with traditional MFCCs on the TIMIT database and show that they improve phonetic recognition performance. This study provides insights into the importance of spectral and modulation features in phonetic recognition and could have implications for future speech recognition systems.",1
1181,"recently , the modulation spectrum has been proposed and found to be a useful source of speech information . the modulation spectrum represents longer term variations in the spectrum and thus implicitly requires features extracted from much longer speech segments compared to mfccs and their delta terms . in this paper , a discrete cosine transform analysis of the log magnitude spectrum combined with a discrete cosine series -lrb- dcs -rrb- expansion of dct coefficients over time is proposed as a method for capturing both the spectral and modulation information . these dct/dcs features can be computed so as to emphasize frequency resolution or time resolution or a combination of the two factors . several variations of the dct/dcs features were evaluated with phonetic recognition experiments using timit and its telephone version -lrb- timit -rrb- . best results obtained with a combined feature set are 73.85 % for timit and 62.5 % for timit . the modulation features are shown to be far more important than the spectral features for automatic speech recognition and far more noise robust .",0
1182,"This paper proposes an active inference approach for dynamic Bayesian networks (DBNs) that can learn and optimize over time. The authors suggest a training phase in which the selective collection of data can be used to improve the model's accuracy, particularly for unobserved variables. The active inference framework is used to make predictions in a way that optimizes the model's performance, thereby improving the overall accuracy of the DBN. The paper highlights how active inference can be used to tackle the challenges of learning and inference in dynamic systems, and how it can complement other techniques such as supervised learning. The approach is evaluated through simulations, demonstrating its effectiveness in improving the performance of DBNs for temporal systems. Overall, this work provides a promising direction for future research on active inference in the context of dynamic systems.",1
1183,"in supervised learning , many techniques focus on optimizing training phase to increase prediction performance . active inference , a relatively novel paradigm , aims to decrease overall prediction error via selective collection of some labels based on relations among instances . in this research , we use dynamic bayesian networks to model temporal systems and we apply active inference to dynamically choose variables for observation so as to improve prediction on unobserved variables .",0
1184,This paper proposes an instantaneous vector representation of delta pitch for predicting speaker changes in conversational dialogue systems. The approach aims to enhance the naturalness of interaction in spoken dialogue systems by improving the system responsiveness to speaker changes. The proposed method uses acoustic modeling techniques to extract features from automatically labeled data and trains a machine learning model to predict speaker changes based on these features. The paper compares the proposed method with a hand-crafted baseline and shows that it outperforms the baseline in terms of precision and recall of locations. The experimental results also demonstrate that the proposed method can improve the naturalness of interaction in pause-only systems and achieve performance close to real human-human conversations.,1
1185,"as spoken dialogue systems become deployed in increasingly complex domains , spoken dialogue systems face rising demands on the naturalness of interaction . we focus on system responsiveness , aiming to mimic human-like dialogue flow control by predicting speaker changes as observed in real human-human conversations . we derive an instantaneous vector representation of pitch variation and show that spoken dialogue systems is amenable to standard acoustic modeling techniques . using a small amount of automatically labeled data , we train models which significantly outperform current state-of-the-art pause-only systems , and replicate to within 1 % absolute the performance of our previously published hand-crafted baseline . the new system additionally offers scope for run-time control over the precision or recall of locations at which to speak .",0
1186,"This paper proposes a novel method for on-line simultaneous learning and tracking of visual feature graphs. The approach relies on a graphical model to represent the visual features, which allows for efficient inference and learning. The proposed method can handle uninformative components in the visual feature graphs and is able to learn the model in an on-line fashion. The approach is evaluated on several datasets, and the results show that it outperforms existing methods in terms of tracking accuracy and speed. This work is relevant for computer vision applications that require robust tracking of objects in video sequences.",1
1187,"model learning and tracking are two important topics in computer vision . while there are many applications where one of them is used to support the other , there are currently only few where both aid each other simultaneously . in this work , we seek to incrementally learn a graphical model from tracking and to simultaneously use whatever has been learned to improve the tracking in the next frames . the main problem encountered in this situation is that the current graphical model may be inconsistent with future observations , creating a bias in the tracking results . we propose an uncertain model that explicitly accounts for such uncertainties by representing relations by an appropriately weighted sum of informative -lrb- parametric -rrb- and uninformative components . the method is completely unsupervised and operates in real time .",0
1188,"This paper presents a method for optimizing binary Markov random fields (MRFs) by introducing higher order cliques. Binary MRFs have been widely used in computer vision problems such as image segmentation, where the pairwise interaction between neighboring pixels is modeled using a Gibbs energy function. However, in certain scenarios, higher order interactions between pixels may be useful to improve the segmentation results. The proposed method adds cliques of higher order to the MRF model and uses energy minimization tools to optimize the resulting energy function. Specifically, the authors introduce higher order counterparts to the pairwise MRF models, which are expressed as polynomial functions of order greater than two. The method is evaluated on image segmentation tasks, and the results show that the higher order cliques can significantly improve the segmentation accuracy compared to the traditional pairwise MRF models.",1
1189,"widespread use of efficient and successful solutions of computer vision problems based on pairwise markov random field models raises a question : does any link exist between the pairwise and higher order mrfs such that the like solutions can be applied to the latter models ? this work explores such a link for binary mrfs that allow us to represent gibbs energy of signal interaction with a polynomial function . we show how a higher order polynomial can be efficiently transformed into a quadratic function . then energy minimization tools for the pairwise mrf models can be easily applied to the higher order counterparts . also , we propose a method to analytically estimate the potential parameter of the asymmetric potts prior . the proposed framework demonstrates very promising experimental results of image segmentation and can be used to solve other computer vision problems .",0
1190,"This paper proposes a method for improving pivot-based statistical machine translation (SMT) using random walks. Pivot-based SMT is a popular approach that uses a pivot language to translate between two languages, instead of translating directly between the two languages. However, this approach can be limited by the availability and quality of the pivot language data. To address this issue, the proposed method leverages Markov random walks to generate new pivot phrases from available bilingual data and improve the source-target translations. The effectiveness of the approach is evaluated using European Parliament data and web data, and the results show significant improvements in translation quality compared to traditional pivot-based SMT approaches. This study demonstrates the potential of machine learning methods for improving pivot-based SMT using additional sources of data.",1
1191,"this paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation . for language pairs with few bilingual data , a possible solution in pivot-based smt using another language as a `` bridge '' to generate source-target translation . however , one of the weaknesses is that some useful source-target translations can not be generated if the corresponding source phrase and target phrase connect to different pivot phrases . to alleviate the problem , we utilize markov random walks to connect possible translation phrases between source and target language . experimental results on european parliament data , spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system .",0
1192,"This paper proposes a method for gap filling in the plant kingdom through trait prediction using hierarchical probabilistic matrix factorization (HPMF). The missing data problem in plant traits is addressed by adapting matrix completion techniques to the joint trait analysis and hierarchical phylogenetic structure of plant communities. The paper describes the use of HPMF to predict trait correlations based on the global database of plant traits, which incorporates phylogenetic structure. The proposed approach is evaluated using real-world plant trait data, and the results demonstrate the effectiveness of the HPMF method for trait prediction and gap filling in the plant kingdom. The paper concludes with a discussion of the potential applications of the proposed approach to ecological communities and the adaptation of ecosystems.",1
1193,"plant traits are a key to understanding and predicting the adaptation of ecosystems to environmental changes , which motivates the try project aiming at constructing a global database for plant traits and becoming a standard resource for the ecological community . despite its unprecedented coverage , a large percentage of missing data substantially constrains joint trait analysis . meanwhile , the trait data is characterized by the hierarchical phylogenetic structure of the plant kingdom . while factorization based matrix completion techniques have been widely used to address the missing data problem , traditional matrix factorization methods are unable to leverage the phylogenetic structure . we propose hierarchical probabilistic matrix factorization , which effectively uses hierarchical phylogenetic information for trait prediction . we demonstrate hierarchical probabilistic matrix factorization 's high accuracy , effectiveness of incorporating hierarchical structure and ability to capture trait correlation through experiments .",0
1194,"This paper proposes a novel approach for intelligent mission profiling of micro air vehicles using a multiscale Viterbi classification algorithm. The algorithm utilizes tree-structured belief networks and complex wavelet transform in a multi-scale Bayesian classification framework to model statistical models of object appearances. The paper also introduces an adaptive feature selection algorithm to select discriminating features for classification, considering the relative poverty of scene detail and complexity of object appearances. The proposed algorithm is evaluated on real-world test images and compared to the MSBC paradigm, considering real-time constraints and robustness to video noise. The experimental results demonstrate the effectiveness of the proposed approach, indicating its potential for object recognition in aerial images and visual contexts with limited scene detail. The paper also discusses the design choices and feature set for the proposed approach, emphasizing the importance of context-based object recognition and the hierarchical representation of visual contexts in the tree-structured belief networks.",1
1195,"in this paper , we present a vision system for object recognition in aerial images , which enables broader mission profiles for micro air vehicles . the most important factors that inform our design choices are : real-time constraints , robustness to video noise , and complexity of object appearances . as such , we first propose the hsi color space and the complex wavelet transform as a set of sufficiently discriminating features . for each feature , we then build tree-structured belief networks as our underlying statistical models of object appearances . to perform object recognition , we develop the novel multiscale viterbi classification algorithm , as an improvement to multi-scale bayesian classification . next , we show how to globally optimize tree-structured belief networks with respect to the feature set , using an adaptive feature selection algorithm . finally , we discuss context-based object recognition , where visual contexts help to disambiguate the identity of an object despite the relative poverty of scene detail in flight images , and obviate the need for an exhaustive search of objects over various scales and locations in the image . experimental results show that the proposed vision system achieves smaller classification error and fewer false positives than systems using the msbc paradigm on challenging real-world test images .",0
1196,"This paper proposes a method for multi-timbre chord classification using wavelet transform and self-organized map neural networks. The proposed method is based on the model of human perception of information of timbres and musical chord recognition. The wavelet-based transform is used to extract the features of the chords and the self-organized map neural network is used for classification. The results show that the proposed method achieves high accuracy in classifying chords with different timbres, and the method has potential applications in music information retrieval and analysis.",1
1197,"this paper presents a new method for musical chord recognition based on a model of human perception . we classify the chords directly from the sound without the information of timbres and notes . a wavelet-based transform as well as a self-organized map neural network is adopted to imitate human ears and cerebra , respectively . the resultant system can classify chords very well even in a noisy environment .",0
1198,"This paper proposes the use of decision trees for incremental parametric speech synthesis. Parametric speech synthesizers use hidden Markov models (HMMs) for state selection, and this work suggests using decision trees to determine the HMM states, taking into account the full-utterance context and features such as phonetic details and prosody modeling. By incrementally updating the vocoding parameters, the proposed approach enables incremental speech synthesis, which has important use cases such as real-time voice conversion. The results show that the proposed method outperforms previous approaches in terms of both naturalness and speaker similarity.",1
1199,"human speakers plan and deliver their utterances incremen-tally , piece-by-piece , and it is obvious that their choice regarding phonetic details -lrb- and the details ' peculiarities -rrb- is rarely determined by globally optimal solutions . in contrast , para-metric speech synthesizers use a full-utterance context when optimizing vocoding parameters and when determing hmm states . apart from being cognitively implausible , this impedes incremental use-cases , where the future context is often at least partially unavailable . this paper investigates the ` locality ' of features in parametric speech synthesis voices and takes some missing steps towards better hmm state selection and prosody modelling for incremental speech synthesis .",0
1200,"This paper proposes a method for detecting bursts of activity in folksonomies based on temporal and social context. The authors present a robust state-based model for burst detection that uses a learning method to identify burst pulses in a temporal stream of social media content. The model takes into account the collaborative context of users and the lifetime of resources, as well as textual features such as topic coverage and user attractiveness. Burst extraction is based on metadata frequency and textual features, and the results show that the proposed method outperforms existing approaches. The paper concludes that the use of temporal and social context can significantly improve burst detection in folksonomies.",1
1201,"burst detection is an important topic in temporal stream analysis . usually , only the textual features are used in burst detection . in the theme extraction from current prevailing social media content , it is necessary to consider not only textual features but also the pervasive collaborative context , e.g. , resource lifetime and user activity . this paper explores novel approaches to combine multiple sources of such indication for better burst extraction . we systematically investigate the characters of collaborative context , i.e. , metadata frequency , topic coverage and user attractiveness . first , a robust state based model is utilized to detect bursts from individual streams . we then propose a learning method to combine these burst pulses . experiments on a large real dataset demonstrate the remarkable improvements over the traditional methods .",0
1202,"The paper proposes a new approach to learn the dynamics of cellular automata using neural networks. Cellular automata are known to exhibit complex or chaotic behavior that is difficult to understand and analyze. The proposed method uses e-ii units that represent the automaton rule and have short-range connections. The weights of the connections are shared across the units, which allows the neural network to learn the asymptotic dynamical behavior of the cellular automata. The paper shows that this approach can successfully learn the dynamics of a variety of cellular automata and can capture the complex behavior that emerges from simple local interactions.",1
1203,"we have trained networks of e-ii units with short-range connections to simulate simple cellular automata that exhibit complex or chaotic behaviour . three levels of learning are possible -lrb- in decreasing order of difficulty -rrb- : learning the underlying automaton rule , learning asymptotic dynamical behaviour , and learning to extrapolate the training history . the levels of learning achieved with and without weight sharing for different automata provide new insight into their dynamics .",0
1204,"This paper proposes a locally adaptive neighborhood morphing classification method for nearest neighbor classification, driven by a combination of linear discriminant analysis and support vector machine learning. The proposed method aims to address the curse of dimensionality and severe bias issues that often arise in finite sample settings. By incorporating class conditional probabilities and discriminant feature dimensions into the algorithm, the proposed method achieves improved classification accuracy compared to traditional nearest neighbor methods. The effectiveness of the approach is demonstrated through experiments on several benchmark datasets.",1
1205,"nearest neighbor -lrb- nn -rrb- classification relies on the assumption that class conditional probabilities are locally constant . this assumption becomes false in high dimensions with finite samples due to the curse of dimensionality . the nearest neighbor classification introduces severe bias under these conditions . we propose a locally adaptive neighborhood morphing classification method to try to minimize bias . we use local support vector machine learning to estimate an effective metric for producing neighborhoods that are elongated along less discriminant feature dimensions and constricted along most discriminant ones . as a result , the class conditional probabilities can be expected to be approximately constant in the modified neighborhoods , whereby better classification performance can be achieved . the efficacy of our locally adaptive neighborhood morphing classification method is validated and compared against other competing techniques using a number of datasets .",0
1206,"This paper proposes a new algorithm for solving deconvolution problems in signal processing applications. The algorithm, called the Recursive Total Least Squares (RTLS) algorithm, is a modification of the Total Least Squares (TLS) approach and is designed to handle noise-corrupted FIR channels. Unlike the non-recursive TLS algorithm, the RTLS algorithm is computationally efficient and can be implemented recursively, making it suitable for blind algorithms. The paper provides a detailed description of the RTLS algorithm and compares its performance with other existing algorithms. The results show that the RTLS algorithm outperforms other algorithms in terms of computational cost and accuracy. The paper concludes that the RTLS algorithm is an effective method for solving deconvolution problems with unknown input signals and FIR channels. Additionally, the paper demonstrates the application of the Kalman filter to the recursive implementation of the RTLS algorithm.",1
1207,"deconvolution problems are encountered in signal processing applications where an unknown input signal can only be observed after propagation through one or more noise corrupted fir channels . the first step in recovering the input usually entails an estimation of the fir channels through training based or blind algorithms . the ` standard ' procedure then uses least squares estimation to recover the input . a recursive implementation with constant computational cost is based on the kalman filter . in this paper we focus on a total least squares based approach , which is more appropriate if errors are expected both on the output samples and the estimates of the fir channels . we will develop a re-cursive total least squares algorithm which closely approximates the performance of the non-recursive tls algorithm and this at a much lower computational cost .",0
1208,This paper presents a novel noise reduction algorithm based on a generalized filter-bank equalizer with reduced signal delay. The proposed method utilizes analysis-synthesis filter-bank (ASFB) structure to obtain non-uniform frequency resolution and reduces the delay of the filter-bank to a low level. The algorithmic complexity of the equalizer is reduced by employing allpass transformation. The proposed method also enhances signal reconstruction by using frequency-domain coefficients. The filter-bank structure is used for signal reconstruction and the allpass transformation is used for achieving non-uniform frequency resolution. The proposed generalized filter-bank equalizer is compared with both time-domain and conventional filter-bank equalizers. The experimental results show that the proposed method outperforms these methods in terms of noise reduction and low delay filter-bank. Different parameter configurations are also evaluated to optimize the performance of the proposed method. The proposed method can be used in various applications that require noise reduction with reduced signal delay.,1
1209,"an efficient realization of a low delay filter-bank , termed as generalized filter-bank equalizer , will be proposed for noise reduction with low signal delay . the generalized filter-bank equalizer is equivalent to a time-domain filter with coefficients adapted in the frequency-domain . this filter-bank structure ensures perfect signal reconstruction for a variety of spectral transforms with less restrictions than for an analysis-synthesis filter-bank -lrb- as fb -rrb- . a non-uniform frequency resolution can be achieved by an allpass transformation . in this case , the generalized filter-bank equalizer has not only a lower signal delay than the generalized filter-bank equalizer , but also a lower algorithmic complexity for most parameter configurations . another advantage of the generalized filter-bank equalizer is the lower number of required delay elements -lrb- memory -rrb- compared to the generalized filter-bank equalizer . the noise reduction achieved by means of the generalized filter-bank equalizer and the generalized filter-bank equalizer is approximately equal .",0
1210,"This paper proposes a novel method for detecting speech embedded in real acoustic background based on amplitude modulation spectrogram (AMS) features. The modulation frequency range of the AMS is divided into modulation components, which are used as classification features. Feature selection techniques are employed to select the most discriminative modulation components. The proposed method is evaluated using the ITU G.729B voice activity detection (VAD) dataset and false positive rates are measured for different classification methods. The accuracy of the proposed method is evaluated using test-data and compared with other state-of-the-art methods. The experimental results show that the proposed method outperforms other methods in terms of detection of speech in real acoustic background. The proposed method is able to effectively distinguish speech from non-speech sounds and signals. The modulation features of the AMS are used for the classification task and the modulation components are used as features for the classification method. The AMS is shown to be effective in capturing the modulation components of the speech signal. The proposed method can be used in various applications that require detection of speech in noisy environments.",1
1211,"a classification method is presented that detects the presence of speech embedded in a real acoustic background of non-speech sounds . features used for classification are modulation components extracted by computation of the amplitude modulation spectrogram . feature selection techniques and support vector classification are employed to identify modulation components most salient for the classification task and therefore considered as highly characteristic for speech . results show that reliable detection of speech can be performed with less than 10 optimally selected modulation features , the most important ones are located in the modulation frequency range below 10 hz . detection of speech in a background of non-speech signals is performed with about 90 % test-data accuracy at a signal-to-noise level of 0 db . compared to standard itu g729.b voice activity detection , the proposed classification method results in increased true positive and reduced false positive rates induced by a real acoustic background .",0
1212,"This paper presents a simple and effective approach for training dependency parsers via structured boosting. The proposed method uses a boosting-like procedure to train a base classifier, which is then used to train structured predictors for predicting dependency links. The base classifier is a logistic regression model, and specialized training algorithms such as support vector machines, maximum margin Markov networks, and conditional random fields are used for coordinated training of the structured predictors. The proposed method achieves high accuracy in dependency parsing tasks, outperforming other supervised training methods. The boosting-like procedure is used to improve the accuracy of the base classifier by incorporating misclassified components in the training process. The structured predictors are trained using coordinated training algorithms, which combine the strengths of both conditional random fields and maximum margin Markov networks. The proposed method is evaluated on various datasets, including Chinese features, and is shown to outperform state-of-the-art methods. The logistic regression model is used as the base classifier, which is shown to be effective in the dependency parsing model. The proposed method can be used in various natural language processing applications that require accurate dependency parsing.",1
1213,"recently , significant progress has been made on learning structured predictors via coordinated training algorithms such as conditional random fields and maximum margin markov networks . unfortunately , these techniques are based on specialized training algorithms , are complex to implement , and expensive to run . we present a much simpler approach to training structured predictors by applying a boosting-like procedure to standard supervised training methods . the idea is to learn a local predictor using standard methods , such as logistic regression or support vector machines , but then achieve improved structured classification by '' boosting '' the influence of misclassified components after structured classification , retraining the local predictor , and repeating . further improvement in structured prediction accuracy can be achieved by incorporating '' dynamic '' features -- i.e. an extension whereby the features for one predicted component can depend on the predictions already made for some other components . we apply our techniques to the problem of learning dependency parsers from annotated natural language corpora . by using logistic regression as an efficient base classifier -lrb- for predicting dependency links between word pairs -rrb- , we are able to efficiently train a dependency parsing model , via structured boosting , that achieves state of the art results in en-glish , and surpasses state of the art in chinese .",0
1214,"This paper presents a new approach for structure learning in Bayesian networks using Annealed Importance Sampling (AIS). AIS is a Markov Chain Monte Carlo method that enables efficient sampling of posterior distributions by constructing a sequence of intermediate distributions. The proposed method uses AIS to sample from the space of all possible Bayesian network structures, which is computationally challenging due to the combinatorial explosion of the search space. The sampling approach is based on order-based sampling, where linear orders of the nodes are sampled and converted to partial orders, followed by a probabilistic guarantees-based approach to generate the posterior distribution. The proposed method is shown to be effective and efficient in parallelizing the sampling process to reduce the computational bias of AIS. The effectiveness of the proposed method is evaluated using various datasets, and it is shown to outperform other sampling methods such as Markov Chain Monte Carlo (MCMC) and other order-based sampling methods. The proposed method is useful for learning Bayesian network structures from data and can be applied to various domains such as healthcare and finance, where the ability to model complex dependencies is crucial. The paper also provides a comparison of AIS with MCMC, showing that the proposed method has better computational efficiency and accuracy.",1
1215,"we present a new sampling approach to bayesian learning of the bayesian network structure . like some earlier sampling methods , we sample linear orders on nodes rather than directed acyclic graphs -lrb- dags -rrb- . the key difference is that we replace the usual markov chain monte carlo method by the sampling approach of annealed importance sampling . we show that markov chain monte carlo method is not only competitive to markov chain monte carlo method in exploring the posterior , but also superior to markov chain monte carlo method in two ways : markov chain monte carlo method enables easy and efficient parallelization , due to the independence of the samples , and lower-bounding of the marginal likelihood of the sampling approach with good probabilistic guarantees . we also provide a principled way to correct the bias due to order-based sampling , by implementing a fast algorithm for counting the linear extensions of a given partial order .",0
1216,"This paper proposes a concept-based probabilistic verification model for language understanding in spoken dialogue systems. The model aims to reduce error rates in concept verification by using a combination of acoustic confidence measures, contextual confidence information, and confidence tags. The model considers neighboring concepts to improve the overall confidence measure, and uses probabilistic techniques to account for uncertainty. The paper evaluates the model's performance in terms of error reduction rates and compares it to other concept verification models. The results show that the proposed model outperforms other models and improves the confidence measure for words and phrases in spoken dialogue systems.",1
1217,"in the past researches , several kinds of information have been explored to assess the confidence measure or to select the confidence tag for a word/phrase . however , the contextual confidence information is little touched . in this paper , we propose a concept-based probabilistic verification model to integrate the contextual confidence information . in this concept-based probabilistic verification model , a concept is verified not only according to its acoustic confidence measure but also according to neighboring concepts and their confidence levels . experimental results show that the proposed concept-based probabilistic verification model significantly outperforms the concept-based probabilistic verification model using only confidence measures . the error rate of confidence tag is reduced from 17.7 % to 15.12 % , which corresponds to an error reduction rate of 14.5 % .",0
1218,"This paper presents a Linear Programming Boosting approach for addressing the problem of uneven datasets in text classification. Uneven datasets often lead to poor classification performance due to biased classifiers towards the majority class. Linear Programming Boosting is a powerful machine learning technique that can improve classification accuracy by iteratively adjusting the weights of misclassified samples. In this work, the authors propose a novel LPBoost algorithm that adapts to uneven datasets by weighting examples based on their difficulty level. The proposed method achieves significant improvements in classification accuracy compared to existing boosting strategies on several real-world text classification datasets with imbalanced class distribution.",1
1219,"the paper extends the notion of linear programming boosting to handle uneven datasets . extensive experiments with text classification problem compare the performance of a number of different boosting strategies , concentrating on the problems posed by uneven datasets .",0
1220,"This paper focuses on the analysis of lexical co-occurrence, statistical significance, and word association in natural language processing. The study examines the span distributions of associated words and global unigram frequencies to detect word associations. It also evaluates co-occurrence measures such as biases ochiai and ttest in benchmark data sets. The results show that lexical co-occurrence is effective in detecting word associations and can be improved with appropriate co-occurrence measures. This study provides insights into the statistical significance of word associations and contributes to the development of effective natural language processing techniques.",1
1221,"lexical co-occurrence is an important cue for detecting word associations . we present a theoretical framework for discovering statistically significant lexical co-occurrences from a given corpus . in contrast with the prevalent practice of giving weightage to unigram frequencies , we focus only on the documents containing both the terms -lrb- of a candidate bi-gram -rrb- . we detect biases in span distributions of associated words , while being agnostic to variations in global unigram frequencies . our framework has the fidelity to distinguish different classes of lexical co-occurrences , based on strengths of the document and corpus-level cues of co-occurrence in the data . we perform extensive experiments on benchmark data sets to study the performance of various co-occurrence measures that are currently known in literature . we find that a relatively obscure measure called ochiai , and a newly introduced measure csa capture the notion of lexical co-occurrence best , followed next by llr , dice , and ttest , while another popular measure , pmi , suprisingly , performs poorly in the context of lexical co-occurrence .",0
1222,This paper presents a session variability subspace projection (SVSP) based model compensation method for speaker verification. The proposed method is shown to reduce the relative equal error rate (EER) when compared to conventional speaker verification systems that use Gaussian mixture model-universal background model (GMM-UBM) systems. The SVSP-based compensation method aims to address the issue of session variability in speaker verification by projecting speaker models onto a low-dimensional subspace that captures the relevant variability across different sessions. The compensated speaker models are then used in a GMM-UBM system for speaker verification. Experimental results demonstrate the effectiveness of the proposed method in reducing the relative EER and improving speaker verification performance in comparison to conventional GMM-UBM systems.,1
1223,"in this paper , a session variability subspace projection svspbased model compensation method for speaker verification is proposed . during the training phase the session variability is removed from speaker models by projection , while during the testing phase the session variability in a test utterance is used to compensate speaker models . finally , the compensated speaker models and ubm are used to recognize the identity of the test utterance . compared with the conventional gmm-ubm system , the relative equal error rate reduction of session variability subspace projection svspbased model compensation method is 16.2 % on the nist 2006 single-side one conversation training , single-side one conversation test .",0
1224,This paper proposes a divide-and-conquer approach to reconstruct 3D objects from single 2D line drawings. The goal is to reconstruct more complex 3D shapes than those achievable by current techniques. The proposed method exploits the geometric structure of 3D objects and employs a face identification algorithm to match line drawings to specific 3D faces. This allows the 3D reconstruction of complex manifold objects. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods for 3D reconstruction of complex objects from line drawings.,1
1225,"3d object reconstruction from a single 2d line drawing is an important problem in both computer vision and graphics . many methods have been put forward to solve this problem , but they usually fail when the geometric structure of a 3d object becomes complex . in this paper , a novel approach based on a divide-and-conquer strategy is proposed to handle 3d reconstruction of complex manifold objects from single 2d line drawings . the approach consists of three steps : 1 -rrb- dividing a complex line drawing into multiple simpler line drawings based on the result of face identification ; 2 -rrb- reconstructing the 3d shapes from these simpler line drawings ; and 3 -rrb- merging the 3d shapes into one complete object represented by the original line drawing . a number of examples are given to show that our approach can handle 3d reconstruction of more complex objects than previous methods .",0
1226,"This paper proposes a new method for estimating multipulse input time series from noisy output without determining the rank of the system. The proposed method, called tapered SVD without rank determination, uses a new optimally tapered window to overcome the difficulties in determining the rank of the system. The method is applied to the resonant transfer system, and compared to the standard SVD-based estimator. The results show that the new method outperforms the standard method in terms of accuracy and sharp truncation of singular-value-decomposition. The proposed method has potential applications in various areas of science and engineering where the input signal is multipulse and the output is noisy.",1
1227,"oue of -lrb- . he key questions to be addressed in t , liis paper is how 1.0 estimate the deterministic input multipulse time series from a noisy respouse of the resonant transfer system iii -lrb- . lie case where t.he snr is low aud the system q -lrb- quality laci.or -rrb- of ihe transfer system is high . by generaliziug the sharp truucabion employed in the standard singular-value-decomposit . ion -lrb- svd -rrb- , a tapering window is iutroduced to t ruucat.e high order non-significant singular values obtained by t.lie svd , t , lie mea : l squared error of the estimat.es due to the st , andard svd-based estimator is reduced to a miui-mum . this paper also preseuts a uew method to design au optimnm tapering window for estimatiug multipulse time series .",0
1228,"This paper presents a method for robust optical flow estimation in MPEG sequences using multiresolution techniques and a robust regularization approach. The proposed method takes into account the MPEG consistency constraint and the motion field from the previous frame, which reduces memory consumption and decompression time. The algorithm is designed to handle motion discontinuities and various motion scenarios, and it uses mathematical constraints to optimize an objective function. The resulting velocity fields are suitable for video analysis tasks such as event analysis and surveillance applications, as they exhibit temporal continuity and are computed directly from the compressed stream without requiring full decompression.",1
1229,"motion information is essential in many computer vision and video analysis tasks . since mpeg is still one of the most prevalent formats for representing , transferring and storing video data , the analysis of its motion field is important for real time video indexing and segmentation , event analysis and surveillance applications . our work considers the problem of improving the optical flow field in mpeg sequences . we address the issues of robust , incremental , dense optical flow estimation by combining information from two different velocity fields : the available mpeg motion field and the one inferred by a multiresolution robust regularization technique applied on the dc coefficients . thus , the multiresolution robust regularization technique is based only on information that is directly available in the compressed stream avoiding therefore the time and memory consuming decompression . we extend standard techniques by adding a temporal continuity and an mpeg consistency constraint , both as mathematical constraints in the objective function and as hypothesis tests for the presence of motion discontinuities . our approach is shown to perform well over a range of different motion scenarios and can serve as a basis for efficient video analysis tasks .",0
1230,"This paper presents a new transform called the fractional Gabor transform, which provides a flexible and non-rectangular time-frequency lattice for signal analysis. This transform is an extension of the Gabor transform and the fractional Fourier transform, and it can be used for constant-bandwidth analysis and non-rectangular tiling of the time-frequency plane. The basis functions of the fractional Gabor transform are shown to satisfy bi-orthogonality conditions and are constructed from Gabor logons. This transform provides a general and compact representation of signals, and it has potential applications in a variety of fields including signal processing, image processing, and communication systems.",1
1231,"we present a fractional gabor expansion on a general , non-rectangular time-frequency lattice . the traditional fractional gabor expansion represents a signal in terms of time and frequency shifted basis functions , called gabor logons . this constant-bandwidth analysis results in a fixed , rectangular time frequency plane tiling . many of the practical signals require a more flexible , non-rectangular time-frequency lattice for a compact representation . the proposed fractional gabor expansion uses a set of basis functions that are related to the fractional fourier basis and generate a non-rectangular tiling . the completeness and bi-orthogonality conditions of the new gabor logons are discussed .",0
1232,This paper proposes a fingerprint matching technique based on distance metric learning. The distance metric learning method is applied to audio fingerprinting systems to improve the identification of query content. The proposed technique uses a cost function and convex optimization to find the global minimum of the Mahalanobis distance metric. The use of distance metric learning allows for improved accuracy in identifying similar audio content. Experimental results show that the proposed approach outperforms traditional audio fingerprinting systems that do not use distance metric learning.,1
1233,"this paper considers a method for learning a distance metric in a fingerprinting system which identifies a query content by measuring the distance between its fingerprint and a fingerprint stored in a database . a metric having a general form of the mahalanobis distance is learned with the goal that the distance between fingerprints extracted from perceptually similar contents should be smaller than the distance between fingerprints extracted from perceptually dissimilar contents . the metric is learned by minimizing a cost function designed to achieve the goal . the cost function is convex , and the global minimum can be obtained using convex optimization . in our experiment , the distance metric learning is applied in an audio fingerprinting system , and it is experimentally shown that the learned distance metric improves the identification performance .",0
1234,"The paper proposes a method for learning trajectory patterns using clustering, which can be applied to solve the trajectory learning problem and automatic activity analysis. The paper evaluates different clustering methodologies and similarity measures on a set of experimental studies to identify the best approach. The clustering rate is used as the evaluation metric to compare the performance of different clustering methods. The results show that the proposed method achieves a better clustering rate than the other methods and is able to identify meaningful trajectory patterns based on motion characteristics. The paper demonstrates the effectiveness of the proposed method in practical applications and highlights its potential for further development.",1
1235,"recently a large amount of research has been devoted to automatic activity analysis . typically , activities have been defined by their motion characteristics and represented by trajectories . these trajectories are collected and clustered to determine typical behaviors . this paper evaluates different similarity measures and clustering methodologies to catalog their strengths and weaknesses when utilized for the trajectory learning problem . the clustering performance is measured by evaluating the correct clustering rate on different datasets with varying characteristics .",0
1236,"This paper proposes a new approach for achieving multiagent meta-level control in complex systems, using a reinforcement learning-based approach. Specifically, the paper focuses on the problem of multiagent tornado tracking, and demonstrates how the proposed method can be used to develop decentralized meta-control policies that coordinate the actions of multiple agents in order to achieve global optimization. The paper also discusses the adaptation process involved in the method, and presents experimental results using a 3-agent network called NetRADS. Overall, the proposed approach shows promise for improving the coordination and collaboration between agents in complex systems.",1
1237,"embedded embedded systems consisting of collaborating agents capable of interacting with their environment are becoming ubiquitous . it is crucial for these embedded systems to be able to adapt to the dynamic and uncertain characteristics of an open environment . in this paper , we argue that multiagent meta-level control is an effective way to determine when this adaptation process should be done and how much effort should be invested in adaptation as opposed to continuing with the current action plan . we describe a reinforcement learning based approach to learn decentralized meta-control policies offline . we then propose to use the learned reinforcement learning based approach as input to a global optimization algorithm to avoid conflicting meta-level decisions between coordinating agents . our initial experiments in the context of netrads , a multiagent tornado tracking application show that netrads significantly improves performance in a 3-agent network .",0
1238,"This paper proposes a method for improving speaker diarization by fusing short-term and long-term features. The short-term features, which include prosodic information, are combined with the top-ranked long-term features to improve diarization performance. The study also evaluates diarization-independent speaker-discriminability and shows that the combination of short-term and long-term features leads to better accuracy in speaker discriminability. The proposed approach is evaluated on NIST speaker diarization tasks and achieves lower diarization error rates compared to using either short-term or long-term features alone. The results suggest that fusing short-term and long-term features can improve the accuracy of speaker diarization systems.",1
1239,"the following article shows how a state-of-the-art speaker di-arization system can be improved by combining traditional short-term features with prosodic and other long-term features . first , we present a framework to study the speaker discriminability of 70 different long-term features . then , we show how the top-ranked long-term features can be combined with short-term features to increase the accuracy of speaker diarization . the results were measured on standardized data sets -lrb- nist rt -rrb- and show a consistent improvement of about 30 % relative in diarization error rate compared to the best speaker di-arization system presented at the nist evaluation in 2007 . this result was also verified on a wide set of meetings , which we call combdev , that contains 21 meetings from previous evaluations . since the prosodic and long-term features were selected using a diarization-independent speaker-discriminability study , we are confident that the same features are able to improve other systems that perform similar tasks",0
1240,"This paper presents performance bounds for channel tracking algorithms in MIMO systems. The study considers the frequency-domain tracking scheme in the presence of additive white Gaussian noise. The channel impulse response filter coefficients are assumed to be known, and the performance bounds are derived for different channel tracking schemes. The analysis provides insights into the limitations of channel tracking algorithms, and can be used to guide the design of more effective schemes.",1
1241,"in this paper we derive performance bounds for tracking time-varying ofdm multiple-input multiple-output -lrb- mimo -rrb- communication channel in the presence of additive white gaussian noise . we discuss two channel tracking schemes . the first tracks the filter coefficients directly in time-domain , while the second separately tracks each tone in the frequency-domain . the channel tracking schemes , with known channel statistics , is utilized for evaluating the performance bounds . it is shown that the channel tracking schemes , which exploits the sparseness of the channel impulse response , outperforms the compu-tationally more efficient , frequency-domain tracking scheme , which does not exploit the smooth frequency response of the channel .",0
1242,"This paper proposes a novel approach for video event detection and summarization using audio, visual, and text saliency. The method involves a multimodal saliency curve that is generated by combining spatiotemporal attention model, multifrequency waveform modulations, and part-of-speech tagging to extract important cues from the video stream. A bottom-up video summarization algorithm is then used to create a summary of the most important events in the video based on the saliency curve. The method also uses nonlinear operators for energy tracking and color refinement cues to improve the accuracy of the detection of perceptually important video events. The proposed approach is evaluated on several datasets and achieves state-of-the-art performance in video summarization and detection of important events.",1
1243,"detection of perceptually important video events is formulated here on the basis of saliency models for the audio , visual and textual information conveyed in a video stream . audio saliency is assessed by cues that quantify multifrequency waveform modulations , extracted through nonlinear operators and energy tracking . visual saliency is measured through a spatiotemporal attention model driven by intensity , color and motion . text saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions . the various modality curves are integrated in a single attention curve , where the presence of an event may be signiſed in one or multiple domains . this multimodal saliency curve is the basis of a bottom-up video summarization algorithm , that reſnes results from unimodal or audiovisual-based skimming . the bottom-up video summarization algorithm performs favorably for video summarization in terms of informativeness and enjoyability .",0
1244,This paper proposes an Optimal Efficient Learning Equilibrium (OELE) for games with incomplete information and imperfect monitoring. The paper focuses on symmetric games and introduces the concept of efficient learning equilibrium as a natural solution concept. The proposed learning algorithm is designed to minimize the expected deviations from this equilibrium strategy. The paper demonstrates that the OELE is a strict refinement of the traditional correlated equilibrium and provides polynomial-time algorithms for computing the OELE. The paper concludes with experiments that show the efficiency of the OELE in a variety of settings.,1
1245,"efficient learning equilibrium -lrb- efficient learning equilibrium -rrb- is a natural solution concept for multi-agent encounters with incomplete information . it requires the learning algorithms themselves to be in equilibrium for any game selected from a set of -lrb- initially unknown -rrb- games . in an optimal efficient learning equilibrium , the learning algorithms would efficiently obtain the surplus the agents would obtain in an optimal nash equilibrium of the initially unknown game which is played . the crucial part is that in an ele deviations from the learning algorithms would become non-beneficial after polynomial time , although the game played is initially unknown . while appealing conceptually , the main challenge for establishing learning algorithms based on this concept is to isolate general classes of games where an efficient learning equilibrium exists . unfortunately , it has been shown that while an efficient learning equilibrium exists for the setting in which each agent can observe all other agents ' actions and payoffs , an efficient learning equilibrium does not exist in general when the other agents ' payoffs can not be observed . in this paper we provide the first positive results on this problem , constructively proving the existence of an optimal efficient learning equilibrium for the class of symmetric games where an agent can not observe other agents ' payoffs .",0
1246,"This paper introduces a method for generating generic, distributed sentence encoders called Skip-Thought Vectors. The method is based on an encoder-decoder model that is trained to predict the surrounding sentences in a sequence. The resulting sentence representations are shown to capture semantic and syntactic properties and can be used for a variety of tasks such as image-sentence ranking, question-type classification, paraphrase detection, and semantic relatedness. The paper also describes a vocabulary expansion method and linear models for using the encoded passages in downstream tasks. Experimental results demonstrate the effectiveness of the proposed approach.",1
1247,"we describe an approach for unsupervised learning of a generic , distributed sentence encoder . using the continuity of text from books , we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage . sentences that share semantic and syntactic properties are thus mapped to similar vector representations . we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words . after training our model , we extract and evaluate our vectors with linear models on 8 tasks : semantic relatedness , paraphrase detection , image-sentence ranking , question-type classification and 4 benchmark sentiment and subjectivity datasets . the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice . we will make our encoder publicly available .",0
1248,"This paper presents an improved parser for data-oriented lexical-functional analysis (LFG). The parser aims to improve parse accuracy by utilizing discounted frequency estimator and relative frequency estimator. Additionally, it employs Monte Carlo search to enhance fragment size selection during parsing. The parser was evaluated on LFG-annotated sentences from VerbMobil and HomeCentre corpora, and the results showed significant improvement in LFG-DOP and tree-DOP accuracy over previous approaches. Overall, this study demonstrates the effectiveness of the proposed LFG-DOP parser for improving parsing accuracy of LFG tree structures.",1
1249,we present an lfg-dop parser which uses fragments from lfg-annotated sentences to parse new sentences . experiments with the verbmobil and homecentre corpora show that -lrb- 1 -rrb- viterbi n best search performs about 100 times faster than monte carlo search while both achieve the same accuracy ; -lrb- 2 -rrb- the dop hypothesis which states that parse accuracy increases with increasing fragment size is confirmed for lfg-dop ; -lrb- 3 -rrb- lfg-dop 's relative frequency estimator performs worse than a discounted frequency estimator ; and -lrb- 4 -rrb- lfg-dop significantly outperforms tree-dop if evaluated on tree structures only .,0
1250,"This paper proposes a model-based speech enhancement method using a modified Kalman filter approach and a signal-to-noise ratio (SNR) dependent minimum mean square error (MMSE) estimator. The method takes advantage of the temporal correlation of successive frames in speech signals to estimate the SNR and use it to adapt the MMSE estimator. By predicting the speech signal based on past frames and using the prediction error signal, the method can efficiently remove noise from speech signals in a single channel. The performance of the proposed method is evaluated using objective measurements, and the results demonstrate the effectiveness of the proposed frequency domain SNR estimators.",1
1251,"this contribution presents a modified kalman filter approach for single channel speech enhancement which is operating in the frequency domain . in the first step , temporal correlation of successive frames is exploited yielding estimates of the current speech and noise dft coefficients . this first prediction is updated in the second step applying an snr dependent mmse estimator which is adapted to the -lrb- measured -rrb- statistics of the speech prediction error signal . objective measurements show consistent improvements compared to estima-tors which do not take into account the temporal correlation or the influence of the input snr on the statistics of the prediction error signal .",0
1252,"The paper focuses on the problem of detecting TV commercials in a real video stream. The main approach involves identifying whether a shot belongs to a commercial or program segment, and then labeling the shots accordingly. The authors propose a Hidden Markov Model (HMM) approach with a Viterbi decoder to detect the commercials. They use features such as shot duration and logo presence to train the model. The results show that the proposed method is effective in detecting TV commercials.",1
1253,this paper presents a system that labels tv shots either as commercial or program shots . the system uses two observations : logo presence and shot duration . this observations are modeled using hmm and the viterbi decoder is finally used for shot labeling . the system has been tested on several hours of real video achieving more than 99 % of correct labeling .,0
1254,"This paper presents discriminative classifiers for deterministic dependency parsing using support vector machines (SVMs) and treebank-induced classifiers. The classifiers are trained on feature models of parsing models and evaluated for parsing accuracy. The proposed approach is applied to data-driven parsing for Chinese, Swedish, and English languages. The results show that the classifier-based deterministic parsing approach outperforms other methods in terms of parsing accuracy. The study also demonstrates the effectiveness of SVMs and treebank-induced classifiers for deterministic dependency parsing.",1
1255,"deterministic parsing guided by treebank-induced classifiers has emerged as a simple and efficient alternative to more complex models for data-driven parsing . we present a systematic comparison of memory-based learning -lrb- mbl -rrb- and support vector machines for inducing classifiers for deterministic dependency parsing , using data from chinese , english and swedish , together with a variety of different feature models . the comparison shows that support vector machines gives higher accuracy for richly articulated feature models across all languages , albeit with considerably longer training times . the results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models .",0
1256,This paper proposes a semi-supervised boosting approach to improve statistical word alignment using both labeled and unlabeled data. The limited availability of labeled data for training often results in high error rates in statistical word alignment. The proposed approach uses a pseudo reference set derived from the unlabeled data to train a supervised boosting algorithm and then applies a semi-supervised learning algorithm to further improve the alignment accuracy. The paper evaluates the approach using relative error reductions on a word alignment task and shows that the semi-supervised boosting approach outperforms both the supervised and unsupervised boosting methods. The proposed approach is shown to be effective in using both labeled and unlabeled data to improve the performance of statistical word alignment.,1
1257,"this paper proposes a semi-supervised boosting approach to improve statistical word alignment with limited labeled data and large amounts of unlabeled data . the proposed semi-supervised boosting approach modifies the supervised boosting algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data . in this semi-supervised boosting approach , we build a word aligner by using both the labeled data and the unlabeled data . then we build a pseudo reference set for the unlabeled data , and calculate the error rate of each word aligner using only the labeled data . based on this supervised boosting algorithm , we investigate two boosting methods for statistical word alignment . in addition , we improve the statistical word alignment results by combining the results of the two semi-supervised boosting methods . experimental results on statistical word alignment indicate that semi-supervised boosting achieves relative error reductions of 28.29 % and 19.52 % as compared with supervised boosting and unsupervised boosting , respectively .",0
1258,"This paper presents an efficient Monte-Carlo algorithm for pricing combinatorial prediction markets for tournaments. The algorithm employs importance sampling to achieve additive error bounds and a multiplicative error factor for combinatorial prediction markets. The method can handle disjunctive normal form (DNF) formulas of polynomial size efficiently, by transforming them to equivalent conjunctive normal form (CNF) formulas. The proposed algorithm has a time polynomial in the number of variables and uses the #P counting class. The experimental results show that the Monte-Carlo technique with importance sampling is more efficient than other methods for pricing combinatorial prediction markets. The proposed algorithm can be used in a wide range of applications, such as sports tournaments, political elections, and stock markets.",1
1259,"computing the market maker price of a security in a combinatorial prediction market is #p - hard . we devise a fully polynomial randomized approximation scheme -lrb- fpras -rrb- that computes the price of any security in disjunctive normal form within an multiplicative error factor in time polynomial in 1 / / and the size of the input , with high probability and under reasonable assumptions . our algorithm is a monte-carlo technique based on importance sampling . the algorithm can also approximately price securities represented in conjunctive normal form with additive error bounds . to illustrate the applicability of our algorithm , we show that many securities in yahoo! 's popular combinatorial prediction market game called conjunctive normal form can be represented by dnf formulas of polynomial size .",0
1260,"This paper proposes an approach for word sense disambiguation tasks using a naive Bayes similarity measure. The approach utilizes a general-purpose naive Bayes model to estimate probabilities and employs an overlap mechanism to compute the similarity measure between the target word and its potential senses. The maximum likelihood estimate is used to determine the sense of the target word. The proposed approach is evaluated using the Lesk algorithm and WordNet lexical knowledge. The results demonstrate that the general-purpose naive Bayes model outperforms the Lesk algorithm in terms of accuracy, and the approach is effective for word sense disambiguation tasks.",1
1261,"we replace the overlap mechanism of the lesk algorithm with a simple , general-purpose naive bayes model that measures many-to-many association between two sets of random variables . even with simple probability estimates such as maximum likelihood , the general-purpose naive bayes model gains significant improvement over the lesk algorithm on word sense disambiguation tasks . with additional lexical knowledge from word-net , performance is further improved to surpass the state-of-the-art results .",0
1262,"This paper presents a knowledge compilation map for ordered real-valued decision diagrams (VDDs). VDDs are a data structure used to represent real-valued functions, and they can be used for various tasks such as computing probabilities, cost functions, utility functions, and marginalizations. The knowledge compilation map presented in this paper provides a systematic way of transforming VDDs using various operations such as cuts and marginalizations to reduce their complexity. The resulting VDDs can be used for solving constraint satisfaction problems (CSPs) efficiently. The paper also discusses how the knowledge compilation map can be used to compile VDD languages for non-negative real numbers and probability distributions. Overall, this work provides a useful tool for researchers and practitioners working with VDDs and related data structures.",1
1263,"valued decision diagrams -lrb- vdd languages -rrb- are data structures that represent functions mapping variable-value assignments to non-negative real numbers . they prove useful to compile cost functions , utility functions , or probability distributions . while the complexity of some queries -lrb- notably optimization -rrb- and transformations -lrb- notably conditioning -rrb- on vdd languages has been known for some time , there remain many significant queries and transformations , such as the various kinds of cuts , marginalizations , and combinations , the complexity of which has not been identified so far . this paper contributes to filling this gap and completing previous results about the time and space efficiency of vdd languages , thus leading to a knowledge compilation map for real-valued functions . our results show that many tasks that are hard on valued csps are actually tractable on vdd languages .",0
1264,"This paper proposes an incremental topic-based translation model adaptation approach for conversational spoken language translation (CSLT). CSLT involves the translation of spoken conversations, which poses unique challenges due to the causal constraints of spoken conversations. To address this challenge, the authors propose a translation model adaptation approach that leverages a monolingual LDA topic model to represent the topic distribution of CSLT. The proposed approach enables up-front knowledge of the topic distribution, which is incrementally updated during translation. The authors evaluate their approach on an English-to-Iraqi CSLT task, using similarity measures such as BLEU, NIST, and TER. The results show that their incremental approach outperforms non-incremental oracle approaches and can effectively adapt to the topic distribution of spoken conversations, improving the translation quality of CSLT.",1
1265,"we describe a translation model adaptation approach for conversational spoken language translation , which encourages the use of contextually appropriate translation options from relevant training conversations . our translation model adaptation approach employs a monolingual lda topic model to derive a similarity measure between the test conversation and the set of training conversations , which is used to bias translation choices towards the current context . a significant novelty of our translation model adaptation approach is its incremental nature ; we continuously update the topic distribution on the evolving test conversation as new utterances become available . thus , our translation model adaptation approach is well-suited to the causal constraint of spoken conversations . on an english-to-iraqi cslt task , the proposed translation model adaptation approach gives significant improvements over a baseline system as measured by bleu , ter , and nist . interestingly , the translation model adaptation approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation .",0
1266,"This paper proposes a joint optimization method for signature sequences and power allocation in Code Division Multiple Access (CDMA) systems. The problem of designing signature sequences and power allocation policies subject to a sum power constraint is considered. The paper presents a novel approach to jointly optimize the signature sequences and power allocation in order to maximize both the user capacity and the information-theoretic capacity of the system. The impact of correlated signals and colored noise on the joint optimization problem is also analyzed. The proposed method addresses power allocation design problems for CDMA systems, and aims to find the jointly optimal solution for both signature sequences and power allocation in the presence of correlated signals and colored noise. The results show that the proposed joint optimization method achieves higher user and information-theoretic capacities compared to existing methods.",1
1267,"the problems of designing signature sequences and power allocation policy for code-division multiple access -lrb- cdma -rrb- are important and have been the subject of intensive research in recent years . two different criteria adopted in such design problems are the user capacity and the information-theoretic capacity . regarding the maxi-mization of the information-theoretic capacity , most of the previous works only consider the optimizations of signature sequences and power allocation separately . in contrast , this paper presents a jointly optimal design of signature sequences and power allocation under the sum power constraint . the proposed design is of closed-form and applicable for the general case of correlated signals and colored noise . numerical results verify the superiority of the proposed design over the existing ones .",0
1268,"This paper presents a novel approach called ForgetMeNot for memory-aware forensic facial sketch matching. The goal of this work is to address the modality gap and forgetting process that can arise in automated facial forensic sketch matching. The proposed method utilizes a well-studied sketch-photo matching algorithm and adapts it to the forensic sketch recognition domain. To evaluate the effectiveness of the ForgetMeNot approach, the authors used a 10,030 mugshot database and conducted experiments to measure the performance of forensic sketch recognition with and without memory-aware matching. The results show that the proposed approach significantly improves the recognition accuracy, thus demonstrating its potential for practical use in law enforcement.",1
1269,"we investigate whether it is possible to improve the performance of automated facial forensic sketch matching by learning from examples of facial forgetting over time . forensic facial sketch recognition is a key capability for law enforcement , but remains an unsolved problem . it is extremely challenging because there are three distinct contributors to the domain gap between forensic sketches and photos : the well-studied sketch-photo modality gap , and the less studied gaps due to -lrb- i -rrb- the forgetting process of the eye-witness and -lrb- ii -rrb- their inability to elucidate their memory . in this paper , we address the memory problem head on by introducing a database of 400 forensic sketches created at different time-delays . based on this database we build a model to reverse the forgetting process . surprisingly , we show that it is possible to systematically '' un-forget '' facial details . moreover , it is possible to apply this model to dramatically improve forensic sketch recognition in practice : we achieve the state of the art results when matching 195 benchmark forensic sketches against corresponding photos and a 10,030 mugshot database .",0
1270,"This paper proposes a novel approach called Convex Neural Networks (ConvexNN) to address the challenges associated with traditional multi-layer artificial neural networks in the machine learning community. The proposed approach aims to reformulate the learning problem as a convex optimization problem, which enables the use of existing optimization techniques to obtain globally optimal solutions. The paper discusses how this reformulation can be achieved, and shows that ConvexNN has the properties of convexity that make it more amenable to analysis than traditional neural networks. The proposed approach also allows for the use of hidden unit activation functions that are linear, which simplifies the learning algorithm. The paper concludes that ConvexNN offers a new perspective on neural network design and has the potential to improve the efficiency and accuracy of machine learning tasks.",1
1271,"convexity has recently received a lot of attention in the machine learning community , and the lack of convexity has been seen as a major disadvantage of many learning algorithms , such as multi-layer artificial neural networks . we show that training multi-layer artificial neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem . this convex optimization problem involves an infinite number of variables , but can be solved by incrementally inserting a hidden unit at a time , each time finding a linear classifier that minimizes a weighted sum of errors .",0
1272,"This paper proposes a new approach to solve stochastic optimal control problems using a model-free, non-parametric approach called Path Integral Control by Reproducing Kernel Hilbert Space Embedding. The approach aims to combine the sample efficiency of sample-based approaches with the sample re-use of invariant and task-dependent components, resulting in improved performance. The method represents control problems in the form of a path integral, and uses an embedding in a Reproducing Kernel Hilbert Space to enable the use of invariant and task-dependent components. The approach is evaluated using sample-based approaches and shows promising results in terms of sample efficiency and performance.",1
1273,"we present an embedding of stochastic optimal control problems , of the so called path integral form , into reproducing kernel hilbert spaces . using consistent , sample based estimates of the embedding leads to a model-free , non-parametric approach for calculation of an approximate solution to the control problem . this model-free , non-parametric approach admits a decomposition of the problem into an invariant and task dependent component . consequently , we make much more efficient use of the sample data compared to previous sample based approaches in this domain , e.g. , by allowing sample re-use across tasks . numerical examples on test problems , which illustrate the sample efficiency , are provided .",0
1274,"This paper proposes an automatic image colorization method using multimodal predictions. The approach involves using a dataset of colored examples, machine learning tools, and a non-uniform spatial coherency criterion in the L-a-b color space. The proposed method is based on estimating the probability distribution of the colors for each pixel in the image at the local level, using local texture and noise, and then combining these estimates at the global level to obtain the final colorization. The method does not require manual intervention and can be used to colorize grayscale images. The method involves using user-provided color landmarks to establish a one-to-one correspondence between the grayscale image and the colored image. The colorization is obtained by estimating the color of each pixel using graph cuts based on a proposition of the colored image. The proposed method outperforms previous methods in terms of visual quality and accuracy, making it a useful tool for automatic image colorization.",1
1275,"we aim to color greyscale images automatically , without any manual intervention . the color proposition could then be interactively corrected by user-provided color landmarks if necessary . automatic col-orization is nontrivial since there is usually no one-to-one correspondence between color and local texture . the contribution of our framework is that we deal directly with multimodality and estimate , for each pixel of the image to be colored , the probability distribution of all possible colors , instead of choosing the most probable color at the local level . we also predict the expected variation of color at each pixel , thus defining a non-uniform spatial coherency criterion . we then use graph cuts to maximize the probability of the whole colored image at the global level . we work in the l-a-b color space in order to approximate the human perception of distances between colors , and we use machine learning tools to extract as much information as possible from a dataset of colored examples . the resulting algorithm is fast , designed to be more robust to texture noise , and is above all able to deal with ambiguity , in contrary to previous approaches .",0
1276,"This paper proposes a Weighted Likelihood Ratio (WLR) Hidden Markov Model (HMM) for improving the performance of speech recognition in the presence of noise. The approach takes into account natural resonances of the vocal tract to model speech spectra, and employs multiple spectral peaks and valleys for extracting dynamic cepstral features. The output probability density function of the model is represented using an exponential kernel, and the WLR-HMM framework is used to estimate the likelihood ratio for each speech frame. The proposed approach is evaluated on the Aurora2 connected digits database in the presence of broad-band noise interferences. Results show that the WLR-HMM approach outperforms the baseline system, which is trained using Gaussian mixture models and Mel-frequency cepstral coefficients.",1
1277,"in this paper we present a weighted likelihood ratio -lrb- wlr -rrb- based hidden markov model and apply it to speech recognition in noise . the wlr measure emphasizes spectral peaks than valleys in comparing two given speech spectra . the measure is more consistent with human perception of speech formants where natural resonances of vocal track are and tends to be more robust to broad-band noise interferences than other measures . a complete hmm framework of this measure is derived and a mixture of exponential kernels is used to model the output probability density function . the new wlr-hmm is tested on the aurora2 connected digits database in noise . it shows more robust performance than the mfcc trained gmm baseline system . when combined with the dynamic cepstral features , the multiple-stream wlr-hmm shows a 39 % relative improvement over the baseline system .",0
1278,"This paper presents the RWTH LVCSR systems for four different languages, namely German, Polish, Spanish, and Portuguese, which were developed for the Quaero and EU-Bridge projects. The systems utilize various techniques such as language model adaptation, minimum phone error trained acoustic models, and multilingual bottleneck features to achieve state-of-the-art performance in the evaluations. Moreover, confusion-network based system combination is employed to combine multiple systems and further improve the results. The systems are evaluated on various tasks such as broadcast news, lecture domain, and podcasts, and open vocabulary approach is used for the podcasts. The paper also discusses the acoustic conditions and language-specific challenges faced during the development of the systems, such as using morphemic units for German LVCSR and adapting the language model for spontaneous speech.",1
1279,"in this paper , german , polish , spanish , and portuguese large vocabulary continuous speech recognition -lrb- lvcsr -rrb- systems developed by the rwth aachen university are presented . all the above mentioned systems for the aforementioned languages are used for the quaero and eu-bridge project evaluations . the lvcsr systems developed for these competitive evaluations focus on various domains like broadcast news , podcasts and lecture domain . transcription of the speech for these tasks is challenging due to huge variability in the acoustic conditions and a significant portion of audio data includes spontaneous speech . good improvements are obtained using state-of-the-art multilingual bottleneck features , minimum phone error trained acoustic models , language model adaptation and confusion-network based system combination . in addition , an open vocabulary approach using morphemic units is investigated along with the lm adaptation for the german lvcsr .",0
1280,"This paper investigates the consistency assumption in forced alignment, particularly in the context of pronunciation variation in read and spontaneous speech synthesis. The authors propose a lattice-based forced alignment system and a front-end text processing system to improve phoneme identity accuracy. They use HMM-based voices and acoustic models for read and spontaneous speech synthesis, and evaluate the performance of their system on phoneme sequence accuracy. Results indicate that the consistency assumption does not always hold in spontaneous speech, and that the proposed system improves accuracy in both read and spontaneous speech synthesis. The paper highlights the importance of considering pronunciation variation in speech synthesis and the potential impact on the consistency assumption.",1
1281,"forced alignment for speech synthesis traditionally aligns a phoneme sequence predetermined by the front-end text processing system . this sequence is not altered during alignment , i.e. , it is forced , despite possibly being faulty . the consistency assumption is the assumption that these mistakes do not degrade models , as long as the mistakes are consistent across training and synthesis . we present evidence that in the alignment of both standard read prompts and spontaneous speech this phoneme sequence is often wrong , and that this is likely to have a negative impact on acoustic models . a lattice-based forced alignment system allowing for pronunciation variation is implemented , resulting in improved phoneme identity accuracy for both types of speech . a perceptual evaluation of hmm-based voices showed that spontaneous models trained on this improved alignment also improved standard synthesis , despite breaking the consistency assumption .",0
1282,This paper presents a new image registration method based on a combination of feature-based and intensity-based matching. The proposed method uses a projective model to account for geometrical variation and a smooth spatially varying illumination variation model to address illumination changes. The projective transformation parameters and illumination model parameters are jointly estimated by optimizing a cost function that integrates both feature-based and intensity-based matching. The proposed method achieves high accuracy and robustness in registration by exploiting both sources of information. Experimental results demonstrate the effectiveness of the proposed approach.,1
1283,"image registration is one of the most important tasks in image processing . the algorithms of image registration are classified into two categories : the feature-based matching and intensity-based matching . each of them has its strength and weakness . in this paper , by combining these two techniques together , we developed a new algorithm for image registration . the algorithm utilises a parametric projective model accounting for geometrical variation and a polynomial model with a small number of polynomial coefficients explicating the smooth spatially varying illumination variation . the initial projective model parameters are first estimated by using feature-based approach . subsequently , the coefficients of the illumination model are determined simultaneously with the projective transformation parameters through the process of intensity matching . the experimental results demonstrated the algorithm is of robustness , efficiency and accuracy .",0
1284,"This paper presents a mean-field approach to a probabilistic model in information retrieval. The model uses an explicit parametric form for the relevance function and employs a cavity method to approximate the leave-one-out cross-validation procedure. The approach also considers hyperparameters and employs mean-field methods to estimate their values. The proposed method is evaluated on a set of documents and queries using rel-evancy assessment. The results show that the mean-field approach provides accurate estimates of the relevance function and outperforms existing methods for information retrieval. The study also demonstrates the importance of hyperparameters in the model and their influence on the retrieval performance. Overall, the paper provides a novel approach to information retrieval that incorporates a range of techniques including cavity method, leave-one-out cross-validation procedure, and mean-field methods.",1
1285,"we study an explicit parametric model of documents , queries , and rel-evancy assessment for information retrieval . mean-field methods are applied to analyze the model and derive efficient practical algorithms to estimate the parameters in the problem . the hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method . the approximate leave-one-out cross-validation procedure is further evaluated on several benchmark databases by comparing with standard algorithms in ir .",0
1286,"This paper proposes a method for low-resource speech recognition that utilizes automatically learned sparse inverse covariance matrices. The approach uses the maximum likelihood estimation with the graphical lasso method to learn the sparse inverse covariance matrix from limited training data. The proposed method is compared to full covariance methods and shows improved performance with unseen test data. The approach also incorporates l1 regularization in the objective function to control the number of free parameters in the model. The study evaluates the proposed method on full covariance acoustic models using sparse inverse covariance methods and shows that it outperforms full covariance methods with limited data. The results demonstrate that the proposed method is effective in dealing with limited training data and provides a viable solution for low-resource speech recognition tasks. Overall, the paper presents a novel approach to low-resource speech recognition that leverages sparse inverse covariance matrices and maximum likelihood estimation with the graphical lasso method.",1
1287,full covariance acoustic models trained with limited training data generalize poorly to unseen test data due to a large number of free parameters . we propose to use sparse inverse co-variance matrices to address this problem . previous sparse inverse covariance methods never outperformed full covari-ance methods . we propose a method to automatically drive the structure of inverse covariance matrices to sparse during training . we use a new objective function by adding l1 reg-ularization to the traditional objective function for maximum likelihood estimation.the graphic lasso method for the estimation of a sparse inverse covariance matrix is incorporated into the expectation maximization algorithm to learn parameters of hmm using the new objective function . experimental results show that we only need about 25 % of the parameters of the inverse covariance matrices to be nonzero in order to achieve the same performance of a full covariance system . our proposed system using sparse inverse covariance gaus-sians also significantly outperforms a system using full co-variance gaussians trained on limited data .,0
1288,"This paper presents a motion-based object segmentation technique using local background sprites. The approach utilizes both global motion estimation and local background sprite generation techniques to segment foreground objects in video sequences. The proposed method incorporates background modeling and background subtraction techniques to effectively segment moving objects from a static or moving background. In addition, video material preprocessing is used to enhance the performance of the segmentation algorithm. The approach is evaluated on various sequences with different types of backgrounds and motion, and the results demonstrate its effectiveness in segmenting foreground objects. The local background sprite generation technique is found to be particularly useful for improving the background modeling technique and enhancing the segmentation of static backgrounds. Overall, the proposed method offers a promising solution for accurate and efficient object segmentation in video sequences.",1
1289,"it is well known that video material with a static background allows easier segmentation than that with a moving background . one approach to segmentation of sequences with a moving background is to use preprocessing to create a static background , after which conventional background subtraction techniques can be used for segmenting foreground objects . it has been recently shown that global motion estimation and/or background sprite generation techniques are reliable . we propose a new background modeling technique for object segmentation using local background sprite generation . experimental results show the excellent performance of this new background modeling technique compared to recent algorithms proposed .",0
1290,"This paper proposes a new approach to opinion dynamics with bounded confidence in the Bayes risk error divergence sense. The approach utilizes bounded confidence opinion dynamic models to account for the effects of limited confidence in decision-making systems. Bayesian decision makers are used to perform hypothesis testing tasks and signal detection, while taking into account the intermediate signal-to-noise ratios. The approach is evaluated in social networks with different priors, limiting values, and clusters. The proposed method shows promising results in improving information propagation and opinion dynamics in social networks. The approach is found to be effective in enhancing the performance of the decision-making system and improving the accuracy of hypothesis testing and signal detection tasks. The proposed approach provides a new perspective on opinion dynamics with bounded confidence in the Bayes risk error divergence sense, which can have practical applications in various fields such as social sciences and engineering.",1
1291,"bounded confidence opinion dynamic models have received much recent interest as models of information propagation in social networks and localized distributed averaging . however in the existing literature , opinions are only viewed as abstract quantities rather than as part of a decision-making system . in this work , opinion dynamics are examined when agents are bayesian decision makers that perform hypothesis testing or signal detection . bounded confidence is defined on prior probabilities of hypotheses through bayes risk error divergence , the appropriate measure between priors in hypothesis testing . this definition contrasts with the measure used between opinions in the standard model : absolute error . it is shown that the rapid convergence of prior probabilities to a small number of limiting values is similar to that seen in the standard model . the most interesting finding in this work is that the number of these limiting values changes with the signal-to-noise ratio in the hypothesis testing task . the number of final values or clusters is maximal at intermediate signal-to-noise ratios , suggesting that the most contentious issues lead to the largest number of factions .",0
1292,"This paper proposes a randomized-feature approach for learning kernels in machine learning tasks. The approach utilizes randomized features to approximate user-defined kernels, allowing for a more efficient optimization problem during training. The proposed method is evaluated in a supervised manner, and generalization bounds are derived for the resulting class of estimators. The approach is found to be effective in improving the performance of kernel machines and achieving better generalization bounds. The use of randomized features offers a promising solution for reducing the training cost of kernel machines while maintaining their accuracy. The proposed approach is applicable in various machine learning tasks and provides a new perspective on kernel learning. Overall, the randomized-feature approach for learning kernels offers a powerful and efficient tool for practitioners in the field of machine learning.",1
1293,"randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks . however , such methods require a user-defined kernel as input . we extend the randomized-feature approach to the task of learning a kernel -lrb- via its associated random features -rrb- . specifically , we present an efficient optimization problem that learns a kernel in a supervised manner . we prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel , and we experimentally evaluate our technique on several datasets . our approach is efficient and highly scalable , and we attain competitive results with a fraction of the training cost of other techniques .",0
1294,"This paper presents an implementation of a combined partial parser and morphosyntactic disambiguator. The proposed approach utilizes a partial parsing formalism to improve the accuracy of simultaneous rule-based morphosyntactic tagging. The approach is evaluated using the IPI PAN Corpus of Polish and a tree-bank of partial parses. The results demonstrate that the combined partial parser and morphosyntactic disambiguator improves the accuracy of morphosyntactic tagging compared to the use of rule-based tagging alone. The approach offers a promising solution for improving the accuracy of natural language processing tasks, particularly in languages with complex morphologies such as Polish. The proposed implementation provides a valuable tool for researchers and practitioners in the field of natural language processing, and its effectiveness can be extended to other languages with similar morphologies.",1
1295,the aim of this paper is to present a simple yet efficient implementation of a tool for simultaneous rule-based morphosyntactic tagging and partial parsing formalism . the parser is currently used for creating a tree-bank of partial parses in a valency acquisition project over the ipi pan corpus of polish .,0
1296,"This paper proposes a new classifier called the Minimax Probability Machine, which aims to minimize the misclassification of future data by constructing nonlinear decision boundaries using Mercer kernels. The optimization problem for the proposed classifier is formulated as a convex optimization problem, which can be efficiently solved using standard optimization algorithms. The proposed approach offers a promising solution for improving the performance of classifiers, particularly in scenarios where future data may differ significantly from the training data. The Minimax Probability Machine is shown to outperform existing classifiers on several benchmark datasets, demonstrating its effectiveness in achieving high classification accuracy. Overall, the proposed approach provides a valuable tool for practitioners in the field of machine learning and offers a new perspective on classifier design. The Minimax Probability Machine offers a powerful and efficient tool for achieving high accuracy in classification tasks.",1
1297,"when constructing a classifier , the probability of correct classification of future data points should be maximized . in the current paper this classifier is translated in a very direct way into an optimization problem , which is solved using methods from convex optimization . we also show how to exploit mercer kernels in this setting to obtain nonlinear decision boundaries . a worst-case bound on the probability of misclassification of future data is obtained explicitly .",0
1298,"This paper presents CHAT, a conversational helper designed to assist users in completing automotive tasks through spoken dialogue interfaces. The proposed system is built on top of computing platforms commonly found in modern cars and aims to provide an intuitive and efficient user experience for completing automotive tasks. The system is evaluated using several benchmark automotive tasks, and the results show that CHAT significantly improves the task completion rate compared to existing dialog systems. Additionally, the system is shown to improve dialog efficiency, providing a more natural and seamless interaction with the user. Overall, the proposed conversational helper offers a valuable tool for improving the user experience of automotive tasks and provides a promising direction for future research in this area. The success of CHAT highlights the potential for conversational interfaces to revolutionize the way we interact with technology in our daily lives.",1
1299,"spoken dialogue interfaces , mostly command-and-control , become more visible in applications where attention needs to be shared with other tasks , such as driving a car . the deployment of the simple dialog systems , instead of more sophisticated ones , is partly because the computing platforms used for such tasks have been less powerful and partly because certain issues from these cognitively challenging tasks have not been well addressed even in the most advanced dialog systems . this paper reports the progress of our research effort in developing a robust , wide-coverage , and cognitive load-sensitive spoken dialog interface called chat : conversational helper for automotive tasks . our research in the past few years has led to promising results , including high task completion rate , dialog efficiency , and improved user experience .",0
1300,"This paper proposes a method for improving the accuracy of automatic speech summarisation by incorporating topic and stylistic adaptation. The proposed method is evaluated on the TED corpus of Eurospeech conference presentations and CNN broadcast news data. The system consists of a linguistic model component and a speech recogniser, which are adapted to the specific topic and style of the input data. The linguistic model component is trained using transcriptions of the input data and a language model, while the speech recogniser is trained using automatically generated summaries and human summaries. The results show that the proposed method significantly improves the accuracy of summarised text compared to contemporary approaches. The topic and stylistic adaptation data and lims used in the system demonstrate the importance of adapting to the specific characteristics of the input data to improve summarisation accuracy. The proposed method provides a promising direction for future research in automatic speech summarisation, highlighting the potential of topic and stylistic adaptation for improving the quality of automatically generated summaries.",1
1301,"contemporary approaches to automatic speech summarisation comprise several components , among them a linguistic model component , which is unrelated to the language model used during the recognition process . this linguistic model component assigns a probability to word sequences from the source text according to their likelihood of appearing in the summarised text . in this paper we investigate lim topic and stylistic adaptation using combinations of lims each trained on different adaptation data . experiments are performed on 9 talks from the ted corpus of eurospeech conference presentations , as well as 5 news stories from cnn broadcast news data , for all of which human -lrb- trs -rrb- and speech recogniser transcriptions along with human summaries were used . in all asr cases , summari-sation accuracy -lrb- sumaccy -rrb- of automatically generated summaries was significantly improved by automatic lim adaptation , with relative improvements of at least 2.5 % in all experiments .",0
1302,"This paper investigates different models for investing, focusing on stochastic and worst-case approaches. The authors consider the problem of constructing an investment strategy that maximizes returns based on stock price returns data. They use the geometric Brownian motion model as a probabilistic model for the stock prices and consider the universal portfolio management framework for constructing investment strategies. The authors analyze the performance of the investment strategies using regret bounds, which measure the difference in performance between the investment strategy and the best possible strategy. They show that regret bounds can be derived for both stochastic and worst-case models using online convex optimization and exp-concave loss functions. The results suggest that the worst-case approach can lead to more robust investment strategies, while the stochastic approach can lead to higher returns in some cases. The paper concludes that both approaches have their advantages and disadvantages, and the choice of model should depend on the specific investment problem at hand.",1
1303,"in practice , most investing is done assuming a probabilistic model of stock price returns known as the geometric brownian motion . while often an acceptable approximation , the geometric brownian motion is not always valid empirically . this motivates a worst-case approach to investing , called universal portfolio management , where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight . in this paper we tie the two approaches , and design an investment strategy which is universal in the worst-case , and yet capable of exploiting the mostly valid geometric brownian motion . our investment strategy is based on new and improved regret bounds for online convex optimization with exp-concave loss functions .",0
1304,"This paper proposes an approximate Gaussian process inference method for estimating drift functions in stochastic differential equations (SDEs). The method utilizes sparse observations of the state vector and unobserved, latent dynamics. The approach involves a piecewise linearized process and sparse Gaussian process regression, with an approximate expectation-maximization (EM) algorithm used for parameter estimation. The paper demonstrates the effectiveness of the method through experiments with an Ornstein-Uhlenbeck type SDE. The proposed approach provides a nonparametric approach to map estimation of the posterior over states, making it a useful tool in SDE modeling.",1
1305,"we introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector . using a gaussian process prior over the drift as a function of the state vector , we develop an approximate em algorithm to deal with the unobserved , latent dynamics between observations . the posterior over states is approximated by a piecewise linearized process of the ornstein-uhlenbeck type and the map estimation of the drift is facilitated by a sparse gaussian process regression .",0
1306,"This paper presents a study on the computational complexity of multi-way dataflow constraint problems in the context of interactive applications. The authors analyze the complexity of local propagation algorithms for solving such problems and identify the factors that contribute to their computational cost. They also discuss restrictions on these problems that can reduce their complexity, including a restriction called ""skyblue"". The results provide insights into the efficiency of algorithms for solving multi-way dataflow constraint problems and can inform the design of more efficient interactive applications.",1
1307,"although it is acknowledged that multi-way dataflow constraints are useful in interactive applications , concerns about their tractability have hindered their acceptance . certain local propagation algorithms that solve these constraints are polynomial , others -lrb- such as sky-blue -rrb- are exponential . every system handles a specific problem and the influence of any particular restriction on the computational complexity is not yet precisely determined . in this paper , we present three theoretical results that allow us to classify existing multi-way constraint problems . especially , we prove that the problem handled by skyblue is np-hard .",0
1308,"This paper proposes a lattice structure for two-band perfect reconstruction (PR) filter banks using Pade approximation. The lattice structure is based on the Padé table of the filter bank's transfer function and can be used to efficiently compute the filter coefficients in the parameter space. The lattice structure has the PR property, and the complementary filters are obtained by scaling the lattice filter coefficients with a scalar quantity. The proposed method reduces the end-to-end delay of the filter lattice and allows for efficient implementation. The effectiveness of the proposed method is demonstrated through simulations.",1
1309,"we show how the pad e table can be utilized to develop a new lattice structure for general two-channel bi-orthogonal perfect reconstruction -lrb- pr -rrb- lter banks . this is achieved through characterization of all two-channel bi-orthogonal pr lter banks . the parameter space found using this method is unique for each lter bank . similarly to any other lattice structure , the pr property is achieved structurally and quantization of the parameters of the lattice does not eeect this property . furthermore , we demonstrate that for a given lter , the set of all complementary lters can be uniquely speciied by two parameters , namely the end-to-end delay of the system and a scalar quantity .",0
1310,"This paper proposes a functional approach to sentence generation using Tree Adjoining Grammar (TAG). TAG-based generation systems have traditionally relied on a syntactic choice generation system that selects the next word in a sentence based on a limited set of syntactic possibilities. The proposed functional approach extends this by allowing for more complex features to be considered during generation, resulting in more natural-sounding sentences. The paper also introduces a new TAG-based generation system called Mumble-86, which incorporates the functional approach and achieves improved performance over existing systems. Overall, this paper provides insights into the use of TAG for sentence generation and demonstrates the advantages of a functional approach.",1
1311,"it has been hypothesized that tree adjoining grammar is particularly well suited for sentence generation . it is unclear , however , how a sentence generation system based on tree adjoining grammar should choose among the syntactic possibilities made available in the grammar . in this paper we consider the question of what needs to be done to generate with tree adjoining grammar and explain a generation system that provides the necessary features . this approach is compared with other tag-based generation systems . particular attention is given to mumble-86 which , like our generation system , makes syntactic choice on sophisticated functional grounds .",0
1312,"This paper investigates the influence of frame-asynchronous grammar scoring in a continuous speech recognition (CSR) system. The authors analyze a tree-based vocabulary search strategy that uses bigrams and unigrams for language modeling and acoustic probabilities to score the acoustic likelihood of speech frames. The theoretical explanation of grammar probabilities is discussed, and their effect on the searching procedure is examined. The study provides an insight into the importance of grammar scoring in a CSR system and shows that frame-asynchronous grammar scoring can significantly improve recognition performance.",1
1313,"it is usually assumed that grammar probabilities and acoustic probabilities in a continuous speech recognition system have to be incorporated to the general score with dierent w eights . this is an experimental fact and there is no generally accepted theoretical explanation . in this paper we propose an explanation to this fact , related to the way grammar scoring is incorporated in the searching procedure . accordingly to this explanation , we perform a set of experiments to test our hypothesis . we are also proposing a new way o f i n troducing grammarprobabilities in a tree-based vocabulary search strategy , where systems are usually bound to use the worst strategy . to apply our ideas to unigrams is rather simple . for more complex language models like bigrams we h a v e t o implement a new procedure .",0
1314,"This paper explores the effect of different color modeling and colorspace transformation approaches on skin detection. The study evaluates the performance of skin detection algorithms using different colorspaces, including Sct and HSI, and compares the results to those obtained using RGB colorspace. The illuminance component of skin color is also investigated in this study. The receiver operating characteristic curve is used to compare the performance of skin detection algorithms. The manual ground truth and human motion analysis are employed to evaluate the performance of different skin color modeling approaches. The study concludes that skin color modeling using non-RGB colorspace and colorspace transformations can improve the performance of skin detection algorithms. The histogram approach and illuminance component-based skin detection methods are effective in classifying skin pixels.",1
1315,"skin detection is an important preliminary process in human motion analysis . it is commonly performed in three steps : transforming the pixel color to a non-rgb col-orspace , dropping the illuminance component of skin color , and classifying by modeling the skin color distribution . in this paper , we evaluate the effect of these three steps on the skin detection performance . the importance of this study is a new comprehensive colorspace and color modeling testing methodology that would allow for making the best choices for skin detection . combinations of nine colorspaces , the presence of the absence of the illuminance component , and the two color model-ing approaches are compared . the performance is measured by using a receiver operating characteristic curve on a large dataset of 805 images with manual ground truth . the results reveal that -lrb- 1 -rrb- colorspace transformations can improve performance in certain instances , -lrb- 2 -rrb- the absence of the illuminance component decreases performance , and -lrb- 3 -rrb- skin color modeling has a greater impact than colorspace transformation . we found that the best performance was obtained by transforming the pixel color to the sct or hsi colorspaces , keeping the illuminance component , and modeling the color with the histogram approach .",0
1316,This paper proposes a pivot language translation-based approach to explore key concept paraphrasing for question retrieval in community-based question answering services. The authors propose a unified question retrieval model that leverages paraphrases of key concepts to enrich the concept level of the query. The approach uses pivot language translation to generate paraphrases at the word level and address word mismatch between queries and answers. The effectiveness of the proposed method is evaluated on benchmark datasets using the community-based question answering services. The results demonstrate that the proposed method outperforms the state-of-the-art methods in terms of question retrieval accuracy. The study highlights the potential of key concept paraphrasing and pivot language translation for improving the performance of question retrieval in community-based question answering services.,1
1317,"question retrieval in current community-based question answering services does not , in general , work well for long and complex queries . one of the main difficulties lies in the word mismatch between queries and candidate questions . existing solutions try to expand the queries at word level , but they usually fail to consider concept level enrichment . in this paper , we explore a pivot language translation based approach to derive the paraphrases of key concepts . we further propose a unified question retrieval model which integrates the key concepts and their paraphrases for the query question . experimental results demonstrate that the pivot language translation based approach significantly outperforms the state-of-the-art models in question retrieval .",0
1318,This paper proposes a kernel density-based acoustic model with cross-lingual bottleneck features for resource-limited large vocabulary continuous speech recognition (LVCSR) tasks. The model combines Gaussian mixture models (GMMs) and deep neural networks (DNNs) to estimate emission probabilities of hidden Markov model (HMM) states. The proposed non-parametric kernel density estimation method uses limited training data and helps improve performance in the case of limited resources. The model is evaluated on the Wall Street Journal task and discriminative score calibration is used to obtain speech class posteriors. The paper shows that the proposed model outperforms GMM and DNN models and demonstrates the effectiveness of using cross-lingual bottleneck features to improve acoustic models.,1
1319,"conventional acoustic models , such as gaussian mixture models or deep neural networks , can not be reliably estimated when there are very little speech training data , e.g. less than 1 hour . in this paper , we investigate the use of a non-parametric kernel density estimation method to predict the emission probability of hmm states . in addition , we introduce a discriminative score calibrator to improve the speech class posteriors generated by the kernel density for speech recognition task . experimental results on the wall street journal task show that the proposed acoustic models using cross-lingual bottleneck features significantly outperforms gmm and dnn models for limited training data case .",0
1320,"This paper proposes a method for extracting collective opinion targets in Chinese microblogs using unsupervised label propagation algorithms. Chinese microblogs have a unique informal writing style and are subject to length limitations, making it challenging to identify opinion targets in them. The proposed approach utilizes clustering algorithms and hashtags to group microblog messages and fine-grained word-level sentiment analysis techniques to identify opinion targets. The effectiveness of the approach is evaluated on Chinese microblogs, demonstrating the potential of unsupervised label propagation algorithms for sentiment analysis in this domain.",1
1321,"microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style . in this paper , we study the problem of extracting opinion targets of chinese microblog messages . such fine-grained word-level task has not been well investigated in chinese microblogs yet . we propose an unsupervised label propagation algorithm to address the problem . the opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets . topics in chinese microblogs are identified by hashtags or using clustering algorithms . experimental results on chinese microblogs show the effectiveness of our unsupervised label propagation algorithm and algorithms .",0
1322,"This paper proposes a method for noise-robust automatic speech recognition using sparse representation and partial least squares regression. The method maps sparse representation to state likelihoods, which are then used to improve recognition accuracy in noisy environments. The study utilizes the CHiME-6 dataset and AT-6dB SNR binary labeling system to evaluate the proposed method's effectiveness. The results demonstrate that mapping recognition exemplars to state likelihoods can significantly improve the performance of automatic speech recognition systems in noise-robust settings. The proposed approach can serve as a viable alternative to other learning-based methods in improving speech recognition performance.",1
1323,"this paper proposes learning-based methods for mapping a sparse representation of noisy speech to state likelihoods in an automatic speech recognition system . we represent speech as a sparse linear combination of exemplars extracted from training data . the weights of exemplars are mapped to speech state likelihoods using ordinary least squares -lrb- ols -rrb- and partial least squares regression . recognition experiments are conducted using the chime noisy speech database . according to the results , both learning-based methods can be successfully used for training the mapping . we achieve improvements over the previous binary labeling system , and recognition scores close to 70 % at-6 db snr .",0
1324,"This paper proposes a recursive estimation method for generative models of video, which involves clustering of Gaussian mixtures, KL approximation methods, and computationally efficient learning procedures. The approach enables fast inference and on-line learning for video clustering, which is crucial for video browsing tools. The efficiency of the method is achieved by recursively estimating the generative model and refining it through the clustering process. The proposed approach is evaluated on unsupervised video clustering tasks, and the results demonstrate the effectiveness of the method in terms of accuracy and efficiency. The paper concludes that the recursive estimation method is a promising direction for generative modeling of video.",1
1325,"in this paper we present a generative model and learning procedure for unsupervised video clustering into scenes . the work addresses two important problems : realistic mod-eling of the sources of variability in the video and fast transformation invariant frame clustering . we suggest a solution to the problem of computationally intensive learning in this generative model and learning procedure by combining the recursive model estimation , fast inference , and on-line learning . thus , we achieve real time frame clustering performance . novel aspects of this generative model and learning procedure include an algorithm for the clustering of gaussian mixtures , and the fast computation of the kl divergence between two mixtures of gaussians . the efficiency and the performance of clustering and kl approximation methods are demonstrated . we also present novel video browsing tool based on the visualization of the variables in the generative model and learning procedure .",0
1326,"This paper proposes a game theoretical algorithm for joint power and topology control in distributed Wireless Sensor Networks (WSNs). The algorithm uses game theory concepts to formulate a non-cooperative game, in which each node aims to maximize its utility. The game is designed for a fully distributed algorithm, which allows each node to operate independently without any central control. The proposed algorithm can be used for network topology control to ensure a connected network, while minimizing power consumption. Simulation results demonstrate the effectiveness of the algorithm in reducing the power consumption while maintaining a connected network topology. The proposed algorithm can be used in various applications of wireless networks.",1
1327,"in this paper , the issue of network topology control in wireless networks using a fully distributed algorithm is considered . whereas the proposed fully distributed algorithm is designed applying game theory concepts to design a non-cooperative game , network topology control is guaranteed based on asymp-totic results of network topology control . simulations show that for a relatively low node density , the probability that the proposed fully distributed algorithm leads to a connected network is close to one .",0
1328,This paper proposes an unsupervised lattice-based acoustic model adaptation approach for speaker-dependent conversational telephone speech transcription. The method uses iterative and cascaded lattice adaptation techniques to update the speaker-dependent models. The system applies thresholding on frame posteriors and then selects the local best-confidence path to construct the lattice. The unsupervised/supervised adaptation gap is evaluated for lattice adaptation. The proposed method is shown to outperform the transcript-based adaptation in terms of word error rate. The results demonstrate the effectiveness of the proposed lattice-based adaptation for speaker-dependent conversational telephone speech transcription.,1
1329,"this paper examines the application of lattice adaptation techniques to speaker-dependent models for the purpose of conversational telephone speech transcription . given sufficient training data per speaker , it is feasible to build adapted speaker-dependent models using lattice mllr and lattice map . experiments on iterative and cascaded adaptation are presented . additionally various strategies for thresholding frame posteriors are investigated , and it is shown that accumulating statistics from the local best-confidence path is sufficient to achieve optimal adaptation . overall , an iterative cascaded lattice system was able to reduce lattice adaptation by 7.0 % abs. , which was a 0.8 % abs . gain over transcript-based adaptation . lattice adaptation reduced the unsupervised/supervised adaptation gap from 2.5 % to 1.7 % .",0
1330,"This paper proposes a new algorithm called Gaussian mixture sigma-point particle filters for sequential probabilistic inference in dynamic state-space models. The algorithm uses a time-update and proposal distribution generation stage, followed by a measurement update step, and finally a resampling stage using a weighted particle set. The algorithm is based on a recursive Bayesian estimation algorithm and uses a weighted EM algorithm to compute the posterior state density. The proposed algorithm is able to handle nonlinear and non-Gaussian systems using a Gaussian mixture model and sigma-point Kalman filters. The paper shows that the proposed algorithm has better performance compared to other state-of-the-art algorithms in terms of accuracy and computational complexity.",1
1331,for sequential probabilistic inference in nonlinear non-gaussian systems approximate solutions must be used . we present a novel recursive bayesian estimation algorithm that combines an importance sampling based measurement update step with a bank of sigma-point kalman filters for the time-update and proposal distribution generation . the posterior state density is represented by a gaussian mixture model that is recovered from the weighted particle set of the measurement update step by means of a weighted em algorithm . this recursive bayesian estimation algorithm replaces the resampling stage needed by most particle filters and mitigates the '' sample depletion '' problem . we show that this new recursive bayesian estimation algorithm has an improved estimation performance and reduced computational complexity compared to other related algorithms .,0
1332,"This paper presents a corpus-based study on gender-specific characteristics in spontaneous telephone and face-to-face conversations. A ""data-mining"" approach is used to analyze spoken language corpora and investigate various spontaneous speech phenomena, such as phoneme substitutions, disfluencies, filled pauses, insertions, repetitions, and deletions. The study reveals significant gender differences in pronunciation variation and articulation rate. In particular, females tend to exhibit more disfluencies, while males tend to use more phoneme substitutions. The results shed light on the role of gender in everyday speech and language and provide insights into gender-related communication styles.",1
1333,"this paper presents an exploratory study on the relations between gender and everyday parlance . a '' data-mining '' approach is used to explore gender-specific characteristics in a large number of spontaneous telephone and face-to-face conversations . our study focuses on speech rate -lrb- speaking rate and articulation rate -rrb- , disfluencies -lrb- filled pauses and repetitions -rrb- , pronunciation variation -lrb- phoneme substitutions , deletions and insertions -rrb- , and preferences for particular parts of speech . our study reveals interesting similarities and differences in everyday male and female speech , and proves that data-mining on large spoken language corpora is a promising approach for obtaining information on spontaneous speech phenomena and for generating new hypotheses for research .",0
1334,"This paper proposes a joint design of the maximum sum-rate receiver and power allocation strategy for multihop wireless sensor networks. The paper introduces constrained maximum sum-rate expressions and equal power allocation parameters for the amplify-and-forward scheme in the relay node. The joint optimization problem is solved by an alternating optimization approach, which includes optimizing the power allocation parameters and the complex amplification coefficients. A linear receiver is used to estimate the transmitted signals, and the objective is to maximize the sum-rate of the network. Simulation results demonstrate the effectiveness of the proposed approach in achieving higher sum-rate performance.",1
1335,"in this paper , we consider a multihop wireless sensor network with multiple relay nodes for each hop where the amplify-and-forward scheme is employed . we present a strategy to jointly design the linear receiver and the power allocation parameters via an alternating optimization approach that maximizes the sum-rate of the multihop wireless sensor network . we derive constrained maximum sum-rate expressions along with an algorithm to compute the linear receiver and the power allocation parameters with the optimal complex amplification coefficients for each relay node . computer simulations show good performance of our proposed methods in terms of sum-rate compared to the method with equal power allocation .",0
1336,"This paper presents the application of the Gaussian Dynamic Warping (GDW) method to text-dependent speaker detection and verification. The GDW method is a temporal structure information component that can account for acoustic variability of speech in acoustic modeling. The proposed method is applied to voice-based entrance door security systems, which require accurate speaker verification under temporal constraints. The GDW method is incorporated into a hierarchical statistical framework that allows for specialization in acoustic modeling. Experimental results demonstrate the effectiveness of the proposed method in real world applications. The paper provides insights into the use of GDW in text-dependent speaker detection and verification, and the potential benefits of hierarchical statistical frameworks for acoustic modeling.",1
1337,"this paper introduces a new acoustic modeling method called gaussian dynamic warping . acoustic modeling method is targeting real world applications such as voice-based entrance door security systems , the example presented in this paper . the proposed acoustic modeling method uses a hierarchical statistical framework with three levels of specialization for the acoustic modeling . the highest level of specialization is in addition responsible for the modeling of the temporal constraints via a specific temporal structure information component . the preliminary results show the ability of the gaussian dynamic warping to elegantly take into account the acoustic variability of speech while capturing important temporal constraints .",0
1338,"This paper presents a concept-based summarization approach using integer linear programming (ILP) that can generate multiple optimal summaries within a given budget. The proposed method addresses the budgeted maximum coverage problem and performs sentence selection based on low-weight concepts, which are used for concept pruning. An approximation algorithm is employed to reduce the computational complexity of the ILP solver, which is known to be NP-hard. The approach is evaluated on several datasets and compared with state-of-the-art summarization techniques. Results show that the proposed method achieves better performance and can generate multiple summaries that are both informative and diverse.",1
1339,"in concept-based summarization , sentence selection is modelled as a budgeted maximum coverage problem . as this problem is np-hard , pruning low-weight concepts is required for the solver to find optimal solutions efficiently . this work shows that reducing the number of concepts in the model leads to lower rouge scores , and more importantly to the presence of multiple optimal solutions . we address these issues by extending the model to provide a single optimal solution , and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference .",0
1340,"This paper proposes a novel approach to phoneme recognition across speech sub-bands using multi-resolution cepstral features. The full bandwidth cepstral features are decomposed into sub-bands and independently modeled using sub-band cepstral features. Discriminative cues are extracted from the sub-band variations and used to calculate log-likelihood probabilities for phoneme recognition. The approach achieves improved recognition performance compared to traditional MFCC features, especially in noisy environments. Linearly weighted and non-linear recombination of partial recognition scores are explored to generate multi-resolution feature vectors for classification. The proposed method is evaluated on the TIMIT database and demonstrates superior performance in localised regions of the spectrum corrupted by broadband noise.",1
1341,"multi-resolution sub-band cepstral features strive to exploit discriminative cues in localised regions of the spectral domain by supplementing the full bandwith cepstral features with sub-band cepstral features derived from several levels of sub-band decomposition . mult-iresolution feature vectors , formed by concatenation of the subband cepstral features into an extended feature vector , are shown to yield better performance than conventional mfcc features for phoneme recognition on the timit database . possible strategies for the recombination of partial recognition scores from independent multi-resoltuion sub-band models are explored . by exploiting the sub-band variations in signal to noise ratio for linearly weighted recombination of the log likelihood probabilities we obtained improved phoneme recognition performance in broadband noise compared to mfcc features . this is an advantage over a purely sub-band approach using non linear recombination which is robust only to narrow band noise .",0
1342,"This paper proposes a novel approach to speech recognition using neural networks with forward-backward probability generated targets. Specifically, the authors focus on the task of continuous digits recognition, where the neural network is trained using forward-backward probabilities as targets instead of the usual discrete targets. The performance of this approach is evaluated in terms of error rate, and the results show that the proposed method outperforms traditional approaches using discrete targets. The use of forward-backward probabilities as continuous targets provides the neural network with more information about the temporal structure of speech, leading to improved recognition accuracy. Overall, the paper highlights the potential of using forward-backward probability generated targets for speech recognition tasks.",1
1343,"neural network training targets for speech recognition are estimated using a novel method . rather than use zero and one , continuous targets are generated using forward-backward probabilities . each training pattern has more than one class active . experiments showed that the new method eectively decreased the error rate by 15 % in a continuous digits recognition task .",0
1344,"This paper proposes methods to improve pronunciation modeling for non-native speech recognition. Two approaches are presented: n-best rescoring and speaker clustering. The n-best rescoring approach is used to compare different pronunciation modeling approaches, while the speaker clustering approach employs latent pronunciation analysis to improve the accuracy of the pronunciation modeling. The results of experiments show that both approaches can reduce the error rate of continuous digits recognition tasks. The paper also discusses the limitations of current pronunciation modeling approaches and suggests that the use of a pronunciation dictionary approach combined with speaker clustering could be a promising direction for future research.",1
1345,"in this paper , three different approaches to pronunciation modeling are investigated . two existing pronunciation modeling approaches , namely the pronunciation modeling and n-best rescoring approach are modified to work with little amount of non-native speech . we also propose a speaker clustering approach , which capable of grouping the speakers based on their pronunciation habits . given some speech , the speaker clustering approach can also be used for pronunciation modeling . this speaker clustering approach is called latent pronunciation analysis . the results show that conventional pronunciation modeling perform slightly better than n-best list rescoring , while the latent pronunciation analysis has shown to be beneficial for speaker clustering , and speaker clustering approach can produce nearly the same improvement as the pronunciation dictionary approach , without the need to know the origin of the speaker .",0
1346,"The paper presents an unsupervised learning method for identifying mixtures of multiple causes in binary data. The proposed approach uses a mixture model and sigmoid squashing to learn cluster centers and weights of the causes. The resulting weighted sum of the causes is used to generate data reconstructions that can be compared to the observed data. The nonlinearity of the sigmoid function allows the model to capture complex relationships between the causes and the binary data. The proposed method is evaluated on images of printed characters and noisy test data, demonstrating its effectiveness in identifying multiple causes in the data. The results suggest that the approach can be used for recognition and learning tasks.",1
1347,"this paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data . unlike the standard mixture model , a multiple cause model accounts for observed data by combining assertions from many hidden causes , each of which can pertain to varying degree to any subset of the observable dimensions . a crucial issue is the mixing-function for combining beliefs from different cluster-centers in order to generate data reconstructions whose errors are minimized both during recognition and learning . we demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing , and offer an alternative form of the nonlinearity . results are presented demonstrating the algorithm 's ability successfully to discover coherent multiple causal representat.ions of noisy test data and in images of printed characters .",0
1348,I'm sorry but the entities and graph provided do not match the title. Can you please provide the correct entities and graph?,1
1349,"in this paper we introduce a new distance for robustly matching vectors of 3d rotations . a special representation of 3d rotations , which we coin full-angle quaternion -lrb- faq -rrb- , allows us to express this distance as euclidean . we apply the distance to the problems of 3d shape recognition from point clouds and 2d object tracking in color video . for the former , we introduce a hashing scheme for scale and translation which outperforms the previous state-of-the-art approach on a public dataset . for the latter , we incorporate online subspace learning with the proposed faq representation to highlight the benefits of the new representation .",0
1350,"This paper presents feature extraction methods for consistent spatio-temporal image sequence classification using Hidden Markov Models (HMMs). Different low-level image features are explored for the classification of dynamic hand gestures, including the geometry of the image, probability density values, and image intensities. The paper emphasizes the importance of selecting appropriate features to achieve accurate classification performance. Specifically, the use of image density functions is highlighted as an effective approach for representing image sequences. The study demonstrates the effectiveness of HMMs for modeling the temporal dynamics of image sequences and achieving high classification accuracy. Overall, the paper provides insights into the importance of feature selection for consistent spatio-temporal image sequence classification using HMMs.",1
1351,"in this paper a general and eecient approach for representing and classifying image sequences by hidden markov models is presented . a consistent modeling of spatial and temporal information is achieved by extracting diierent low level image features . these implicitly convert the image intensities into probability density values , while preserving the geometry of the image . the resulting so called image density functions are contained in the states of the hidden markov models . first results of applying the approach to the classiica-tion of dynamic hand gestures demonstrate the performance of the modeling .",0
1352,"This paper proposes a method for extracting causes of emotions from text. The method involves detecting the linguistic relations between emotion keywords and potential causes, using an automatic extraction of phrases and a syntactic and dependency parser. The method is evaluated on a corpus of emotion texts and achieves high accuracy using a set of emotion-cause linguistic relations and rules. Overall, the paper presents a promising approach for automatically identifying the causes of emotions in text data.",1
1353,"this paper focuses on the novel task of automatic extraction of phrases related to causes of emotions . the analysis of emotional causes in sentences , where emotions are explicitly indicated through emotion keywords can provide the foundation for research on challenging task of recognition of implicit affect from text . we developed a corpus of emotion causes specific for 22 emotions . based on the analysis of this corpus we introduce a method for the detection of the linguistic relations between an emotion and its cause and the extraction of the phrases describing the emotion causes . the method employs syntactic and dependency parser and rules for the analysis of eight types of the emotion-cause linguistic relations . the results of evaluation showed that our method performed with high level of accuracy -lrb- 82 % -rrb- .",0
1354,"This paper presents the Geometric min-Hashing approach, a method for clustering large scale image data based on their spatial extent and visual appearance. The method employs semi-local geometric information to extract repeatable hash keys, which are used to find a ""thick needle in a haystack"" - meaning a small object in a large image. The approach improves upon traditional min-Hash methods by considering the false positive rates of random collisions in hash keys. The paper evaluates the recall and probability of collision for min-Hash and the Geometric min-Hashing approach, and demonstrates its usefulness for automatic object discovery, image retrieval, and clustering. The method is evaluated using a public dataset and achieves high accuracy. The paper concludes that the Geometric min-Hashing approach can efficiently handle the large scale image clustering problem, while maintaining high precision and recall.",1
1355,"we propose a novel geometric min-hashing approach for image retrieval , clustering and automatic object discovery . unlike commonly used bag-of-words approaches , the spatial extent of image features is exploited in our geometric min-hashing approach . the geometric information is used both to construct repeatable hash keys and to increase the discriminability of the description . each hash key combines visual appearance -lrb- visual words -rrb- with semi-local geometric information . compared with the state-of-the-art min-hash , the proposed geometric min-hashing approach has both higher recall -lrb- probability of collision for hashes on the same object -rrb- and lower false positive rates -lrb- random collisions -rrb- . the advantages of geometric min-hashing approach are most pronounced in the presence of viewpoint and scale change , significant occlusion or small physical overlap of the viewing fields . we demonstrate the power of the proposed geometric min-hashing approach on small object discovery in a large unordered collection of images and on a large scale image clustering problem .",0
1356,"This paper proposes a method to speed up the n-gram language model lookahead using order-preserving perfect hashing (OP MPH) with a subtree cache structure. The order-preserving property is used to design a string-key based MPH function, which allows for minimum perfect hashing of the trigrams in a language model. By exploiting the subtree structure of the LM lookahead, the proposed method caches subtrees of the LM lookahead for faster decoding. The experimental results show that the proposed method achieves significant speedup in LVCSR decoding with Switchboard data, while maintaining the same word error rate as the baseline. The paper concludes that the OP MPH and subtree cache structure can be used to improve the efficiency of LM lookahead without sacrificing its accuracy.",1
1357,"minimum perfect hashing -lrb- minimum perfect hashing -rrb- has recently been shown successful in reducing language model lookahead time in lvcsr decoding . in this paper we propose to exploit the order-preserving property of a string-key based mph function to further reduce hashing operation and speed up lm lookahead . a subtree structure is proposed for lm lookahead and an order-preserving mph is integrated into the structure design . subtrees are generated on demand and stored in caches . experiments were performed on switchboard data . by using the proposed method of op mph and subtree cache structure for both trigrams and backoff bigrams , the lm lookahead time was reduced by a factor of 2.9 in comparison with the baseline case of using minimum perfect hashing alone .",0
1358,"This paper proposes a non-convex statistical optimization approach for estimating a sparse tensor graphical model from high-dimensional tensor-valued data. The model assumes a Kronecker product structure and a sparse precision matrix, and is estimated via penalized maximum likelihood estimation with an alternating minimization algorithm. The proposed approach is shown to achieve consistent graph recovery under certain conditions and is evaluated using numerical studies on tensor samples. The paper also discusses the statistical rate of convergence and estimation consistency of the estimator. Overall, the proposed non-convex optimization approach is demonstrated to be effective for estimating sparse graphical models with a tensor structure and a sparse precision matrix.",1
1359,"we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data . to facilitate the estimation of the precision matrix corresponding to each way of the tensor , we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure . the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function . in spite of the non-convexity of this estimation problem , we prove that an alternating minimization algorithm , which iteratively estimates each sparse precision matrix while fixing the others , attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery . notably , such an estimator achieves estimation consistency with only one tensor sample , which is unobserved in previous work . our theoretical results are backed by thorough numerical studies .",0
1360,"This paper proposes a novel online tensor subspace learning algorithm for robust visual tracking. The proposed algorithm utilizes low-order tensor eigenspace representation for the representation of image ensembles. The incremental learning of the subspace is achieved by minimizing the tensor reconstruction error norm. The proposed subspace analysis-based tracking algorithms achieve robustness to occlusion, illumination changes, and pose variations in the object to be tracked. The computational and memory cost of the proposed method is lower than that of existing methods. The multilinear framework is used to model the spatio-temporal redundancies in high-order tensors. The likelihood function is estimated using the flattened vector of the tensor subspace model, which is then used in particle filter tracking. The state inference of the object being tracked is achieved by the theoretic analysis of the proposed algorithm. Numerical experiments show that the proposed algorithm outperforms state-of-the-art tracking algorithms.",1
1361,"most existing subspace analysis-based tracking algorithms utilize a flattened vector to represent a target , resulting in a high dimensional data learning problem . recently , subspace analysis is incorporated into the multilin-ear framework which offline constructs a representation of image ensembles using high-order tensors . this reduces spatio-temporal redundancies substantially , whereas the computational and memory cost is high . in this paper , we present an effective online tensor subspace learning algorithm which models the appearance changes of a target by incrementally learning a low-order tensor eigenspace representation through adaptively updating the sample mean and eigenbasis . tracking then is led by the state inference within the framework in which a particle filter is used for propagating sample distributions over the time . a novel likelihood function , based on the tensor reconstruction error norm , is developed to measure the similarity between the test image and the learned tensor subspace model during the tracking . theoretic analysis and experimental evaluations against a state-of-the-art method demonstrate the promise and effectiveness of this online tensor subspace learning algorithm .",0
1362,"This paper explores the potential relevance of audio-visual integration in mammals for computational modeling. Specifically, it discusses the importance of audiovisual integration in early word learning and recognition of arbitrary geometrical objects in both humans and non-human animals. The paper also analyzes the significance of audiovisual integration in the context of speech recognition systems and neural networks. The implications of this research for robotics are also discussed.",1
1363,"the purpose of this study was to examine typically developing infants ' integration of audiovisual sensory information as a fundamental process involved in early word learning . one hundred sixty pre-linguistic children were randomly assigned to watch one of four counterbalanced versions of audiovisual video sequences . the infants ' eye-movements were recorded and their looking behavior was analyzed throughout three repetitions of exposure-test-phases . the results indicate that the infants were able to learn covariance between shapes and colors of arbitrary geometrical objects and to them corresponding nonsense words . implications of audiovisual integration in infants and in non-human animals for modeling within speech recognition systems , neural networks and robotics are discussed .",0
1364,"This paper presents a method for online unsupervised pattern discovery in speech using parallelization. The approach utilizes segmental dynamic time warping (DTW) for comparing speech utterances and discovering patterns in a static or dynamic way. The method is designed for online processing, which allows for unsupervised discovery of patterns as new data arrives. The approach is implemented on an 8-processor cluster and tested on multi-core servers with varying computational requirements, including a 32-processor system. The results demonstrate the feasibility and effectiveness of the proposed approach for online unsupervised pattern discovery in speech.",1
1365,"segmental dynamic time warping -lrb- dtw -rrb- has been demonstrated to be a useful technique for finding acoustic similarity scores between segments of two speech utterances . due to its high computational requirements , it had to be computed in an offline manner , limiting the applications of the technique . in this paper , we present results of parallelization of this task by distributing the workload in either a static or dynamic way on an 8-processor cluster and discuss the trade-offs among different distribution schemes . we show that online unsupervised pattern discovery using segmental dtw is plausible with as low as 8 processors . this brings the task within reach of today 's general purpose multi-core servers . we also show results on a 32-processor system , and discuss factors affecting scalability of our methods .",0
1366,"This paper proposes an improved approach for solving the adversarial contextual bandit problem by using an oracle-based algorithm. The authors present a relaxation-based approach and analyze its performance in terms of regret bounds. Specifically, they derive new bounds that improve upon those of previous algorithms. The proposed method is evaluated through simulations and compared with baseline policies. The results demonstrate the effectiveness of the approach, especially in terms of the number of a priori iterations required to achieve optimal performance. Overall, this work contributes to the development of efficient solutions for online learning in complex decision-making scenarios.",1
1367,"we give an oracle-based algorithm for the adversarial contextual bandit problem , where either contexts are drawn i.i.d. or the sequence of contexts is known a priori , but where the losses are picked adversarially . our oracle-based algorithm is computationally efficient , assuming access to an offline optimization oracle , and enjoys a regret of order o -lrb- -lrb- kt -rrb- 2 3 -lrb- log n -rrb- 1 3 -rrb- , where k is the number of actions , t is the number of iterations and n is the number of baseline policies . our result is the first to break the o -lrb- t 3 4 -rrb- barrier that is achieved by recently introduced algorithms . breaking this barrier was left as a major open problem . our analysis is based on the recent relaxation based approach of rakhlin and sridharan -lsb- 7 -rsb- .",0
1368,"The paper ""Robust binary least squares: Relaxations and algorithms"" presents methods for solving robust optimization problems of binary least squares (LS) type. The authors consider the worst-case scenario and provide semidefinite relaxations of the problem to improve its tractability. The paper also proposes an algorithm based on Lagrangian duality and presents a relaxation step to compare with existing approaches based on semi-definite programming. The problem is known to be NP-hard, and the authors show how their proposed methods can improve the computation time compared to previous algorithms. The paper includes numerical experiments to demonstrate the effectiveness of the proposed methods.",1
1369,"finding the least squares -lrb- ls -rrb- solution s to a system of linear equations hs = y where h , y are given and s is a vector of binary variables , is a well known np-hard problem . in this paper , we consider binary ls problems under the assumption that the coefſcient matrix h is also unknown , and lies in a given uncertainty ellipsoid . we show that the corresponding worst-case robust optimization problem , although np-hard , is still amenable to semideſnite relaxation - based approximations . however , the relaxation step is not obvious , and requires a certain problem reformulation to be efſcient . the proposed relaxation step is motivated using lagrangian duality and simulations suggest that relaxation step performs well , offering a robust alternative over the traditional sdr approaches for binary ls problems .",0
1370,"This paper explores two types of defeasibility in defeasible deontic logic: cancelling and overshadowing. Defeasible deontic logic deals with the reasoning about obligations and permissions that can be overridden by other obligations and permissions. The paper examines the Chisholm and Forrester's paradoxes in this context and proposes dyadic deontic logics to address them. It then defines factual and overridden defeasibility and shows how these concepts can be used to account for sub-ideal behavior in decision theories. The paper also discusses the relation between defeasibility and non-monotonic logics, and the role of conditional obligations in defeasible deontic logic. Finally, it provides a formalization of the two types of defeasibility and shows how they can be incorporated into deontic logic.",1
1371,"in this paper we give a general analysis of dyadic deontic logics that were introduced in the early seventies to formalize deontic reasoning about subideal behavior . recently it was observed that dyadic deontic logics are closely related to non-monotonic logics , theories of diagnosis and decision theories . in particular , we argue that two types of defeasibility must be distinguished in a defeasible deontic logic : overridden defeasi-bility that formalizes cancelling of an obligation by other conditional obligations and factual defeasibility that formalizes overshadowing of an obligation by a violating fact . we also show that this distinction is essential for an adequate analysis of notorious ` paradoxes ' of deontic logic such as the chisholm and for-rester ` paradoxes ' .",0
1372,"This paper proposes an efficient approach to creating 3D training data for fine hand pose estimation. The approach takes into account spatial, temporal, and appearance constraints to enhance the accuracy of hand pose estimation methods. The method involves using a sub-modular loss function and a semi-automated labeling process to generate 3D locations and reference frames for hand poses. The proposed approach is evaluated using hand depth videos, and the results demonstrate improved labeling accuracy. The paper concludes that the proposed approach can be used to enhance the performance of hand pose estimation methods.",1
1373,"while many recent hand pose estimation methods critically rely on a training set of labelled frames , the creation of such a dataset is a challenging task that has been overlooked so far . as a result , existing datasets are limited to a few sequences and individuals , with limited accuracy , and this prevents these methods from delivering their full potential . we propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3d locations of the joints : the user is asked to provide only an estimate of the 2d reprojec-tions of the visible joints in some reference frames , which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function . we then exploit spatial , temporal , and appearance constraints to retrieve the full 3d poses of the hand over the complete sequence . we show that this data can be used to train a recent state-of-the-art hand pose estimation method , leading to increased accuracy . the hand pose estimation method and dataset can be found on our website https://cvarlab.icg.tugraz . at/projects/hand _ detection / .",0
1374,"This paper presents a study on learning from demonstration, which is a technique used to acquire new skills by observing demonstrations performed by an expert. The paper covers topics such as reinforcement learning, value function, and policy learning. The main focus of this paper is on the application of learning from demonstration in various scenarios, such as nonlinear learning problems, model-based reinforcement learning, and linear quadratic regulator problems. The study shows that learning from demonstration can significantly improve learning robustness and overcome initial biases. Overall, this paper provides valuable insights into the benefits and challenges of learning from demonstration for solving complex learning problems.",1
1375,"by now it is widely accepted that learning a task from scratch , i.e. , without any prior knowledge , is a daunting undertaking . humans , however , rarely attempt to learn from scratch . they extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans . for learning problem , this paper investigates how learning from demonstration can be applied in the context of reinforcement learning . we consider priming the q-function , the value function , the policy , and the model of the task dynamics as possible areas where demonstrations can speed up learning . in general nonlinear learning problems , only model-based reinforcement learning shows significant speed-up after a demonstration , while in the special case of linear quadratic regulator problems , all methods profit from the demonstration . in an implementation of pole balancing on a complex anthropomorphic robot arm , we demonstrate that , when facing the complexities of real signal processing , model-based reinforcement learning offers the most robustness for linear quadratic regulator problems . using the suggested methods , the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor .",0
1376,"This paper examines the issue of multiple optima and non-strict convexity in IBM Model 1 for machine translation. The authors focus on the importance of initialization, as it has a significant impact on the resulting model parameters and alignment error rate. They propose using a linear programming approach to find the optimal parameters and evaluate their approach using test set log-likelihood and alignment error rate on random trials. The results show that initialization can greatly affect the quality of the final model, and the proposed approach outperforms previous methods in terms of alignment error rate.",1
1377,"contrary to popular belief , we show that the optimal parameters for ibm model 1 are not unique . we demonstrate that , for a large class of words , ibm model 1 is indifferent among a continuum of ways to allocate probability mass to their translations . we study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials , and demonstrate that ibm model 1 results in variance in test set log-likelihood and alignment error rate .",0
1378,"This paper proposes a blind source separation (BSS) method for improving teleconferencing in a distributed microphone meeting environment. The proposed approach uses adaptive noise cancellation to reduce background noise and convolutive BSS to separate mixed audio signals into individual sources. A depermutation scheme is also employed to handle speaker overlap. The efficacy of the method is evaluated using both simulated and real data, and a least-squares post-processing scheme is applied to further enhance the separation performance. The results show that the proposed method achieves significant improvement in speech quality and intelligibility, demonstrating its potential for practical applications in teleconferencing technology.",1
1379,"from an audio perspective , the present state of teleconferencing technology leaves something to be desired ; speaker overlap is one of the causes of this inadequate performance . to that end , this paper presents a frequency-domain implementation of convolutive bss specifically designed for the nature of the teleconferencing environment . in addition to presenting a novel depermutation scheme , this paper presents a least-squares post-processing scheme , which exploits segments during which only a subset of all speakers are active . experiments with simulated and real data demonstrate the ability of the proposed least-squares post-processing scheme to provide sirs at or near that of the adaptive noise cancellation solution which is obtained under idealistic assumptions that the adaptive noise cancellation solution are adapted with one source being on at a time .",0
1380,"This paper presents an invited talk that focuses on the processes that shape conversation and their implications for computational linguistics. Communication media, cognitive and interpersonal processes, and interactive language use are all explored in this context. The paper highlights the importance of understanding these processes in order to develop more effective computational linguistics models for analyzing and understanding spontaneous speech, referring expressions, and other aspects of interactive processes. One key aspect of the talk is the discussion of hedges and their role in shaping conversation. The paper concludes by emphasizing the need for continued research in this area to improve our understanding of the complex and dynamic nature of language use in communication.",1
1381,"experimental studies of interactive language use have shed light on the cognitive and interpersonal processes that shape conversation ; corpora are the emergent products of these processes . i will survey studies that focus on under-modelled aspects of interactive language use , including the processing of spontaneous speech and disfluencies ; metalinguistic displays such as hedges ; interactive processes that affect choices of referring expressions ; and how communication media shape conversations . the findings suggest some agendas for computational linguistics .",0
1382,"This paper discusses the application of homeostatic plasticity in Bayesian spiking networks for expectation maximization with posterior constraints. The authors explore how homeostatic dynamics can be implemented in spiking network models through synaptic plasticity rules and neuronal activation functions. They show that the mathematical treatment of this approach involves nontrivial terms, but that it is possible to implement probabilistic inference in cortical microcircuits using this technique. The paper concludes with a discussion of the theoretical limitations of homeostatic plasticity in Bayesian spiking networks, as well as its potential for unsupervised learning and inference.",1
1383,"recent spiking network models of bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules . here we show in a rigorous mathematical treatment how homeostatic processes , which have previously received little attention in this context , can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models . in particular , we show that homeostatic plasticity can be understood as the enforcement of a ` balancing ' posterior constraint during probabilis-tic inference and learning with expectation maximization . we link homeostatic dynamics to the theory of variational inference , and show that nontrivial terms , which typically appear during probabilistic inference in a large class of models , drop out . we demonstrate the feasibility of our approach in a spiking winner-take-all architecture of bayesian inference and learning . finally , we sketch how the mathematical treatment can be extended to richer recurrent network archi-tectures . altogether , our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits , and points to an essential role of homeostasis during inference and learning in spiking networks .",0
1384,"Speaker recognition systems have proven to be effective in many real-world applications. However, their performance is often degraded by various factors, including session variability, channel variability, and language factors. The language gap problem is particularly challenging since it arises from differences in the pronunciation of non-native speakers. In this paper, we investigate the effect of language factors on speaker recognition systems and propose a joint factor analysis model to compensate for the language gap problem. We evaluate our method on the NIST speaker recognition evaluation task and show that incorporating language factor compensation can significantly improve the Equal Error Rate (EER) of the system, especially when trained on English data and tested on non-English trials. Our results suggest that language factor compensation is a promising direction for improving the robustness of speaker recognition systems in real-world scenarios.",1
1385,"from the results of the nist speaker recognition evaluation in resent years , speaker recognition systems which are mainly developed based on english training data suffer the language gap problem , namely , the performance of non-english trails is much worse than that of english trails . this problem is addressed in this paper . based on the conventional joint factor analysis model , we enrolled in the language factors which are mean to capture the language character of each testing and training speech utterance , and compensation was carried out by removing the language factors in order to shrink the difference between languages . experiments on 2006 nist sre data show that , the language factor compensation alone can reduce the gap between the performance of english and non-english trails , and the score level combination with eigenchannels can further improve the performance of non-english trails , e.g. , for female part , we observed about 19 % relatively reduction in eer , when compared with eigenchannels session variability compensation alone .",0
1386,"This paper presents an eye fixation database for saliency detection in images, which is aimed at improving the semantics-driven human understanding of images. The database contains eye fixation data obtained through an eye-tracker, which is used to compute saliency and segmentation algorithms. Characteristic fixation seeds and fixation clusters are extracted from the data to identify salient objects and categorize image content according to semantic categories. Feature-driven approaches are also explored to enhance the accuracy of saliency detection. The eye fixation database is shown to be a useful tool for active image segmentation applications and improving image understanding. Overall, this paper demonstrates the importance of eye fixations in saliency detection and highlights the potential of using an eye-tracker and fixation data for image analysis.",1
1387,"to learn the preferential visual attention given by humans to specific image content , we present an eye fixation database compiled from a pool of 758 images and 75 subjects . eye fixations are an excellent modality to learn semantics-driven human understanding of images , which is vastly different from feature-driven approaches employed by saliency computation algorithms . the eye fixation database comprises fixation patterns acquired using an eye-tracker , as subjects free-viewed images corresponding to many semantic categories such as faces -lrb- human and mammal -rrb- , nudes and actions -lrb- look , read and shoot -rrb- . the consistent presence of fixation clusters around specific image regions confirms that visual attention is not subjective , but is directed towards salient objects and object-interactions . we then show how the fixation clusters can be exploited for enhancing image understanding , by using our eye fixation database in an active image segmentation application . apart from proposing a mechanism to automatically determine characteristic fixation seeds for segmentation , we show that the use of fixation seeds generated from multiple fixation clusters on the salient object can lead to a 10 % improvement in segmen-tation performance over the state-of-the-art .",0
1388,"This paper proposes a set of system-driven metrics for the design and adaptation of analog to digital converters (ADCs), with a focus on improving communication links in the presence of forward error correction and intersymbol interference. The proposed metrics are based on mutual information and bit-error rate, and are designed to provide meaningful criteria for the evaluation of ADCs in communication applications. The metrics are shown to be useful for guiding ADC design methods, such as fixed uniform quantization, and for adapting ADCs to different communication requirements. The paper emphasizes the importance of system-driven metrics for improving communication performance and highlights the potential of using these metrics in the design and optimization of ADCs for various applications. Overall, this paper presents a comprehensive approach to system-driven ADC design and evaluation, with a focus on improving communication performance.",1
1389,"-- in this paper , we review some recent advances in the design of adcs that exploit system-driven metrics , such as the bit-error rate in a communication link , or mutual information in a scheme employing forward error correction . we show , for example , that adcs can be designed that maximize the information rate between the quantized output of the channel and the input to the channel for communication links with intersymbol-interference and additive noise . these adcs dramatically outper-form -lrb- in terms of achievable information rates -rrb- traditional adc design methods that are based on fixed uniform quantization . architectures are also developed for adcs such that adcs can be used to dynamically adapt the structure of the adcs to optimize application meaningful criteria , such as bit-error rate for communication over intersymbol interference links .",0
1390,"This paper proposes a symmetrical dense optical flow estimation method with occlusion detection, which can accurately estimate the dense optical flow field map for both synthetic and real images. The proposed method is based on the Euler-Lagrange equations and employs a multi-resolution technique to estimate the displacements vectors between images I1 and I2. The symmetrical solutions are obtained by minimizing a flow field energy functional that includes an optical flow gradient flow term and a focusing strategy to deal with occlusions. The proposed method is shown to be effective in handling occlusions and generating accurate dense optical flow estimation results. The paper emphasizes the importance of the variational problem in the proposed method and highlights the potential of using symmetrical solutions for dense optical flow estimation. Overall, this paper presents a comprehensive approach to dense optical flow estimation, with a focus on occlusion detection and symmetrical solutions.",1
1391,"traditional techniques of dense optical flow estimation do n't generally yield symmetrical solutions : the results will differ if symmetrical solutions are applied between images i1 and i2 or between images i2 and i1 . in this work , we present a method to recover a dense optical flow field map from two images , while explicitely taking into account the symmetry across the images as well as possible occlusions and discontinuities in the flow field . the idea is to consider both displacements vectors from i1 to i2 and i2 to i1 and to minimise an energy functional that explicitely encodes all those properties . this variational problem is then solved using the gradient flow defined by the euler -- lagrange equations associated to the energy . in order to reduce the risk to be trapped within some irrelevant minimum , a focusing strategy based on a multi-resolution technique is used to converge toward the solution . promising experimental results on both synthetic and real images are presented to illustrate the capabilities of this symmetrical variational approach to recover accurate optical flow .",0
1392,"This paper focuses on linear structure from motion (SfM) for light field cameras, specifically addressing the relative pose estimation of hand-held consumer light field cameras. The proposed method utilizes direct linear pose estimation to estimate the relative pose of the light field cameras, which is then used to reconstruct a 3D point cloud of the scene. The method also utilizes Plücker ray coordinates and ray-to-ray correspondences to estimate the relative pose between light field cameras. The linear constraints imposed by the method ensure the consistency of the estimated pose with the scene geometry, resulting in accurate and efficient pose estimates. The proposed method is shown to be effective in reconstructing 3D point clouds from light field cameras and can be extended to generalized cameras. The paper highlights the potential of the proposed method for applications such as refocus-able panoramas and light field projection. Overall, this paper presents a comprehensive approach to linear SfM for light field cameras, with a focus on relative pose estimation and the use of linear constraints.",1
1393,"we present a novel approach to relative pose estimation which is tailored to 4d light field cameras . from the relationships between scene geometry and light field structure and an analysis of the light field projection in terms of plücker ray coordinates , we deduce a set of linear constraints on ray space correspondences between a pair of light field cameras . these can be applied to infer relative pose of the light field cameras and thus obtain a point cloud reconstruction of the scene . while the proposed method has interesting relationships to pose estimation for generalized cameras based on ray-to-ray correspondence , our experiments demonstrate that our approach is both more accurate and computationally more efficient . it also compares favorably to direct linear pose estimation based on aligning the 3d point clouds obtained by reconstructing depth for each individual light field . to further validate the method , we employ the pose estimates to merge light fields captured with hand-held consumer light field cameras into refocus-able panoramas .",0
1394,"This paper explores the relationship between two important signal processing techniques, namely the Karhunen-Loeve transform (KLT) and the prolate spheroidal wave functions (PSWF). Both techniques have been widely used in various applications such as signal compression, feature extraction, and spectral analysis. The paper focuses on the discrete versions of the two transforms and shows that the eigenfunctions of the discrete KLT are closely related to the discrete PSWFs. Specifically, it is shown that the PSWFs can be used to approximate the eigenfunctions of the KLT, and that the center frequencies of the PSWFs can be used as an approximation of the signal spectrum. The paper also presents a medium-order solution for the PSWFs, which is used to derive the relationship between the two transforms. The results of this study can have important implications for the design of signal processing algorithms, particularly in applications that require a high level of spectral accuracy.",1
1395,"we find a close relationship between the discrete karhunen-loeve transform and the discrete prolate spheroidal wave functions . we show that the discrete prolate spheroidal wave functions form a natural basis for an expansion of the eigenfunctions of the discrete karhunen-loeve transform in the frequency domain , and then determine more general conditions that any set of functions must obey to be a valid basis . we also present approximate solutions for small , medium , and large filter orders . the medium order solution suggests that the principal eigenfunc-tion is , to a high degree of approximation , the principal discrete prolate spheroidal wave functions modulated so that its center frequency coincides with the peak of maximum energy in the signal spectrum . we then use this result to propose a new basis .",0
1396,"This paper introduces the ""neural gas"" method, a clustering algorithm for unsupervised learning. The method uses a vector quantization technique and a performance criterion to iteratively adjust the network model to better fit the input data. The algorithm also employs a hebb-like learning rule to adapt the topological relations between the nodes in the network. This paper proposes an extension to the ""neural gas"" algorithm called the growing neural gas network, which is an incremental model that can learn topologies as it adapts to new data. The growing neural gas network uses interpolation to add new nodes to the network and adjusts the topology based on a performance criterion. The paper presents experimental results that demonstrate the effectiveness of the growing neural gas network for clustering and visualization tasks.",1
1397,"an incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple hebb-like learning rule . in contrast to previous approaches like the `` neural gas '' method of martinetz and schulten -lrb- 1991 , 1994 -rrb- , this `` neural gas '' method has no parameters which change over time and is able to continue learning , adding units and connections , until a performance criterion has been met . applications of the `` neural gas '' method include vector quantization , clustering , and interpolation .",0
1398,"This paper proposes a new model called Factorial Markov Random Fields (FMRFs) which extends Markov random field models to incorporate the dependencies of multiple layers of hidden variables. The authors use this model to improve on existing methods for binary segmentation of images by employing a wide inference step to reduce the need for deep inference. The model is applied to both real and synthetic images and is shown to outperform existing methods, including Hidden Markov Models (HMMs) and factorial MRFs. The authors also introduce an EM-based algorithm for learning the parameters of the FMRF model and show how it can be used to improve inference in the model. Additionally, the authors demonstrate the use of graph cuts for binary segmentation with FMRFs. Overall, the results suggest that FMRFs provide a powerful framework for modeling complex dependencies in observable image layers and improving inference for a range of image processing applications.",1
1399,"in this paper we propose an extension to the standard markov random field model in order to handle layers . our extension , which we call a factorial mrf , is analogous to the extension from hidden markov models -lrb- hmm 's -rrb- to factorial hmm 's . we present an efficient em-based algorithm for inference on factorial mrf 's . our em-based algorithm makes use of the fact that layers are a priori independent , and that layers only interact through the observable image . the em-based algorithm iterates between wide inference , i.e. , inference within each layer for the entire set of pixels , and deep inference , i.e. , inference through the layers for each single pixel . the efficiency of our em-based algorithm is partly due to the use of graph cuts for binary segmentation , which is part of the wide inference step . we show experimental results for both real and synthetic images .",0
1400,"This paper presents a real-time method for shape analysis of a human body in clothing using time-series part-labeled volumes. The proposed algorithm uses a hierarchical search strategy to generate 3D reconstruction of the human body from input visual hulls, and leverages time-series sample volumes with body-part labels to perform shape analysis. The method is capable of handling loose-fitting clothing and utilizes eigenspace PCA for shape representation. The real-time performance of the method is achieved through the use of time-series volumes and a fast reconstruction algorithm. Experimental results demonstrate the effectiveness of the proposed method for real-time shape analysis of human bodies in clothing.",1
1401,"we propose a real-time method for simultaneously refining the reconstructed volume of a human body with loose-fitting clothing and identifying body-parts in it . time-series volumes , which are acquired by a slow but sophisticated 3d reconstruction algorithm , with body-part labels are obtained offline . the time-series sample volumes are represented by trajectories in the eigenspaces using pca . an input visual hull reconstructed online is projected into the eigenspace and compared with the trajectories in order to find similar high-precision samples with body-part labels . the hierarchical search taking into account 3d reconstruction errors can achieve robust and fast matching . experimental results demonstrate that our real-time method can refine the input visual hull including loose-fitting clothing and identify its body-parts in real time .",0
1402,"This paper presents a deep neural network architecture for solving the single channel source separation problem. The proposed method utilizes a combination of classifiers and energy minimization techniques to estimate source spectra from mixed signal spectra. Specifically, the deep neural network is trained using training data to classify time-frequency bins and separate sources during the separation stage. The architecture is compared against nonnegative matrix factorization, showing improved performance. The proposed method can handle energy scale differences and allows for real-time separation of sources.",1
1403,"in this paper , a novel approach for single channel source separation using a deep neural network architecture is introduced . unlike previous studies in which deep neural network architecture and other classifiers were used for classifying time-frequency bins to obtain hard masks for each source , we use the deep neural network architecture to classify estimated source spectra to check for their validity during separation . in the training stage , the training data for the source signals are used to train a deep neural network architecture . in the separation stage , the trained deep neural network architecture is utilized to aid in estimation of each source in the mixed signal . single channel source separation problem is formulated as an energy minimization problem where each source spectra estimate is encouraged to fit the trained deep neural network architecture and the mixed signal spectrum is encouraged to be written as a weighted sum of the estimated source spectra . the proposed approach works regardless of the energy scale differences between the source signals in the training and separation stages . nonnegative matrix factorization is used to initialize the deep neural network architecture for each source . the experimental results show that using deep neural network architecture initialized by deep neural network architecture for source separation improves the quality of the separated signal compared with using deep neural network architecture for source separation .",0
1404,"This paper investigates the spectral efficiency of Code Division Multiple Access (CDMA) uplink cellular networks operating in wireless flat fading channels. The study focuses on uplink CDMA systems with random spreading and considers multi-cell interference and base station coverage. The paper examines different receiver structures, including the matched filter, Wiener filter, and optimum filter, and uses asymptotic arguments to derive network capacity and spectral efficiency. Results show that the spectral efficiency of CDMA uplink cellular networks can be significantly improved by using the Wiener filter or the optimum filter as the receiver structure.",1
1405,"in this contribution , the performance of an uplink cdma system with random spreading and multi-cell interference is analyzed . a useful framework is provided in order to determine the base station coverage for wireless flat fading channels with very dense networks -lrb- in the number of users per meter -rrb- considering different receiver structures at the base station , namely the matched filter , the wiener filter and the optimum filter . using asymptotic arguments , analytical expressions of the spectral efficiency are obtained and provide a simple expression of the network capacity based only on a few meaningful parameters .",0
1406,"The paper proposes a new approach to linear dimensionality reduction called Local Learning Projections (LLP). Unlike traditional methods such as Principal Component Analysis (PCA), LLP aims to minimize local estimation error rather than global estimation error. This is achieved by computing projection values based on local information rather than global information. The paper demonstrates the effectiveness of LLP in various classification tasks and compares it with other methods. The results show that LLP outperforms traditional methods in scenarios where local information is important.",1
1407,"this paper presents a <i> local learning projection </i> -lrb- llp -rrb- approach for linear dimensionality reduction . we first point out that the well known <i> principal component analysis </i> essentially seeks the projection that has the minimal <i> global </i> estimation error . then we propose a dimensionality reduction algorithm that leads to the projection with the minimal <i> local </i> estimation error , and elucidate its advantages for classification tasks . we also indicate that llp keeps the local information in the sense that the projection value of each point can be well estimated based on its neighbors and their projection values . experimental results are provided to validate the effectiveness of the proposed dimensionality reduction algorithm .",0
1408,"This paper proposes an adaptive channel equalization method using context trees. The method addresses the inter-symbol interference problem in a linear ISI channel by utilizing a variable order Markov model and a decision feedback equalization scheme. The training sequence is crudely quantized and used to estimate the model parameters through a maximum likelihood sequence estimator. The adaptation is gradient-based, and the context trees are used to capture the local information in the signal. Gaussian-based algorithms are used to perform the equalization, and the receiver performance is evaluated in the presence of heavy-tailed noise and additive white noise. The proposed method outperforms existing methods in terms of receiver performance.",1
1409,the maximum likelihood sequence estimator is the optimal receiver for the inter-symbol interference channel with additive white noise . a receiver is demonstrated that estimates sequence likelihood using a variable order markov model constructed from a crudely quantized training sequence . receiver performance is relatively unaffected by heavy-tailed noise that can undermine the performance of gaussian based algorithms such as decision feedback equalization with gradient based adaptation . we consider the problem of decoding binary symbols across a linear isi channel contaminated with additive white noise . given discrete-time observations of the channel output r n,0
1410,"This paper investigates the outcomes of the equivalence between adaptive ridge and least absolute shrinkage and selection operator (LASSO) in sparse regression. It presents a series of possible extensions of LASSO, such as quadratic penalization, model complexity, and additive modeling. The paper also discusses the fixed point algorithm for obtaining the LASSO solution and the use of kernel smoothing in sparse regression. The adaptive ridge is compared with ridge regression and is shown to be an effective alternative. Additionally, the paper addresses hyper-parameters in LASSO and their role in determining model complexity. Overall, the results suggest that adaptive ridge is a powerful tool for sparse regression and can be used in conjunction with additive modeling and neural net training.",1
1411,"adaptive ridge is a special form of ridge regression , balancing the quadratic penalization on each parameter of the model . it was shown to be equivalent to lasso -lrb- least absolute shrinkage and selection operator -rrb- , in the sense that both procedures produce the same estimate . lasso can thus be viewed as a particular quadratic penalizer . from this observation , we derive a fixed point algorithm to compute the lasso solution . the analogy provides also a new hyper-parameter for tuning effectively the model complexity . we finally present a series ofpossi-ble extensions oflasso performing sparse regression in kernel smoothing , additive modeling and neural net training .",0
1412,"Natural Language Processing (NLP) has made significant progress in recent years, with many breakthroughs in the areas of machine translation, sentiment analysis, and text classification, among others. However, most of this progress has been focused on high-resource languages such as English, French, and Chinese, leaving less privileged languages behind. In this paper, we examine the state of NLP research for less privileged languages, discussing where we are today and where we need to go. We explore the challenges and opportunities associated with NLP for less privileged languages, including the lack of resources and data, the need for community involvement, and the importance of ethical considerations. We conclude with a call to action for the NLP community to work together to advance the field for all languages, regardless of their resource level.",1
1413,"in the context of the ijcnlp workshop on natural language processing for less privileged languages , we discuss the obstacles to research on such languages . we also briefly discuss the ways to make progress in removing these obstacles . we mention some previous work and comment on the papers selected for the workshop .",0
1414,"This paper proposes a novel approach called ""Behaviosites"" for manipulating the behavior of a multiagent system through parasitic infection. The Behaviosite paradigm involves the use of code modules that infect a distributed system of agents and manipulate their behavior. This distributed swarm control mechanism is inspired by biological parasites and can cause behavioral changes in the infected agents. The paper discusses the infection rate and the sensory activity of the Behaviosites, as well as their impact on social norms within the multiagent system. The authors present several experiments demonstrating the effectiveness of Behaviosites in changing the behavior of a swarm of distributed agents, and discuss potential applications of this approach, such as in swarm herding or distributed control.",1
1415,"in this paper we present the behaviosite paradigm , a new approach to coordination and control of distributed agents in a multiagent system , inspired by biological parasites with behavior manipulation properties . behaviosite paradigm are code modules that '' infect '' a multiagent system , attaching themselves to agents and altering the sensory activity and actions of those agents . these behavioral changes can be used to achieve altered , potentially improved , performance of the overall multiagent system ; thus , behaviosite paradigm provide a mechanism for distributed control over a distributed system . behaviosite paradigm need to be designed so that behaviosite paradigm are intimately familiar with the internal workings of the environment and of the agents operating within it . to demonstrate our approach , we use behaviosites to control the behavior of a swarm of simple agents . with a relatively low infection rate , a few behaviosites can engender desired behavior over the swarm as a whole : keeping it in one place , leading it through checkpoints , or moving the swarm from one stable equilibrium to another . we contrast behaviosites as a distributed swarm control mechanism with alternatives , such as the use of group leaders , herders , or social norms .",0
1416,"This paper presents an analysis of the generalization performance of game-theoretic machine learning algorithms. Specifically, the study focuses on parametric and non-parametric behavior learning methods and develops non-asymptotic error bounds for the behavior learning error. The paper proposes a generalization analysis framework that incorporates the behavior model and uniform convergence mechanism to establish the generalization error bounds. The techniques are applied to sponsored search internet applications with self-interested agents, and the results demonstrate the effectiveness of the proposed methods in analyzing the generalization performance of game-theoretic machine learning algorithms in real-world scenarios.",1
1417,"for internet applications like sponsored search , cautions need to be taken when using machine learning to optimize their mechanisms -lrb- e.g. , auction -rrb- since self-interested agents in these applications may change their behaviors -lrb- and thus the data distribution -rrb- in response to the mechanisms . to tackle this problem , a framework called game-theoretic machine learning -lrb- gtml -rrb- was recently proposed , which first learns a markov behavior model to characterize agents behaviors , and then learns the optimal markov behavior model by simulating agents ' behavior changes in response to the markov behavior model . while gtml has demonstrated practical success , its generalization analysis is challenging because the behavior data are non-i.i.d. and dependent on the markov behavior model . to address this challenge , first , we decompose the generalization error for gtml into the behavior learning error and the behavior learning error ; second , for the behavior learning error , we obtain novel non-asymptotic error bounds for both parametric and non-parametric behavior learning methods ; third , for the behavior learning error , we derive a uniform convergence bound based on a new concept called nested covering number of the mechanism space and the generalization analysis techniques developed for mixing sequences .",0
1418,"This paper proposes a probabilistic framework for characterizing physical-layer secrecy in wireless communication systems when the locations and channels of eavesdroppers are unknown. The proposed approach employs a Poisson point process to model the distribution of eavesdroppers, and accounts for path loss and Rayleigh fading effects. In addition, the paper discusses secure communication strategies, such as beamforming, in the context of the proposed framework. The results demonstrate the effectiveness of the proposed method in achieving physical-layer secrecy.",1
1419,-- we present a probabilistic framework for physical layer secrecy when the locations and channels of the eavesdrop-pers are unknown . the locations are modeled by a poisson point process . the channels include path loss and rayleigh fading . beamforming and frequency-selectivity of the fading channels are shown to greatly increase the probability of secure communications .,0
1420,"This paper proposes a method for directly modeling both voiced and unvoiced components in natural speech waveforms using neural networks. The method utilizes a non-zero mean Gaussian process to represent stochastic components, and it models the mean and covariance functions of the process with neural networks. The paper shows that this approach is effective for modeling both voiced and unvoiced components of speech waveforms, and it achieves state-of-the-art results on a benchmark dataset. The proposed method has potential applications in statistical parametric speech synthesis and acoustic modeling, where accurate modeling of speech waveforms is crucial for generating natural-sounding speech. The method also allows for the incorporation of linguistic features into the model, which further improves the quality of synthesized speech.",1
1421,"this paper proposes a novel acoustic model based on neural networks for statistical parametric speech synthesis . the neural networks outputs parameters of a non-zero mean gaussian process , which defines a probability density function of a speech waveform given linguistic features . the mean and covariance functions of the gaussian process represent deterministic -lrb- voiced -rrb- and stochastic components of a speech waveform , whereas the previous approach considered the unvoiced component only . experimental results show that the proposed approach can generate speech waveforms approximating natural speech waveforms .",0
1422,"This paper proposes an agent-based model for studying the acquisition of a language system that consists of logical constructions. The model considers the use of logical combinations of categories and Boolean functions to construct grammatical constructions. The process of language acquisition is modeled through the interplay of adoption, induction, and adaptation. The model provides insights into how logical constructions are acquired and how they evolve over time. The results show that the adoption of logical constructions depends on the complexity of the constructions and the cognitive resources of the agents. The model also suggests that the invention of new logical constructions is influenced by the agents' cognitive resources and the communicative demands of the environment. Overall, this paper offers a novel approach to studying language acquisition through an agent-based model that incorporates logical constructions.",1
1423,"this paper presents an agent-based model that studies the emergence and evolution of a language system of logical constructions , i.e. a vocabulary and a set of grammatical constructions that allows the expression of logical combinations of categories . the agent-based model assumes the agents have a common vocabulary for basic categories , the ability to construct logical combinations of categories using boolean functions , and some general purpose cognitive capacities for invention , adoption , induction and adaptation . but it does not assume the agents have a vocabulary for boolean functions nor grammatical constructions for expressing such logical combinations of categories through language . the results of the experiments we have performed show that a language system of logical constructions emerges as a result of a process of self-organisation of the individual agents ' interactions when these agents adapt their preferences for vocabulary and grammatical constructions to those they observe are used more often by the rest of the population , and that such a language system is transmitted from one generation to the next .",0
1424,"This paper proposes a novel classification framework for categorical data based on hyperplane classifiers on the multinomial manifold. We introduce margin-based hyperplane models on this manifold and generalize the concept of margin to this setting. The proposed linear classifiers take into account the categorical structure of the data and are based on multinomial models. We show that the Fisher information on the manifold can be used to derive the margin-based hyperplane classifiers, and we explore the geometry of the multinomial manifold. The proposed framework is particularly suitable for text classification tasks, where categorical data is prevalent. We illustrate the effectiveness of the proposed approach on several text classification benchmarks and demonstrate the advantage of incorporating the multinomial geometry and the Riemannian structure of the manifold in the classification process.",1
1425,"the assumptions behind linear classifiers for categorical data are examined and reformulated in the context of the multinomial manifold , the simplex of multinomial models furnished with the riemannian structure induced by the fisher information . this leads to a new view of hyperplane classifiers which , together with a generalized margin concept , shows how to adapt existing margin-based hyperplane models to multinomial geometry . experiments show the new classification framework to be effective for text classification , where the categorical structure of the data is modeled naturally within the multinomial family .",0
1426,"This paper proposes a novel approach for stereo correspondence based on trellis-based parallel matching. We present a sparsely connected trellis model for stereo correspondence, where the nodes in the trellis correspond to the disparity estimates of the stereo pairs. Natural constraints such as occlusion are incorporated into the model to improve the accuracy of the disparity estimates. The proposed method is capable of generating a parallel solution for stereo matching, which significantly reduces the computational complexity compared to existing methods. Our approach utilizes a center-referenced basis map to represent the stereo pairs in a discrete manner, which enables us to efficiently compute the correspondence. We demonstrate the effectiveness of our method on several benchmarks and show that it achieves state-of-the-art performance while maintaining a low computational cost. Overall, our trellis-based parallel stereo matching method provides a promising approach for stereo correspondence, especially for real-time applications where computational efficiency is crucial.",1
1427,"we present a center-referenced basis for discrete representation of stereo correspondence that includes new occlu-sion nodes . this basis improves the inclusion of constraints and the parallelism of the final algorithm . disparity estirna-tion is formulated in a map context and natural constraints are incorporated , resulting in an optimal path problem in a sparsely connected trellis . like other d p methods , the computational complexity is low at -lrb- 3 -lrb- m n 2 -rrb- for m x n pixel images . however , this method is better suited to parallel solution , scaling up to -lrb- 3 -lrb- m n -rrb- processors . experimental results confirm the performance of this method .",0
1428,"In this paper, we propose a Bayesian approach to spectrum sensing, denoising, and anomaly detection in cognitive radio networks. We develop a soft decision detector for spectrum sensing that is capable of accurately detecting the presence of primary users in the frequency band of interest. The proposed detector is based on a Bayesian approach, which enables us to incorporate prior knowledge about the signal and noise statistics to improve the detection performance. We also propose a signal denoising technique that is based on a Bayesian framework and can be used to reduce the noise in the received signal. To detect anomalies in the signal, we formulate an anomaly detection problem as a joint Bayesian inference problem, which allows us to detect and localize the anomalies in the received signal. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing methods in terms of detection accuracy and complexity. Overall, our approach provides a unified framework for spectrum sensing, denoising, and anomaly detection in cognitive radio networks, which can significantly enhance the performance and reliability of the system.",1
1429,"sion to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any copyrighted component of this work in other works must be obtained from the ieee . '' abstract this paper deals with the problem of discriminating samples that contain only noise from samples that contain a signal embedded in noise . the focus is on the case when the variance of the noise is unknown . we derive the optimal soft decision detector using a bayesian approach . the complexity of this optimal soft decision detector grows exponentially with the number of observations and as a remedy , we propose a number of approximations to soft decision detector . the problem under study is a fundamental one and soft decision detector has applications in signal denoising , anomaly detection , and spectrum sensing for cognitive radio . we illustrate the results in the context of the latter .",0
1430,"This paper proposes a competition-based score analysis approach for utterance verification in name recognition using selective components of an automatic speech recognition (ASR) system. We present a novel competition-based measurement framework for obtaining a sequence of sorted likelihood ratios from the n-best hypothesis testing of ASR system outputs. The proposed n-best utterance verification (UV) approach utilizes the sorted likelihood ratios to verify the correctness of the spoken name. The selective components of the ASR system are used to filter out irrelevant information and improve the verification performance. We also introduce a measurement score to evaluate the quality of the sorted likelihood ratios, which can be used to further improve the performance of the UV system. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing methods in terms of verification accuracy. Overall, our competition-based score analysis approach provides a promising solution for utterance verification in name recognition, which can enhance the performance and reliability of speech recognition systems in real-world applications.",1
1431,utterance utterance verification based on n-best hmm scores has been widely used in asr system . there are a number of ways to calculate a measurement score for utterance verification from n-best scores . most of proposed methods are based on the n-best uv approach of the hypothesis testing . this has lead to use the second best score or an overall average of available n-best scores for normalisation . in this study we examine n-best uv approach from a competition-based measurement framework . with this n-best uv approach different competitive measurements can be derived from a sequence of sorted likelihood ratios . the evaluation results demonstrate that oov performance can be improved by using some selective components in sorted likelihood ratios . in our experiments by using the first four components oov rejection errors can be reduced about 30 % in comparison with the baseline results .,0
1432,"In this paper, we propose a noise-robust template-based approach for bird-phrase segmentation and verification in continuous recordings. We evaluate our method on Cassin's vireo recordings, which pose significant challenges due to class variability and additive noise. Unlike existing energy or entropy-based birdsong segmentation algorithms, our approach is based on a birdsong-phrase segmentation and verification algorithm that employs a noise-robust, dynamic-time-warping (DTW) algorithm and a support vector machine (SVM) discriminative classifier. The segmentation algorithm utilizes manual annotations to learn a noise-robust template that captures the spectrogram amplitudes at segment boundaries. The verification algorithm then compares the template with the test segments to determine whether they belong to the same bird-phrase. To handle limited training data and class variability, we propose an outlier rejection technique that employs the discriminative classifier to identify and reject outlier segments. Our experiments show that our approach outperforms existing methods in terms of segmentation accuracy and verification precision. Our noise-robust template-based approach provides a promising solution for bird-phrase segmentation and verification in noisy environments, which can enhance the performance and reliability of automated bird monitoring systems.",1
1433,"in this paper , we present a birdsong-phrase segmentation and verification algorithm that is robust to limited training data , class variability , and noise . the birdsong-phrase segmentation and verification algorithm comprises a noise-robust , dynamic-time-warping - based segmentation and a discriminative classifier for outlier rejection . the birdsong-phrase segmentation and verification algorithm utilizes noise-robust , dynamic-time-warping and prominent -lrb- high energy -rrb- time-frequency regions of training spectrograms to derive a reliable noise-robust template for each phrase class . the resulting birdsong-phrase segmentation and verification algorithm is then used for segmenting continuous recordings to obtain segment candidates whose spectrogram amplitudes in the prominent regions are used as features to a support vector machine . the birdsong-phrase segmentation and verification algorithm is evaluated on the cassin 's vireo recordings ; our proposed birdsong-phrase segmentation and verification algorithm yields low equal error rates -lrb- eer -rrb- and segment boundaries that are close to those obtained from manual annotations and , is better than energy or entropy-based birdsong segmentation algorithms . in the presence of additive noise -lrb- -10 to 10 db snr -rrb- , the proposed birdsong-phrase segmentation and verification algorithm does not degrade as significantly as the other algorithms do .",0
1434,"This paper presents a method for recovering planar homographies between 2D shapes, which is an important task in computer vision. The proposed approach is based on establishing correspondences between points on the two shapes, and then solving for the homography using linearly independent functions and nonlinear equations. The method is applicable to real images of planar objects, and can handle segmentation errors and deformation. The proposed approach is evaluated on various datasets, including traffic signs, hip prosthesis x-ray images, and other real images, and the results show its effectiveness in recovering planar homographies. The use of binary images and linearly independent functions are highlighted as key features of the approach.",1
1435,"images taken from different views of a planar object are related by planar homography . recovering the parameters of such images is a fundamental problem in computer vision with various applications . this paper proposes a novel method to estimate the parameters of a homography that aligns two binary images . it is obtained by solving a system of nonlinear equations generated by integrating linearly independent functions over the domains determined by the shapes . the advantage of the proposed solution is that it is easy to implement , less sensitive to the strength of the deformation , works without established correspondences and robust against segmentation errors . the method has been tested on synthetic as well as on real images and its efficiency has been demonstrated in the context of two different applications : alignment of hip prosthesis x-ray images and matching of traffic signs .",0
1436,"This paper proposes a subspace decomposition method for point source localization in blurred images. The method is a 2-D generalization of the MUSIC (Multiple Signal Classification) algorithm for 1-D signals. It operates in the frequency domain and uses a covariance matrix to obtain an estimate of the signal subspace. The method then applies a rank enhancement technique to improve the accuracy of the subspace estimate. An array smoothing operation is performed to reduce the effect of noise on the subspace estimate. Finally, the proposed method uses a regularization operator to obtain a stable solution to the localization problem. Experimental results demonstrate the effectiveness of the proposed method in localizing point sources in intensity images that have been blurred.",1
1437,"in this paper we address the problem of redving blurred point sources in intensity images . a new approach to image restoration is introduced which is a 2-d generalization uf techniques originating from the field of direction of arrival estimation -lrb- doa -rrb- . i n the 2-d frequency domain . algorithms , such as music . may be adapted to search for these blurred point sources . a generalization of array smoothing bused on a regularization operator is introduced for 2-11 arrays in order to achieve rank enhanrmient in the signal space of the covariance mutrir .",0
1438,"This paper presents a query answering method for the Horn fragments of the expressive description logics SHOIQ and SROIQ. These logics are a non-trivial generalization of plain conjunctive queries and are used for reasoning about complex domains. The paper investigates the decidability of plain conjunctive queries in the DLS SHOIQ and SROIQ, and proposes a method for query answering in the Horn fragments of these logics. The computational complexity of the method is analyzed and it is shown to be 2EXPTIME-complete. The paper also discusses the complexity of SROIQ with nominals and the OWL standard, and how it relates to query answering in the full SHOIQ.",1
1439,"the high computational complexity of the expressive description logics that underlie the owl standard has motivated the study of their horn fragments , which are usually tractable in data complexity and can also have lower combined complexity , particularly for query answering . in this paper we provide algorithms for answering conjunc-tive 2-way regular path queries -lrb- 2crpqs -rrb- , a non-trivial generalization of plain conjunctive queries , in the horn fragments of the dls shoiq and sroiq underlying owl 1 and owl 2 . we show that the combined complexity of the problem is ex-ptime-complete for sroiq and 2exptime-complete for the more expressive sroiq , but is ptime-complete in data complexity for both . in contrast , even decidability of plain conjunctive queries is still open for full shoiq and sroiq . these are the first completeness results for query answering in expressive description logics with inverses , nominals , and counting , and show that for the considered logics the problem is not more expensive than standard reasoning .",0
1440,This paper proposes a method for improving sentence compression by mining Wikipedia revision histories. The authors use grammaticality and compression rate criteria to evaluate the quality of sentence compression. They apply a supervised sentence compression approach based on a lexical-ized noisy channel model and use the Ziff-Davis corpus as training data. The authors then mine Wikipedia revision histories to generate additional sentence compression training data. The resulting Wikipedia data is evaluated and compared with the Ziff-Davis corpus. The experimental results demonstrate that using the mined Wikipedia data improves the performance of sentence compression based on both grammaticality and compression rate criteria. The proposed method provides a new and effective way to improve sentence compression using large-scale data sources.,1
1441,"a well-recognized limitation of research on supervised sentence compression is the dearth of available training data . we propose a new and bountiful resource for such training data , which we obtain by mining the revision history of wikipedia for sentence compressions and expansions . using only a fraction of the available wikipedia data , we have collected a training corpus of over 380,000 sentence pairs , two orders of magnitude larger than the standardly used ziff-davis corpus . using this newfound data , we propose a novel lexical-ized noisy channel model for sentence compression , achieving improved results in gram-maticality and compression rate criteria with a slight decrease in importance .",0
1442,"This paper proposes a nonparametric Bayesian approach to enhance unsupervised feature learning algorithms by introducing stacked convolutional independent component analysis (SCICA). The proposed model uses deep unsupervised hierarchical feature extractors that can be trained in an unsupervised manner. The model is designed to overcome some of the challenges faced by traditional feature learning algorithms, such as tedious parameter tuning, overcomplete feature learning, and successive model layers. SCICA uses a hybrid variational inference algorithm and an Indian buffet process to infer latent components and features from high-dimensional data. The proposed model is evaluated on action recognition benchmarks and compared to other state-of-the-art unsupervised feature learning algorithms. The results show that SCICA outperforms other methods in terms of accuracy while avoiding tedious cross-validation procedures.",1
1443,"unsupervised feature learning algorithms based on con-volutional formulations of independent components analysis have been demonstrated to yield state-of-the-art results in several action recognition benchmarks . however , existing approaches do not allow for the number of latent components -lrb- features -rrb- to be automatically inferred from the data in an unsupervised manner . this is a significant disadvantage of the state-of-the-art , as it results in considerable burden imposed on researchers and practitioners , who must resort to tedious cross-validation procedures to obtain the optimal number of latent features . to resolve these issues , in this paper we introduce a convolutional nonpara-metric bayesian sparse ica architecture for overcomplete feature learning from high-dimensional data . our method utilizes an indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm , scalable to massive datasets . as we show , our model can be naturally used to obtain deep unsupervised hierarchical feature extractors , by greedily stacking successive model layers , similar to existing approaches . in addition , inference for this model is completely heuristics-free ; thus , it obviates the need of tedious parameter tuning , which is a major challenge most deep learning approaches are faced with . we evaluate our method on several action recognition benchmarks , and exhibit its advantages over the state-of-the-art .",0
1444,"This paper proposes a joint segmentation and parsing approach for Hebrew text using a PCFGLA lattice parser. The unsegmented Hebrew text poses challenges due to uncertain inputs, leading to errors in the syntactic model. To address this, the proposed methodology utilizes lattice parsing, which allows for error reduction by considering multiple possible analyses of the input. The PCFG-LA Berkeley parser is used in this framework, which incorporates a probabilistic context-free grammar and a lexicalized probabilistic model. Experimental results show that the proposed approach achieves a high f-score for parsing accuracy, demonstrating the effectiveness of the lattice parsing methodology and the PCFG-LA Berkeley parser in joint segmentation and parsing of Hebrew text.",1
1445,"we experiment with extending a lattice parsing methodology for parsing hebrew -lrb- gold-berg and tsarfaty , 2008 ; golderg et al. , 2009 -rrb- to make use of a stronger syntactic model : the pcfg-la berkeley parser . we show that the lattice parsing methodology is very effective : using a small training set of about 5500 trees , we construct a parser which parses and segments unseg-mented hebrew text with an f-score of almost 80 % , an error reduction of over 20 % over the best previous result for this task . this result indicates that lattice parsing with the pcfg-la berkeley parser is an effective lattice parsing methodology for parsing over uncertain inputs .",0
1446,"This paper discusses the topic of auditory visual speech processing and its interdisciplinary nature. The field involves the study of human-machine interaction, speech science, psycholinguistics, animation, psychology, and communication for the hearing-impaired. The paper highlights the importance of integrating knowledge from these fields to improve auditory visual speech processing. The paper also emphasizes the potential benefits of auditory visual speech processing, such as improved speech recognition and communication.",1
1447,"this paper provides an overview of the developments in auditory visual speech processing , a special interest group within eurospeech . i hope that this discussion will be informative and useful to readers in a variety of fields , including psychology , speech science , animation , psycholinguistics , human-machine interaction , hearing-impaired communication , and numerous other fields which also share in this fruitful intersection .",0
1448,"This paper proposes a method for acoustic class specific vocal tract length normalization (VTLN) using regression class trees. The method uses phonetic knowledge to define regression class trees for VTLN warp-factors, which can then be used for acoustic class specific VTLN. The proposed method is data-driven and uses the WSJ database, Gaussian components, and MFCC features. The paper shows that the proposed method outperforms the conventional VTLN method and CMLLR adaptation for acoustic class specific warp-factor estimation, and achieves improved recognition performance. The proposed method provides a promising approach for incorporating physiological differences in the vocal tract into acoustic models.",1
1449,"in this paper , we study the use of different frequency warp-factors for different acoustic classes in a computationally efficient framework of vocal tract length normalization . this is motivated by the fact that all acoustic classes do not exhibit similar spectral variations as a result of physiological differences in vocal tract , and therefore , the use of a single frequency-warp for the entire utterance may not be appropriate . we have recently proposed a vtln method that implements vocal tract length normalization through a linear-transformation of the conventional mfcc features and efficiently estimates the warp-factor using the same sufficient statistics as that are used in cmllr adaptation . in this paper we have shown that , in this framework of vocal tract length normalization , and using the idea of regression class tree , we can obtain separate vocal tract length normalization for different acoustic classes . the use of regression class tree ensures that warp-factor is estimated for each class even when there is very little data available for that class . the acoustic classes , in general , can be any collection of the gaussian components in the acoustic model . we have built acoustic classes by using data-driven approach and by using phonetic knowledge . using wsj database we have shown the recognition performance of the proposed acoustic class specific warp-factor both for the data driven and the phonetic knowledge based regression class tree definitions and compare vtln method with the case of the single warp-factor .",0
1450,"This paper presents advances in packet-loss concealment technology for Enhanced Voice Services (EVS). EVS is a standardized codec that provides high-quality voice communication in mobile services. One of the key technical features of EVS is its robustness to packet loss and jitter buffer management. This paper evaluates the reference codecs used in EVS and highlights the improvements in error resilience achieved through packet-loss concealment. The results show that EVS provides improved voice quality and robustness compared to other codecs, making it a suitable choice for Voice over LTE (VoLTE) services.",1
1451,"evs , the newly standardized 3gpp codec for enhanced voice services was developed for mobile services such as volte , where error resilience is highly essential . the presented paper outlines all aspects of the advances brought during the evs development on packet loss concealment , by presenting a high level description of all technical features present in the final standardized codec . coupled with jitter buffer management , the enhanced voice services provides robustness against late or lost packets . the advantages of the new enhanced voice services over reference codecs are further discussed based on listening test results .",0
1452,"This paper presents a novel approach for content-based image retrieval through the extraction of detailed image regions. Specifically, the proposed technique utilizes the directional detail histogram and sum-of-angles criterion to create perceptually modified distance measures. These measures enable effective histogram thresholding techniques and database indexing for natural color images. The method is evaluated on an image database, and the results show significant improvements in edge and texture retrieval compared to previous approaches. The paper concludes that the proposed technique is a promising method for detailed image region extraction and content-based image retrieval.",1
1453,"we present a technique for coarsely extracting the regions of natural color images which contain directional detail , e.g. , edges , texture , etc. , which we then use for image database indexing . as a measure of color activity , we use a perceptually modified distance measure based on the sum-of-angles criterion . we then apply histogram thresholding techniques to separate the image into smooth color regions and busy regions where edge , texture and colour activity exists . database indices are then created from the busy regions using the direcional detail histogram technique and retrieval is performed using these .",0
1454,"This paper proposes a new scene parsing algorithm for indoor images that integrates function, geometry, and appearance models. The algorithm employs a function-geometry-appearance hierarchy, where 3D geometric shapes and indoor functional objects are represented by stochastic grammar models. The algorithm starts with a posteriori solution and uses a Metropolis-Hastings acceptance probability algorithm to recover missing objects/parts in indoor scenes. The algorithm uses 2D segmentation maps and a parse tree to label functional objects and groups, which are then combined to recover 3D shapes and functional parts. The algorithm also employs appearance-based classification paradigms to recognize object function. Experimental results on indoor datasets show that the proposed algorithm outperforms existing state-of-the-art methods in scene parsing accuracy.",1
1455,"indoor functional objects exhibit large view and appearance variations , thus are difficult to be recognized by the traditional appearance-based classification paradigm . in this paper , we present an algorithm to parse indoor images based on two observations : i -rrb- the functionality is the most essential property to define an indoor object , e.g. '' a chair to sit on '' ; ii -rrb- the geometry -lrb- 3d shape -rrb- of an object is designed to serve its function . we formulate the nature of the object function into a stochastic grammar model . this model characterizes a joint distribution over the function-geometry-appearance hierarchy . the joint distribution includes a scene category , functional groups , functional objects , functional parts and 3d geometric shapes . we use a simulated annealing mcmc algorithm to find the maximum a posteriori solution , i.e. a parse tree . we design four data-driven steps to accelerate the search in the fga space : i -rrb- group the line segments into 3d primitive shapes , ii -rrb- assign functional labels to these 3d primitive shapes , iii -rrb- fill in missing objects/parts according to the functional labels , and iv -rrb- synthesize 2d segmentation maps and verify the current parse tree by the metropolis-hastings acceptance probability . the experimental results on several challenging indoor datasets demonstrate the proposed approach not only significantly widens the scope of indoor scene parsing algorithm from the segmentation and the 3d recovery to the functional object recognition , but also yields improved overall performance .",0
1456,This paper proposes a novel approach for improving music auto-tagging by utilizing the temporal information of music signals. The approach employs an intra-song instance bagging method which bootstraps short-time features from multiple instances within a song. This ensemble learning technique is compared with the inter-song instance bagging method and meta-algorithm bootstraps song-level features. The experimental results show that the proposed approach outperforms the baseline method in terms of tag prediction accuracies. The paper concludes that incorporating bagging methods into music auto-tagging can be a useful tool for improving the performance of MIR systems. The proposed method could be a valuable contribution to the literature on machine learning for music analysis.,1
1457,"bagging is one the most classic ensemble learning techniques in the machine learning literature . the idea is to generate multiple subsets of the training data via bootstrapping -lrb- random sampling with replacement -rrb- , and then aggregate the output of the models trained from each subset via voting or averaging . as music is a temporal signal , we propose and study two bagging methods in this paper : the inter-song instance bagging that bootstraps song-level features , and the intra-song instance bagging that draws bootstrapping samples directly from short-time features for each training song . in particular , we focus on the latter method , as bagging methods better exploits the temporal information of music signals . the bagging methods result in surprisingly effective models for music auto-tagging : incorporating the idea to a simple linear support vector machine -lrb- svm -rrb- based system yields accuracies that are comparable or even superior to state-of-the-art , possibly more sophisticated methods for three different datasets . as the bagging methods is a meta algorithm , bagging methods holds the promise of improving other mir systems .",0
1458,"This paper proposes a novel embedding method for an anti-collusion fingerprinting system that combines both a code and an orthogonal fingerprint. The proposed method is based on code modulation and uses basis vectors to embed the fingerprint. The code modulation embedding method is used for the fingerprinting system, in conjunction with an orthogonal fingerprint, to provide an additional layer of security against collusion attacks. The proposed embedding method also includes an anti-collusion code to protect against linear combination collusion attacks. The detection of linear combination collusion attacks is achieved through the use of the embedding method, which incorporates code modulation. The fingerprinting system employs both the code modulation embedding method and the anti-collusion code to protect against collusion attacks. The paper concludes that the proposed embedding method is effective in protecting against linear combination collusion attacks and provides an additional layer of security in fingerprinting systems.",1
1459,"in this paper , a fingerprint embedding method better-suited for the and anti-collusion code is proposed . the proposed fingerprint embedding method embeds both a code and an orthogonal fingerprint using different basis vectors depending on the bit . although the detection for the embedding method is complex , the performance of the fingerprinting system using proposed embedding method with the and-acc against average attack is improved compared with the and anti-collusion code using code modulation embedding method . the fingerprinting system using the proposed embedding method is robust against the linear combination collusion attack whereas the fingerprinting system using the code modulation is not .",0
1460,"This paper explores the topic of narrative prose generation, with a focus on improving poor writing quality in natural language generation. The paper presents a narrative prose generator that aims to improve the overall prose quality by using narrative theory to guide the story generation process. The generator is based on an author architecture that incorporates story grammars to increase the complexity of the generated stories. The paper presents a detailed analysis of corpora to identify common patterns in narrative prose and to inform the design of the generator. The analyses of corpora and narrative theory inform the plot design of the generator, resulting in an improved quality of the generated stories. The paper concludes that the proposed narrative prose generator is effective in improving the poor writing quality in natural language generation and provides a promising approach for future research in this area.",1
1461,"story generation is experiencing a revival , despite disappointing preliminary results from the preceding three decades . one of the principle reasons for previous inadequacies was the low level of writing quality , which resulted from the excessive focus of story grammars on plot design . although these systems leveraged narrative theory via corpora analyses , they failed to thoroughly extend those analyses to all relevant linguistic levels . the end result was narratives that were recognizable as stories , but whose prose quality was unsatisfactory . however , the blame for poor writing quality can not be laid squarely at the feet of story grammars , as natural language generation has to-date not fielded systems capable of faithfully reproducing either the variety or complexity of naturally occurring stories . this paper presents the author architecture for accomplishing precisely that task , the storybook implementation of a narrative prose generator , and a brief description of a formal evaluation of the stories it produces .",0
1462,"This paper presents a robust adaptive beamforming and steering vector estimation approach for partly calibrated sensor arrays, using a structured ellipsoidal uncertainty model. The paper focuses on mitigating intersubarray gain and/or phase mismatches, as well as gain-and-phase and phase-only intersubarray distortions, in sparse subarray-based sensor arrays. The proposed approach employs a minimum variance beamformer and worst-case beamformer design to estimate the signal steering vector in the presence of these distortions. The structured ellipsoidal uncertainty model is used to represent the uncertainties in the array calibration, and the worst-case designs are obtained by solving a convex optimization problem that accounts for these uncertainties. The paper concludes that the proposed approach is effective in robust adaptive beamforming and steering vector estimation for partly calibrated sensor arrays, and provides a promising solution for future research in this area.",1
1463,"two new approaches to adaptive beamforming in sparse subarray-based sensor arrays are proposed . each subarray is assumed to be well calibrated but the intersubarray gain and/or phase mismatches are assumed to remain unknown or imperfectly known . our first approach is based on a worst-case beamformer design that , unlike the existing worst-case designs , exploits a structured ellipsoidal uncertainty model for the signal steering vector . our second approach exploits the idea of estimating the signal steering vector by maximizing the output power of the minimum variance beamformer . several modifications of our second approach are developed for the cases of gain-and-phase and phase-only intersubarray distortions .",0
1464,"This paper proposes a novel framework called CrowdMR, which integrates crowdsourcing with MapReduce for solving AI-hard problems. The paper highlights the limitations of completely automated public Turing tests (CAPTCHAs) in solving complex problems and presents CrowdMR as a solution to overcome these limitations. The framework employs a human-machine solution to solve AI-hard problems by integrating crowdsourcing and MapReduce. The paper presents an incremental scheduling method that improves the accuracy of large-scale distributed computing by optimizing the allocation of tasks to workers. The paper also discusses the benefits of using crowdsourcing in the human-machine solution and the advantages of using MapReduce in the framework. The paper concludes that CrowdMR is an effective framework for integrating crowdsourcing with MapReduce to solve AI-hard problems and provides a promising approach for future research in this area.",1
1465,"large-scale distributed computing has made available the resources necessary to solve '' ai-hard '' problems . as a result , it becomes feasible to automate the processing of such problems , but accuracy is not very high due to the conceptual difficulty of these problems . in this paper , we integrated crowdsourcing with mapreduce to provide a scalable innovative human-machine solution to ai-hard problems , which is called crowdmr . in crowdmr , the majority of problem instances are automatically processed by machine while the troublesome instances are redirected to human via crowdsourcing . the results returned from crowdsourcing are validated in the form of captcha -lrb- completely automated public turing test to tell computers and humans apart -rrb- before adding to the output . an incremental scheduling method was brought forward to combine the results from machine and human in a '' pay-as-you-go '' way .",0
1466,"This paper presents a novel non-linear domain adaptation approach with boosting, which addresses the problem of adapting models from one domain to another without requiring labeled data in the target domain. The paper proposes a parameter-free domain adaptation method based on a multi-task learning algorithm that learns a shared feature space and task-specific decision boundaries in a global analytical form. The proposed approach employs a non-linear mapping between the source and target domains to enable effective transfer of knowledge from the source domain to the target domain. The paper evaluates the proposed approach on 3D data and bio-medical datasets for machine vision and bio-medical applications, respectively. The paper shows that the proposed approach outperforms state-of-the-art domain adaptation methods and achieves high accuracy with minimal distribution annotation. The paper also introduces a boosting-trick to further enhance the performance of the multi-task learning algorithm. The paper concludes that the proposed non-linear domain adaptation approach with boosting is an effective method for addressing the domain adaptation problem and provides a promising approach for future research in this area.",1
1467,"a common assumption in machine vision is that the training and test samples are drawn from the same distribution . however , there are many problems when this assumption is grossly violated , as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions . this machine vision is accentuated with 3d data , for which annotation is very time-consuming , limiting the amount of data that can be labeled in new acquisitions for training . in this paper we present a multi-task learning algorithm for domain adaptation based on boosting . unlike previous approaches that learn task-specific decision boundaries , our multi-task learning algorithm learns a single decision boundary in a shared feature space , common to all tasks . we use the boosting-trick to learn a non-linear mapping of the observations in each task , with no need for specific a-priori knowledge of its global analytical form . this yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce . we evaluate our multi-task learning algorithm on two challenging bio-medical datasets and achieve a significant improvement over the state of the art .",0
1468,"This paper proposes a memory-efficient dynamic programming algorithm for learning optimal Bayesian networks with a layered structure. The algorithm is based on recursive decomposition of the Bayesian network and employs binomial coefficient calculations to determine the optimal structure. The paper demonstrates that the proposed algorithm reduces the memory requirements of traditional dynamic programming approaches while maintaining the same level of accuracy. The paper highlights the importance of dynamic programming in learning Bayesian networks and shows how the layered structure of the network can be exploited to reduce the computational complexity of the algorithm. The paper also discusses the relationship between the layered structure and the recursive decomposition of the network. The proposed algorithm is evaluated on a range of benchmark datasets, and the results show that it outperforms existing memory-efficient algorithms while achieving similar accuracy to traditional dynamic programming approaches. The paper concludes that the proposed memory-efficient dynamic programming algorithm provides a promising approach for learning optimal Bayesian networks and can be applied to a wide range of applications in machine learning and data mining.",1
1469,"we describe a memory-efficient implementation of a dynamic programming algorithm for learning the optimal structure of a bayesian network from training data . the dynamic programming algorithm leverages the layered structure of the dynamic programming graphs representing the re-cursive decomposition of the problem to reduce the memory requirements of the dynamic programming algorithm from o -lrb- n2 n -rrb- to o -lrb- c -lrb- n , n/2 -rrb- -rrb- , where c -lrb- n , n/2 -rrb- is the binomial coefficient . experimental results show that the dynamic programming algorithm runs up to an order of magnitude faster and scales to datasets with more variables than previous approaches .",0
1470,"This paper proposes a new method for measuring the creaseness of an image landscape based on level set extrinsic curvature. The method focuses on identifying the discontinuities in the level curve curvature of an image, which are indicative of creases or sharp folds. By analyzing the local conditions of these creases, the proposed creaseness measure can distinguish between different types of creases and provide a more comprehensive characterization of the image's shape. The paper also discusses the relationship between creases and other geometric features such as medialness and extrema of the level set curvatures. The effectiveness of the proposed method is demonstrated through experiments on both synthetic and real-world images, showcasing the advantages of the proposed creaseness measure over traditional methods in terms of accuracy and robustness.",1
1471,"creases are a type of ridge/valley -lcb- like structures of a d dimensional image , characterized by local conditions . as creases tend to be at the center of anisotropic grey -lcb- level shapes , creaseness can be considered as a type of medialness . among the several crease deenitions , one of the most important is based on the extrema of the level set curvatures . in 2 -lcb- d it is used the curvature of the level curves of the image landscape , however , the way it is usually computed produces a discon-tinuous creaseness measure . the same problem arises in 3 -lcb- d with its straightforward extension and with other related creaseness measures . in this paper , we rst present an alternative method of computing the level curve curvature that avoids the discontinuities . next , we propose the mean curvature of the level surfaces as creaseness measure of 3 -lcb- d images , computed by the same method . finally , we propose a natural extension of our rst alternative method in order to enhance the creaseness measure .",0
1472,"This paper focuses on the acquisition of second language intonation, specifically the phonological and phonetic properties of intonation in English spoken by Korean speakers. The study examines the segmental and prosodic features of the language and investigates the surface tonal realizations of American English speakers and Korean speakers learning English as a second language. The research analyzes the prosodic structure and phonological system of the language to understand the segment-tone interaction that contributes to foreign accents. The results show that the acquisition of native-like intonation patterns is challenging for L2 speakers due to the influence of their first language, as evidenced by the analysis of segmental data and the comparison of phonological and phonetic properties of intonation.",1
1473,"foreign accents in second language production are caused by interference from the phonological system and phonetic realization of the speaker 's first language -lrb- l1 -rrb- , including both segmental and prosodic features . this paper examines the intonation structure of seoul korean and its realization by american english speakers . four english speakers of korean , differing in fluency , and two korean speakers participated in the experiment . forty sentences were designed to test the realization of intonation patterns by varying the number of syllables within a word and a sentence , and by varying the conditions for the segment-tone interaction . results show that , as with segmental data , more advanced l2 speakers produce more native-like intonation patterns and prosodic structure than less advanced speakers . however , although advanced l2 speakers are better at grouping words into phrases , l2 speakers are not better at producing surface tonal realizations of an accentual phrase than less advanced speakers . this suggests that phonological properties of intonation are acquired earlier than phonetic properties of intonation .",0
1474,"This paper presents an analysis of a polyphase IIR adaptive filter, including its error surface and application. The filter is compared to a direct form IIR adaptive filter and its local and global convergence properties are examined. The adverse effects of underdamped low-frequency poles are also discussed. Constant gain algorithms are found to be useful for direct form IIR adaptive filters. In contrast, the polyphase IIR adaptive filter has improved local convergence speed and global convergence speed. A reduced error surface is achieved by modeling the system with complex poles on the unit circle and real axis. Finally, the computational complexity of the direct form and polyphase IIR adaptive filters are compared.",1
1475,"an analysis of the local convergence speed of constant gain algorithms for direct form iir adaptive ¯ lters is initially presented , showing the adverse e ® ects that result from the proximity of the poles of the modelled system to the unit circle and , for complex poles , to the real axis . a global analysis of the reduced error surface in these cases is also presented , which shows that , away from the global minimum , there will be regions with an almost constant error , where the convergence of constant gain algorithms tends to be slow . a polyphase iir adaptive ¯ lter is then proposed and its local and global convergence properties are investigated , showing polyphase iir adaptive ¯ lter to be specially well suited for applications with underdamped low-frequency poles . the polyphase iir adaptive ¯ lter is tested with di ® erent constant gain algorithms in an echo-cancellation example , attaining a gain of 14 to 70 times in global convergence speed over the direct form , at the price of a relatively modest increase in computational complexity . a theorem concerning the existence of stationary points for the polyphase iir adaptive ¯ lter is also presented .",0
1476,"This paper proposes a novel method for localizing multiple sound sources using a microphone array and a Cross-Power Spectrum Phase (CSP) analysis. The proposed method is based on high-quality sound capture using a microphone array and single sound source localization using different microphone pairs. The method leverages the properties of the CSP method to remove undesired cross-correlations and improve localization accuracy. The proposed method is evaluated using a synchronous addition of CSP coefficients, and the results show that it achieves accurate localization of multiple sound sources. The proposed method has potential applications in areas such as speech recognition, speaker tracking, and source separation.",1
1477,"accurate localization of multiple sound sources is indispensable for the microphone array-based high quality sound capture . for single sound source localization , the csp -lrb- cross-power spectrum phase analysis -rrb- method has been proposed . the csp -lrb- cross-power spectrum phase analysis -rrb- method localizes a sound source as a crossing point of sound directions estimated using dier-ent microphone pairs . however , when localizing multiple sound sources , the csp -lrb- cross-power spectrum phase analysis -rrb- method has a problem that the localization accuracy is degraded due to cross-correlation among dierent sound sources . to solve this problem , this paper proposes a new method which suppresses the un-desired cross-correlation by synchronous addition of csp coecients derived from multiple microphone pairs . experiment results in a real room showed that the proposed method improves the localization accuracy when increasing the number of the synchronous addition .",0
1478,"This paper presents a learning framework for neural network policies using guided policy search under unknown dynamics. The proposed method employs time-varying linear dynamics models and a policy search technique to learn trajectory distributions for simulated robotic manipulation tasks. The approach utilizes local linear models and model-free methods to handle nonsmooth dynamics and underactuation. The guided policy search method is used to generate policies with an arbitrary parameterization, and trajectory distributions are learned to handle contact discontinuities. The paper also discusses the use of global models and model-based techniques to improve policy search under unknown dynamics. Experimental results demonstrate the effectiveness of the proposed method for high-quality sound capture and single sound source localization.",1
1479,"we present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large , continuous problems . these trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization . our policy search method fits time-varying linear dynamics models to speed up learning , but does not rely on learning a global model , which can be difficult when the dynamics are complex and discontinuous . we show that this policy search method requires many fewer samples than model-free methods , and can handle complex , nonsmooth dynamics that can pose a challenge for model-based techniques . we present experiments showing that our policy search method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation .",0
1480,"This paper proposes a novel approach for multimodal classification using Conditioned Hidden Markov Model (HMM) Fusion. Traditional HMMs have been extensively used for classification, but their performance can be limited in naturalistic, multi-party dialogues. To address this issue, the authors introduce a fusion strategy that conditions the HMMs on the late fusion approaches. The proposed approach can effectively combine multiple modalities to improve classification accuracy. Experimental results on a dataset of affective computing show that the proposed approach outperforms classical HMMs, and other state-of-the-art models. The proposed fusion strategy can be easily applied to any HMM-based classification problem to improve its accuracy.",1
1481,"classification using hidden markov models is in general done by comparing the model likelihoods and choosing the class more likely to have generated the data . this work investigates a conditioned hmm which additionally provides a probability for a class label and compares different fusion strategies . the notion is twofold : on the one hand applications in affec-tive computing might pass their uncertainty of the classification to the next processing unit , on the other hand different streams might be fused to increase the performance . the data set studied incorporates two modalities and is based on a naturalistic mul-tiparty dialogue . the goal is to discriminate between laughter and utterances . it turned out that the conditioned hmm out-performs classical hmm using different late fusion approaches while additionally providing a certainty about class decision .",0
1482,"This paper presents an efficient and robust spatio-temporal interest point detector that is both dense and scale-invariant. The proposed detector operates using approximative box-filter operations on the integral video structure, and it can extract features at user-defined scales. The detector employs a scale-space theory and a saliency measure to identify the scale-invariant features. The Hessian matrix is used to compute the spatio-temporal interest points efficiently. The proposed detector is evaluated on video content for action recognition, and it achieves a good balance between speed and accuracy. The experimental results demonstrate the effectiveness of the proposed detector in comparison to other state-of-the-art spatio-temporal interest point detectors.",1
1483,"over the years , several spatio-temporal interest point detectors have been proposed . while some detectors can only extract a sparse set of scale-invariant features , others allow for the detection of a larger amount of features at user-defined scales . this paper presents for the first time spatio-temporal interest points that are at the same time scale-invariant -lrb- both spatially and temporally -rrb- and densely cover the video content . moreover , as opposed to earlier work , the features can be computed efficiently . applying scale-space theory , we show that features can be achieved by using the determinant of the hessian as the saliency measure . computations are speeded-up further through the use of approximative box-filter operations on an integral video structure . a quantitative evaluation and experimental results on action recognition show the strengths of the proposed detector in terms of repeatability , accuracy and speed , in comparison with previously proposed detectors .",0
1484,"This paper proposes a new method for modeling reflections in images using multiperspective imaging. The approach takes into account the different perspectives and camera types involved in capturing the image, as well as the caustic surfaces and irregular mirror surfaces that can produce reflections. The method is based on a general linear camera model that is extended to include multiperspective cameras and catadioptric imaging systems. An analytical framework is developed to locally model reflections and mirror design, while also accounting for image distortions and arbitrary surfaces. The proposed approach allows for accurate modeling of reflections in a variety of imaging scenarios, with potential applications in computer vision and image processing.",1
1485,"we present a novel method for analyzing reflections on arbitrary surfaces . we model reflections using a broader than usual class of imaging models , which include both perspective and multiperspective camera types . we provide an analytical framework to locally model reflections as specific multiperspective cameras around every ray based on a new theory of general linear cameras . our analytical framework better characterizes the complicated image distortions seen on irregular mirror surfaces as well as the conventional cata-dioptric mirrors . we show the connection between multiper-spective camera models and caustic surfaces of reflections and demonstrate how multiper-spective camera models reveal important surface rulings of the caustics . finally , we show how to use our analytical framework to assist mirror design and characterize distortions seen in catadioptric imaging systems .",0
1486,"This paper presents a novel approach for a real-time software MPEG transcoder that uses motion vector reuse and SIMD optimization techniques to improve performance. The transcoder uses a mean absolute error approximation criteria to achieve quality while minimizing quality degradation. The proposed motion vector reuse technique scales motion vectors and reduces the amount of data transferred between different frames. The SIMD optimization techniques help to parallelize the processing of data, reducing computational load. The results show that the proposed approach achieves a high degree of real-time performance while maintaining acceptable video quality, and outperforms existing approaches that use similar techniques.",1
1487,"a realtime software mpeg transcoder has been developed . a novel motion vector reuse and a simd optimization techniques are introduced to accelerate the transcoder without any quality degradation . mean absolute error approximation criteria are employed in the reuse technique to refine scaled motion vectors . the developed transcoder on pentium ii 266mhz runs 2.5 times as fast as realtime , when scaling an mpeg-1 bitstream to half size .",0
1488,"This paper presents an analysis and modeling approach for predicting the timing of the next speaking start in natural multi-party meetings, based on gaze behavior. The authors examine the gaze transition patterns in these meetings and extract features that are used to develop a prediction model. The model is incorporated into a conversational interface agent system to enable more natural and efficient communication in multi-party meetings. The results demonstrate the effectiveness of the proposed approach in accurately predicting the next speaking start timing, and highlight the potential of gaze behavior as a valuable input for conversational interfaces in multi-party meetings.",1
1489,"to realize a conversational interface where an agent system can smoothly communicate with multiple persons , it is imperative to know how the start timing of speaking is decided . in this research , we demonstrate a relationship between gaze transition patterns and the start timing of next speaking against the end of the last speaking in multi-party meetings . then , we construct a prediction model for the start timing using gaze transition patterns near the end of an utterance . an analysis of data collected from natural multi-party meetings reveals a strong relationship between gaze transition patterns of the speaker , next speaker , and listener and the start timing of the next speaker . on the basis of the results , we used gaze transition patterns of the speaker , next speaker , and listener and mutual gaze as variables , and devised several prediction models . a prediction model using all features performed the best and was able to predict the start timing well .",0
1490,"This paper proposes a Bayesian tensor inference approach for sketch-based facial photo hallucination, which synthesizes high-resolution facial images from low-resolution sketches. The method uses a patch-based tensor model in both sketch and photo patch spaces and incorporates cooperative factors between image appearance and style to enable bidirectional mapping/inference between the two spaces. Specifically, the proposed statistical inference approach models the common variation space between sketch and photo images and leverages the cooperative factors to generate high-quality photo images. Experimental results demonstrate that the proposed approach outperforms other state-of-the-art methods in terms of visual quality and fidelity of the synthesized images.",1
1491,"this paper develops a statistical inference approach , statistical inference approach , for style transformation between photo images and sketch images of human faces . motivated by the rationale that image appearance is determined by two cooperative factors : image content and image style , we first model the interaction between these factors through learning a patch-based tensor model . second , by introducing a common variation space , we capture the inherent connection between photo patch space and sketch patch space , thus building bidirectional mapping/inferring between the two spaces . subsequently , we formulate a bayesian approach accounting for the statistical inference from sketches to their corresponding photos in terms of the learned tensor model . comparative experiments are conducted to contrast the proposed bayesian approach with state-of-the-art algorithms for facial sketch synthesis in a novel face hallucination scenario : sketch-based facial photo hallucination . the encouraging results obtained convincingly validate the effectiveness of our bayesian approach .",0
1492,"This paper presents a sub-band feedback cancellation system with variable step sizes for music signals in hearing aids. The system employs adaptive feedback cancellation algorithms and prediction error filtering to mitigate distortion artifacts caused by tonal signals. The proposed system incorporates control concepts such as biased adaptation and setup for frequency shifting to optimize the cancellation process. The computational complexity of the system is reduced by the use of variable step sizes. The experimental results show that the system effectively cancels feedback with minimal degradation of music signals, making it suitable for use in hearing aids.",1
1493,"standard adaptive feedback cancellation algorithms in hearing aids suffer from a biased adaptation if the input signal is spectrally colored , as it is for tonal signals , like music . due to that , distortion artifacts are generated . in this paper , a sub-band feedback cancellation system is presented combined with an adaptation control to deal with those signals . two control concepts for determining the variable step sizes -lsb- 1 , 2 -rsb- , known from general adaptive filter algorithms , are theoretically and practically analyzed and evaluated for an application to feedback cancellation . for feedback cancellation the control is combined with known methods to reduce the bias , such as prediction error filtering or frequency shifting . based on this combination , a completely new setup for feedback cancellation is proposed . setup relies entirely on signals accessible in real systems , shows a low computational complexity , and therefore has a strong practical relevance .",0
1494,"This paper proposes Wasserstein training of Restricted Boltzmann Machines (RBM), which are commonly used in generative models for data completion and denoising tasks. Traditional training methods for RBMs, such as the Kullback-Leibler divergence, have limitations in capturing the statistical properties of the underlying data. By using the Wasserstein distance, the authors aim to overcome these limitations and improve the model's ability to learn from data with complex distributions. The paper discusses the mathematical foundations of the proposed method and provides empirical evidence of its effectiveness in learning model parameters through gradient descent. The results show that Wasserstein training can improve the denoising performance of RBMs, leading to more accurate data completion.",1
1495,"boltzmann machines are able to learn highly complex , multimodal , structured and multiscale real-world data distributions . parameters of the model are usually learned by minimizing the kullback-leibler divergence from training samples to the learned model . we propose in this work a novel approach for boltzmann machine training which assumes that a meaningful metric between observations is given . this metric can be represented by the wasserstein distance between distributions , for which we derive a gradient with respect to the model parameters . minimization of this new objective leads to generative models with different statistical properties . we demonstrate their practical potential on data completion and denoising , for which the metric between observations plays a crucial role .",0
1496,"This paper proposes a method for cross-lingual opinion analysis using negative transfer detection. Cross-lingual opinion analysis involves analyzing opinions expressed in different languages. However, a challenge in this task is negative transfer, which occurs when knowledge learned from one language interferes with the analysis of another language. The proposed method detects negative transfer by using transductive transfer learning and cumulative class noise. The method is evaluated on a cross-lingual opinion analysis dataset from NLP&CC 2013, and the results show that the proposed method outperforms other methods that do not detect negative transfer. The paper concludes that the proposed method is effective for cross-lingual opinion analysis and can be used to improve the accuracy of other transfer learning tasks that suffer from noises.",1
1497,"transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages . however , the cumulative class noise in transfer learning adversely affects performance when more training data is used . in this paper , we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers . evaluation on nlp&cc 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems . more significantly , our system shows a monotonic increase trend in performance improvement when more training data are used .",0
1498,"This paper proposes an adaptive channel prediction technique for coded OFDM systems with time-varying channels. The approach utilizes normalized least-mean-square (NLMS) and recursive least-squares (RLS) algorithms to generate adaptive channel predictors that can accurately estimate the channel state at each OFDM subcarrier. The predicted channel information is then used to perform delay-free equalization, which mitigates the effects of channel distortion. Simulation results show that the proposed technique outperforms existing methods in terms of bit-error-rate (BER) and channel estimation accuracy. The proposed approach has potential applications in wireless communication systems that utilize OFDM modulation.",1
1499,"we propose adaptive channel predictors for orthogonal frequency division multiplexing communications over time-varying channels . successful application of the normalized least-mean-square and recursive least-squares algorithms is demonstrated . we also consider the use of adaptive channel pre-dictors for delay-free equalization , thereby avoiding the need for regular transmission of pilot symbols . simulation results demonstrate the good performance of the proposed techniques .",0
1500,"This paper proposes a method to predict brand preferences using an individual's personal traits. The approach leverages automated social media analytics and a psychometric survey to extract ground truth information about an individual's brand preferences and personal values. The analysis of the data aims to identify the individual's character traits and personality, which are then used as features in a predictive model for brand preference. The study shows promising results, indicating that personal traits can be used to accurately predict brand preference.",1
1501,"in this paper , we present a comprehensive study of the relationship between an indi-vidual 's personal traits and his/her brand preferences . in our analysis , we included a large number of character traits such as personality , personal values and individual needs . these character traits were obtained from both a psychometric survey and automated social media analytics . we also included an extensive set of brand names from diverse product categories . from this analysis , we want to shed some light on -lrb- 1 -rrb- whether it is possible to use personal traits to infer an individual 's brand preferences -lrb- 2 -rrb- whether the character traits automatically inferred from social media are good proxies for the ground truth character traits in brand preference prediction .",0
1502,This paper presents a new approach to motor simulation using coupled internal models and sequential Monte Carlo methods. Motor simulation involves the reenactment of internal model pairs to understand action and perception in real-world scenarios. The paper proposes an inverse-forward internal model pair that can be used to generate hypotheses and make predictions about actions. The proposed approach uses a generative Bayesian model and sequential Monte Carlo methods to perform approximate inference and to compute the prediction error. The method is computationally efficient and allows for action recognition in real-world scenarios. The paper concludes by evaluating the method on various tasks and showing that it outperforms other methods in terms of accuracy and computational resources.,1
1503,"we describe a generative bayesian model for action understanding in which inverse-forward internal model pairs are considered ` hypotheses ' of plausible action goals that are explored in parallel via an approximate inference mechanism based on sequential monte carlo methods . the reenactment of internal model pairs can be considered a form of motor simulation , which supports both perceptual prediction and action understanding at the goal level . however , this generative bayesian model is generally considered to be computationally inefficient . we present a generative bayesian model that dynamically reallocates computational resources to more accurate internal models depending on both the available prior information and the prediction error of the inverse-forward models , and which leads to successful action recognition . we present experimental results that test the robustness and efficiency of our generative bayesian model in real-world scenarios .",0
1504,"This paper proposes a novel approach to transfer learning, called Multi-View Discriminant Transfer Learning (MDTL), which aims to resolve within-view and/or between-view conflicts in multi-view learning scenarios. In this framework, discriminant weight vectors are learned to project the data from different views into a common subspace, such that the projected data can be used for discriminant analysis. The proposed transfer learning algorithm utilizes the discriminant analysis perspective and is able to handle single- or cross-domain transfer learning problems. Specifically, it deals with the domain discrepancy and view disagreement by learning transferable knowledge from both source and target domains, and projecting them into a common subspace. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed approach compared to other multi-view learning approaches.",1
1505,"we study to incorporate multiple views of data in a perceptive transfer learning framework and propose a multi-view discriminant transfer learning approach for domain adaptation . the main idea is to find the optimal discriminant weight vectors for each view such that the correlation between the two-view projected data is maximized , while both the domain discrepancy and the view disagreement are minimized simultaneously . furthermore , we analyze multi-view discriminant transfer learning approach theoretically from discriminant analysis perspective to explain the condition and reason , under which the proposed multi-view discriminant transfer learning approach is not applicable . the analytical results allow us to investigate whether there exist within-view and/or between-view conflicts , and thus provides a deep insight into whether the transfer learning algorithm work properly or not in the view-based problems and the combined learning problem . experiments show that multi-view discriminant transfer learning approach significantly outperforms the state-of-the-art baselines including some typical multi-view learning approaches in single-or cross-domain .",0
1506,"This paper proposes a data-driven level set method for reconstructing shapes from unorganized and noisy point data. The approach formulates a constrained energy minimization problem using the observed point set and a signed distance function, and solves it using the level set formalism. A smoothness prior is enforced to avoid local minima, and the optimization problem is solved using an iterative algorithm. The reconstructed shape can handle topological changes and shape noise clutter. Results demonstrate the effectiveness of the proposed method for shape reconstruction.",1
1507,"we propose a new method for shape reconstruction from noisy and unorganized point data . we represent a shape through its signed distance function and formulate shape reconstruction as a constrained energy minimization problem directly based on the observed point set . the associated energy function includes both the likelihood of the observed data points and a smoothness prior on the reconstructed shape . to solve this optimization problem , an efficient data-driven level set method is developed . our method is robust to local minima , clutter , and noise . it is also applicable to situations where the data are sparse . the topologi-cal nature of the underlying shape is handled automatically through the level set formalism .",0
1508,"This paper proposes a method for enhancing the intelligibility of HMM-based synthetic speech in noisy environments by combining perceptually-motivated spectral shaping with loudness and duration modification. The method involves adapting the duration and excitation patterns of speech based on Lombard-adapted changes, and applying a dynamic range compressor to the speech signal. A perceptually-motivated spectral shaper is then used to shape the spectral envelope of the speech signal. The proposed method is evaluated using the glimpse proportion measure and the Hurricane Challenge under various SNR conditions. The results show that the proposed method outperforms both unmodified synthetic speech and speech-shaped noise in terms of intelligibility enhancement. The method can be used as a voice enhancement strategy for text-to-speech systems in noisy environments.",1
1509,"this paper presents our entry to a speech-in-noise intelligibility enhancement evaluation : the hurricane challenge . the system consists of a text-to-speech voice manipulated through a combination of enhancement strategies , each of which is known to be individually successful : a perceptually-motivated spectral shaper based on the glimpse proportion measure , dynamic range compression , and adaptation to lombard excitation and duration patterns . we achieved substantial intelligibility improvements relative to unmodified synthetic speech : 4.9 db in competing speaker and 4.1 db in speech-shaped noise . an analysis conducted across this and other two similar evaluations shows that the spectral shaper and the compressor -lrb- both of which are loudness boosters -rrb- contribute most under higher snr conditions , particularly for speech-shaped noise . duration and excitation lombard-adapted changes are more beneficial in lower snr conditions , and for competing speaker noise .",0
1510,"This paper proposes a novel approach for motion estimation using ordinal measures. The proposed method uses rank permutations and temporal monotonicity to estimate the motion of image regions. The approach is based on the observation that the relative ordering of intensity values within a region is more robust than absolute intensity values. The proposed method is evaluated on rotating ring phantom and real heart image sequences, as well as tagged magnetic resonance images. The experimental results show that the proposed method outperforms other state-of-the-art methods, such as sum-of squared-difference (SSD), normalized correlation, and linearity between corresponding intensity values. The proposed method can be used in various applications that require accurate motion estimation, such as medical imaging and video processing.",1
1511,"we present a method for motion estimation using ordinal measures . ordinal measures are based on relative ordering of intensity values in a image region called rank permutation . while popular measures like the sum-of squared-difference -lrb- s s d -rrb- and normalized correlation rely on linear-ity between corresponding intensity values , ordinal measures only require them to be monotonically related so that rank permutations between corresponding regions are preserved . this property turns out to be usefil for motion estimation in tagged magnetic resonance images . we study the imaging equation involved in two methods of tagging and observe temporal monotonicity in intensity under certain conditions though the tags themselves fade . we compare our method to s s d and normalized correlation in a rotating ring phantom image sequence . we present an experiment on a real heart image sequence which suggests the suitability of our method .",0
1512,"Automated planning is an important area of research that has many practical applications, but it has traditionally relied on rich domain models. However, the web age has brought with it new challenges to planning, as incomplete and evolving domain models are becoming increasingly common. Model-lite planning technology has emerged as a potential solution to these challenges, as it allows for planning with incomplete and evolving domain models. This paper discusses the challenges of planning with incomplete and evolving domain models, and explores the potential of model-lite planning technology to address these challenges. The paper concludes with a discussion of the future of planning technology and the role that model-lite planning technology may play in it.",1
1513,"the automated planning community has traditionally focused on the efficient synthesis of plans given a complete domain theory . in the past several years , this line of work met with significant successes , and the future course of the community seems to be set on efficient planning with even richer models . while this line of research has its applications , there are also many domains and scenarios where the first bottleneck is getting the domain model at any level of completeness . in these scenarios , the domain model automatically renders the planning technology unusable . to counter this , i will motivate model-lite planning technology aimed at reducing the domain-modeling burden -lrb- possibly at the expense of reduced functionality -rrb- , and outline the research challenges that need to be addressed to realize model-lite planning technology .",0
1514,"This paper proposes a method for separating overlapping RFID signals using antenna arrays. Radio Frequency Identification (RFID) technology has become popular for identifying and tracking objects in a variety of applications. However, in crowded environments, RFID signals from multiple tags can overlap and create interference. The proposed method uses Blind Source Separation techniques to separate overlapping signals based on Zero Constant Modulus (ZCM) signals. The method is evaluated using synthetic and measured data sets, and the performance of the ZCM algorithms is compared to other methods such as Binary Tree Algorithms. Additionally, the paper discusses collision avoidance using MAC protocols and binary tree algorithms to improve the identification of tagged objects. The proposed antenna array-based approach shows promise in solving the problem of overlapping RFID signals, which can improve the efficiency and accuracy of RFID systems.",1
1515,"-- radio frequency identi ¿ cation is a technology to wirelessly transmit the identity of tagged objects . for long-range systems with multiple tags , the tag replies may overlap . current solutions are based on collision avoidance using mac protocols -lrb- e.g. slotted aloha and binary tree algorithms -rrb- . this can be a time-consuming process . in this paper , it is shown how an antenna array in combination with blind source separation techniques can be used to separate multiple overlapping tag signals . the source signals are modeled as zero constant modulus signals , and the corresponding zcm algorithms are tested on synthetic and measured data sets .",0
1516,"This paper presents a joint power allocation scheme for multiple-input multiple-output (MIMO) systems assisted by a relay. The scheme is based on link reliability and uses convex optimization methods to determine the optimal power allocation parameters. The optimization criterion is the error probability, and a cost function is defined to minimize it. The problem is formulated as a convex optimization problem, and the joint optimal power allocation solution is obtained. The proposed power allocation scheme is evaluated and compared to other schemes. The results show that the proposed scheme outperforms other schemes in terms of error probability and achieves better system performance.",1
1517,"a new optimization criterion is proposed to minimize error probability for the proposed joint optimal power allocation of the mimo systems enhanced by relay in this paper . it is proved that the cost function obtained is only convex with respect to -lrb- w.r.t. -rrb- the power parameters of the source or those of the relay separately , but not convex w.r.t. the whole parameters . in order to use convex optimization methods with high efficiency to solve this complicated problem , a tight upper bound of the sum mse -lrb- mean squared error -rrb- is derived , and employed to modify the cost function in order to obtain a convex problem . it is verified through simulation results that the proposed pa scheme outperforms the existing one .",0
1518,"This paper proposes generalization bounds for learning kernels. The method involves a convex combination of p base kernels and a non-negative combination of p base kernels with L2 regularization. The goal is to analyze the generalization error complexity of learning kernels. The paper uses combinatorial analysis and Rademacher complexity to derive the generalization bounds. The proposed method provides a way to improve the performance of kernel-based learning algorithms, especially for problems with large datasets. The experimental results demonstrate the effectiveness of the proposed method.",1
1519,"this paper presents several novel generalization bounds for the problem of learning kernels based on a combinatorial analysis of the rademacher complexity of the corresponding hypothesis sets . our bound for learning kernels with a convex combination of p base kernels using l 1 regular-ization admits only a √ log p dependency on the number of kernels , which is tight and considerably more favorable than the previous best bound given for the same problem . we also give a novel bound for learning with a non-negative combination of p base kernels with an l 2 regularization whose dependency on p is also tight and only in p 1/4 . we present similar results for l q regular-ization with other values of q , and outline the relevance of our proof techniques to the analysis of the complexity of the class of linear functions . experiments with a large number of kernels further validate the behavior of the generalization error as a function of p predicted by our bounds .",0
1520,"This paper presents experimental support for a categorical compositional distributional model of meaning. The model combines abstract categorical structures with distributional semantics, using unsupervised learning of matrices to capture compositional meaning. The paper describes how the model is applied to word disambiguation tasks and the evaluation of the model on intransitive and transitive sentences of varying syntactic complexity. The results show that the model outperforms other empirical distributional methods and provides insights for computational linguists working on modeling meaning. The experiments were conducted using the British National Corpus (BNC) and provide empirical evidence for the effectiveness of the proposed model.",1
1521,"modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists . we implement the abstract categorical model of coecke et al. -lrb- 2010 -rrb- using data from the bnc and evaluate abstract categorical model . the implementation is based on unsupervised learning of matrices for relational words and applying unsupervised learning of matrices to the vectors of their arguments . the evaluation is based on the word disambiguation task developed by mitchell and lapata -lrb- 2008 -rrb- for intransitive sentences , and on a similar new experiment designed for transitive sentences . our model matches the results of its competitors in the first experiment , and betters unsupervised learning of matrices in the second . the general improvement in results with increase in syntactic complexity show-cases the compositional power of our model .",0
1522,This paper explores the use of electric network frequency (ENF) signals for multimedia synchronization. ENF signals are time-varying random processes that are embedded in multimedia recordings due to the electromagnetic influences of the power grid. The paper proposes an ENF-based synchronization approach that uses ENF signals to automatically synchronize audio and video in multimedia signals. The approach also includes detecting forgery of ENF-containing multimedia signals and multi-view video synchronization. The paper evaluates the proposed approach using empirical analysis and discusses its potential for forensic applications. The results show the effectiveness of the ENF-based synchronization approach for multimedia synchronization and suggest its usefulness for various forensic applications.,1
1523,"the electric network frequency signal can be captured in multimedia recordings due to electromagnetic influences from the power grid at the time of recording . recent work has exploited the enf signals for forensic applications , such as authenticating and detecting forgery of enf-containing mul-timedia signals , and inferring their time and location of creation . in this paper , we explore a new potential of enf signals for automatic synchronization of audio and video . the enf signals as a time-varying random process can be used as a timing fingerprint of multimedia signals . synchronization of audio and video recordings can be achieved by aligning their embedded enf signals . we demonstrate the proposed scheme with two applications : multi-view video synchronization and synchronization of historical audio recordings . the experimental results show the enf based synchronization approach is effective , and has the potential to solve problems that are intractable by other existing methods .",0
1524,"This paper proposes an error-tolerant scribbles based interactive image segmentation method that can be used on mobile touch-screen devices. The method uses a graph-cut algorithm and a ratio energy function that takes user input information in the form of scribbles to segment the foreground from the background of an image. The authors evaluate the performance of the algorithm using synthetic and manual scribbles, as well as the GrabCut dataset. The results demonstrate the robustness of the scribbles-based approach and the effectiveness of the iterated graph cut algorithm.",1
1525,"scribbles in scribble-based interactive segmentation such as graph-cut are usually assumed to be perfectly accurate , i.e. , foreground scribble pixels will never be segmented as background in the final segmentation . however , it can be hard to draw perfectly accurate scribbles , especially on fine structures of the image or on mobile touch-screen devices . in this paper , we propose a novel ratio energy function that tolerates errors in the user input while encouraging maximum use of the user input information . more specifically , the ratio energy function aims to minimize the graph-cut energy while maximizing the user input respected in the segmentation . the ratio energy function can be exactly optimized using an efficient iterated graph cut algorithm . the robustness of the proposed ratio energy function is validated on the grabcut dataset using both synthetic scribbles and manual scribbles . the experimental results show that the proposed ratio energy function is robust to the errors in the user input and preserves the '' anchoring '' capability of the user input .",0
1526,"This paper presents a statistical mapping approach between articulatory and acoustic data for an ultrasound-based silent speech interface. The proposed method uses a joint model of visual and spectral features, including tongue and lip motions, to generate audible speech from video imaging. The method employs artificial neural networks, Gaussian mixture models, and hidden Markov models to learn the statistical relationship between the articulatory and acoustic data. A continuous speech database is used to evaluate the proposed method, and a unit selection approach is employed to select the best speech sound for a given visual articulatory data. The proposed statistical mapping technique is shown to effectively capture the voiced/unvoiced parameter of speech sounds, indicating the potential of the proposed method for use in silent speech interfaces.",1
1527,"this paper presents recent developments on our '' silent speech interface '' that converts tongue and lip motions , captured by ultrasound and video imaging , into audible speech . in our previous studies , the mapping between the observed articulatory movements and the resulting speech sound was achieved using a unit selection approach . we investigate here the use of statistical mapping techniques , based on the joint modeling of visual and spectral features , using respectively gaussian mixture models and hidden markov models . the prediction of the voiced/unvoiced parameter from visual articulatory data is also investigated using an artificial neural network . a continuous speech database consisting of one-hour of high-speed ultrasound and video sequences was specifically recorded to evaluate the proposed statistical mapping techniques .",0
1528,This paper presents an analysis of mixed natural and symbolic input in mathematical dialogs. The authors use a corpus of dialogs between a simulated tutorial system and human users to study the interaction of telegraphic natural language and embedded symbolic language in the context of mathematics. They apply deep syntactic and semantic analysis to the dialogs and examine the verbalization of mathematical concepts and language phenomena. The study focuses on the formal domains of mathematics and investigates how the mixed input affects the discourse between the users and the tutorial system. The findings can help improve the design of natural language interfaces for mathematical tutoring systems.,1
1529,"discourse in formal domains , such as mathematics , is characterized by a mixture of telegraphic natural language and embedded -lrb- semi - -rrb- formal symbolic mathematical expressions . we present language phenomena observed in a corpus of dialogs with a simulated tutorial system for proving theorems as evidence for the need for deep syntactic and semantic analysis . we propose an approach to input understanding in this setting . our goal is a uniform analysis of inputs of different degree of verbaliza-tion : ranging from symbolic alone to fully worded mathematical expressions .",0
1530,"This paper presents an estimation method for talker's head orientation based on discrimination of the shape of cross-power spectrum phase (CSP) coefficients. The method uses a network of microphone arrays, including 2-channel microphones, sub-microphone arrays, and feature vectors of CSP coefficients to estimate the talker's head orientation. The sound amplitude and peak value of CSP coefficients are also used in conjunction with talker localization. The proposed method is evaluated using a corpus of microphone array network systems and demonstrates improved performance compared to previous methods. Results indicate that the proposed method has the potential for accurate and robust estimation of talker's head orientation in the presence of reverberation.",1
1531,"this paper presents a talker 's head orientation estimation method using 2-channel microphones . in recent research , some approaches based on a network of microphone arrays have been proposed in order to estimate the talker 's head orientation . in those methods , the talker 's head orientation is estimated using the sound amplitude or peak value of csp -lrb- cross-power spectrum phase -rrb- coefficients obtained from each microphone array . however , microphone array network systems need many microphone arrays to be set along the walls of a given room so that sub-microphone arrays surround the user . in this paper , we focus on the shape of the csp coefficients affected by the reverberation , which depends on the talker 's position and the head orientation . in our proposed talker 's head orientation estimation method , we use not only the peak value but also the other values of the csp coefficients as feature vectors , and the talker 's position and the head orientation are estimated by discriminating the csp vector . the effectiveness of this talker 's head orientation estimation method has been confirmed by talker localization and head orientation estimation experiments performed in a real environment .",0
1532,"This paper proposes a new approach for face recognition applications, called the Kernel Fukunaga-Koontz Transform Subspaces. This approach enhances the discrimination ability of the Fukunaga-Koontz Transform by building discriminative subspaces in a higher-dimensional feature space. The discriminative subspaces building approach is based on the linear Fukunaga-Koontz Transform and is particularly suited for small-sample-size problems. The proposed method is evaluated for domain-specific problems and is shown to outperform the linear Fukunaga-Koontz Transform in terms of recognition accuracy. Moreover, the Kernel Fukunaga-Koontz Transform is particularly effective for multi-class problems, where it achieves high recognition accuracy with a small number of training samples. The experimental results demonstrate the effectiveness of the proposed method for face recognition applications.",1
1533,"traditional linear fukunaga-koontz transform -lsb- 1 -rsb- is a powerful discriminative subspaces building approach . previous work has successfully extended linear fukunaga-koontz transform to be able to deal with small-sample-size . in this paper , we extend traditional linear fukunaga-koontz transform to enable it to work in multi-class problem and also in higher dimensional subspaces and therefore provide enhanced discrimination ability . we verify the effectiveness of the proposed kernel fukunaga-koontz transform by demonstrating its effectiveness in face recognition applications ; however the proposed kernel fukunaga-koontz transform can be applied to any other domain specific problems .",0
1534,This paper proposes a data-specific concept correlation estimation procedure for video annotation refinement. The proposed approach involves probability-calculation based video annotation refinement using visual and high-level characteristics of the data. The high-level concept correlation bases are estimated to provide sparse representation of the correlation bases. Data-specific concept correlation calculation is then performed using the TRECVID 2006 dataset. Feature-level sparse coefficients are used to determine generic concept correlation and concept distribution. The results demonstrate that the proposed approach provides an effective means of video annotation refinement based on data-specific characteristics.,1
1535,"for video annotation refinement , a reasonable concept correlation representation is crucial . in this paper , we present a data-specific concept correlation estimation procedure for this task , where the resulting correlation with respect to each data encodes both its visual and high-level characteristics . specifically , this data-specific concept correlation estimation procedure comprises two major modules : concept correlation basis estimation and data-specific concept correlation calculation . under the framework of sparse representation , the data-specific concept correlation estimation procedure introduces a set of high-level concept correlation bases to represent the concept distribution of each feature-level basis , while the latter constructs the concept correlation of a specific data by combining its feature-level sparse coefficients and correlation bases together . in the end , given this new correlation , a probability-calculation based video annotation refinement is performed on trecvid 2006 dataset . the experiments show that such a representation capturing data-specific characteristics could achieve better performance , than the generic concept correlation applied to all data .",0
1536,"This paper presents a fast trust region approach for optimization of segmentation energies. The proposed approach uses a non-linear approximation model to achieve more accurate segmentation results. It includes constrained optimization algorithm and target appearance distributions, as well as shape prior constraint and non-linear regional terms. The approach is compared to gradient descent techniques and achieves faster convergence. The non-linear approximation models are used to handle volume constraint and segment size. The proposed approach is evaluated using the Bhattacharyya distance and achieves global optimum with the KL divergence. The iterative approach is used for optimization, resulting in fast convergence and improved segmentation performance.",1
1537,"trust region is a well-known general iterative approach to optimization which offers many advantages over standard gradient descent techniques . in particular , it allows more accurate nonlinear approximation models . in each iteration this approach computes a global optimum of a suitable approximation model within a fixed radius around the current solution , a.k.a. trust region . in general , this approach can be used only when some efficient constrained optimization algorithm is available for the selected non-linear -lrb- more accurate -rrb- approximation model . in this paper we propose a fast trust region approach for optimization of segmentation energies with non-linear regional terms , which are known to be challenging for existing algorithms . these non-linear regional terms include , but are not limited to , kl divergence and bhattacharyya distance between the observed and the target appearance distributions , volume constraint on segment size , and shape prior constraint in a form of l 2 distance from target shape moments . our method is 1-2 orders of magnitude faster than the existing state-of-the-art methods while converging to comparable or better solutions .",0
1538,"This paper proposes a statistical analysis method for coupled time series using kernel cross-spectral density operators. The approach can be applied to stationary time series of arbitrary objects, such as electrophysiological neural time series, and can handle arbitrary input domains and complex data structures. The method utilizes positive definite kernels to estimate the cross-spectral density operator and applies a similarity measure to detect errors and test independence. The approach is shown to be effective in detecting coupling between time series and constructing coupling graphs. The paper demonstrates the usefulness of the proposed method through experiments and analysis of real-world data sets.",1
1539,"many applications require the analysis of complex interactions between time series . these interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings . here we provide a general framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects . to achieve this goal , we study the properties of the kernel cross-spectral density operator induced by positive definite kernels on arbitrary input domains . this framework enables us to develop an independence test between time series , as well as a similarity measure to compare different types of coupling . the performance of our test is compared to the hsic test using i.i.d. assumptions , showing strong improvements in terms of detection errors , as well as the suitability of this approach for testing dependency in complex dynamical systems . this similarity measure enables us to identify different types of interactions in electrophysiological neural time series .",0
1540,"This paper proposes the creation of a ""Connotation Lexicon"" which captures the nuanced, connotative sentiments that exist beneath the surface meaning of words. The authors describe the semantic parallelism of coordination and the broad-coverage connotation of words as important factors in building such a lexicon. The lexicon is created using distributional similarity induction algorithms and other lexical resources, as well as prior knowledge. The paper also discusses the concept of semantic prosody and how it relates to the creation of the Connotation Lexicon. The proposed lexicon has the potential to improve sentiment analysis and other natural language processing tasks that rely on understanding the connotative meanings of words.",1
1541,"understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text , as seemingly objective statements often allude nuanced sentiment of the writer , and even purposefully conjure emotion from the readers ' minds . the focus of this paper is drawing nuanced , connotative sentiments from even those words that are objective on the surface , such as '' intelligence '' , '' human '' , and '' cheesecake '' . we propose induction algorithms encoding a diverse set of linguistic insights -lrb- semantic prosody , distri-butional similarity , semantic parallelism of coordination -rrb- and prior knowledge drawn from lexical resources , resulting in the first broad-coverage connotation lexicon .",0
1542,"This paper proposes a novel collaborative filtering method called Mixture Probabilistic Matrix Approximation (MPMA) for accurately predicting user/item ratings. MPMA uses a Gaussian mixture model to approximate the user/item rating matrix and locally optimizes user/item feature vectors for better prediction accuracy. The proposed method is evaluated on the Movielens and Netflix datasets and compared with other matrix approximation-based collaborative filtering methods. The experimental results show that MPMA outperforms existing methods in terms of recommendation accuracy, both in terms of local and global predictions. Overall, MPMA demonstrates the effectiveness of mixture probabilistic matrix approximation in improving the accuracy of collaborative filtering for user/item recommendations.",1
1543,"matrix approximation -lrb- ma -rrb- is one of the most popular techniques for collaborative filtering . most existing ma methods train user/item latent factors based on a user-item rating matrix and then use the global latent factors to model all users/items . however , globally optimized latent factors may not reflect the unique interests shared among only subsets of users/items , without which unique interests of users may not be accurately modelled . as a result , existing ma methods , which can not capture the uniqueness of different user/item , can not provide optimal recommendation . in this paper , a mixture probabilistic matrix approximation method is proposed , which unifies globally optimized user/item feature vectors -lrb- on the entire rating matrix -rrb- and locally optimized user/item feature vectors -lrb- on subsets of user/item ratings -rrb- to improve recommendation accuracy . more specifically , in mixture probabilistic matrix approximation method , a method is developed to find both globally and locally optimized user/item feature vectors . then , a gaus-sian mixture model is adopted to combine global predictions and local predictions to produce accurate rating predictions . experimental study using movielens and netflix datasets demonstrates that mixture probabilistic matrix approximation method outperforms five state-of-the-art ma based cf methods in recommendation accuracy with good scalability .",0
1544,"This paper proposes a novel approach for multi-view hashing, called Sequential Spectral Learning to Hash with Multiple Representations (SSL-MR), which learns similarity-preserving binary codes for high-dimensional visual descriptors. SSL-MR involves constructing an approximate k-nearest neighbor graph using α-averaging view-specific distance matrices, followed by a recursive spectral bisection to obtain small partitions of the data. A single-view spectral hashing is then performed on each partition to obtain view-specific hash functions. The hash functions are learned to satisfy decorrelation constraints, and the resulting binary codes are obtained by averaging the α-averaged distances. SSL-MR is evaluated on the NUS-WIDE datasets using SIFT and GIST visual descriptors, and compared to other multi-view hashing methods. The results show that SSL-MR outperforms state-of-the-art methods in terms of retrieval accuracy.",1
1545,"learning to hash involves learning hash functions from a set of images for embedding high-dimensional visual descriptors into a similarity-preserving low-dimensional hamming space . most of existing methods resort to a single representation of images , that is , only one type of visual descriptors is used to learn a hash function to assign binary codes to images . however , images are often described by multiple different visual descriptors -lrb- such as sift , gist , hog -rrb- , so it is desirable to incorporate these multiple representations into learning a hash function , leading to multi-view hashing . in this paper we present a sequential spectral learning approach to multi-view hashing where a hash function is sequentially determined by solving the successive maximiza-tion of local variances subject to decorrelation constraints . we compute multi-view local variances by α-averaging view-specific distance matrices such that the best averaged distance matrix is determined by minimizing its α-divergence from view-specific distance matrices . we also present a scalable implementation , exploiting a fast approximate k-nn graph construction method , in which α-averaged distances computed in small partitions determined by recursive spectral bisection are gradually merged in conquer steps until whole examples are used . numerical experiments on caltech-256 , cifar-20 , and nus-wide datasets confirm the high performance of our sequential spectral learning approach , in comparison to single-view spectral hashing as well as existing multi-view hashing methods .",0
1546,"I'm sorry, but the provided graph is too sparse to generate a comprehensive paper abstract. It only contains one node and one relation, which is not enough to provide context or insights into the paper's content. Can you please provide more information or a more complete graph?",1
1547,"recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance . we seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances -lrb- e.g. pedestrian images -rrb- using a representation based on pixel-wise similarities , similarity templates . because of its invariance to the colors of particular components of an object , this representation enables detection of instances of an object class and enables alignment of those instances . further , this model implicitly represents the regions of color regularity in the class-specific image set enabling a decomposition of that object class into component regions .",0
1548,"This paper investigates the optimality of the classifier chain algorithm for multi-label classification problems. The greedy classifier chain algorithm has been widely used due to its simplicity and effectiveness in transforming multi-label problems into a sequence of binary classification tasks. However, the optimal label order is not guaranteed by this algorithm. In this paper, the globally optimal label order is obtained by a deterministic high-order Markov chain model. The CC-DP algorithm is proposed to find the optimal label order in polynomial time. Real-world multi-label datasets are used to evaluate the performance of the CC-DP algorithm, which achieves better generalization error than the greedy classifier chain algorithm. The experimental results demonstrate the effectiveness and efficiency of the proposed method.",1
1549,"to capture the interdependencies between labels in multi-label classification problems , classifier chain tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model . since its performance is sensitive to the choice of label order , the key issue is how to determine the optimal label order for classifier chain . in this work , we first generalize the classifier chain over a random label order . then , we present a theoretical analysis of the generalization error for the proposed deterministic high-order markov chain model . based on our results , we propose a dynamic programming based classifier chain -lrb- cc-dp -rrb- algorithm to search the globally optimal label order for classifier chain and a greedy classifier chain algorithm to find a locally optimal classifier chain . comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed cc-dp algorithm outperforms state-of-the-art approaches and the cc-dp algorithm achieves comparable prediction performance with cc-dp .",0
1550,"The paper proposes a co-clustering based self-taught clustering algorithm for unsupervised transfer learning, called ""self-taught clustering"". The algorithm makes use of auxiliary unlabeled data, which may be irrelevant to the target clustering task, to learn a better feature representation. Specifically, the algorithm learns a topic distribution over the auxiliary data and then uses this distribution to perform clustering on the target data. The proposed method is compared to other clustering methods, and the results demonstrate the effectiveness of the self-taught clustering algorithm. Overall, the paper presents a novel approach to feature representation learning for clustering, which can be applied to a wide range of applications, such as image clustering.",1
1551,"this paper focuses on a new clustering task , called <i> self-taught clustering </i> . self-taught clustering is an instance of <i> unsupervised transfer learning </i> , which aims at clustering a small collection of target unlabeled data with the help of a large amount of <i> auxiliary </i> unlabeled data . the target and auxiliary data can be different in topic distribution . we show that even when the target data are not sufficient to allow effective learning of a high quality feature representation , it is possible to learn the useful features with the help of the auxiliary data on which the target data can be clustered effectively . we propose a co-clustering based self-taught clustering algorithm to tackle this problem , by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of features . under the new feature representation , clustering on the target data can be improved . our experiments on image clustering show that our co-clustering based self-taught clustering algorithm can greatly outperform several state-of-the-art clustering methods when utilizing irrelevant unlabeled auxiliary data .",0
1552,"This paper proposes a machine learning approach to automatically evaluate the quality of machine translation systems. The approach involves comparing machine translations to human reference translations and using failure analysis classifiers to identify errors. The paper focuses on the development of these classifiers and their use in evaluating machine translations. The proposed approach is demonstrated to be effective in identifying translation errors and is shown to outperform existing approaches. Overall, this work contributes to the development of more reliable and efficient methods for evaluating machine translation systems.",1
1553,"we present a machine learning approach to evaluating the well-formedness of output of a machine translation system , using classifiers that learn to distinguish human reference translations from machine translations . this machine learning approach can be used to evaluate an machine translation system , tracking improvements over time ; to aid in the kind of failure analysis that can help guide system development ; and to select among alternative output strings . the machine learning approach presented is fully automated and independent of source language , target language and domain .",0
1554,"This paper proposes an adaptive listening room equalization (LRE) technique for massive multichannel reproduction systems. The approach is based on wave field synthesis and wave-domain adaptive filtering, using a scalable filtering structure that ensures robustness and computational efficiency. The proposed method addresses various filtering tasks for different listener positions and reproduction channels. The paper presents experiments to demonstrate the effectiveness of the proposed approach and compares it with existing LRE techniques. The results show that the proposed method achieves superior performance in terms of equalization accuracy and computational complexity, making it suitable for real-world applications.",1
1555,"massive multichannel reproduction systems like wave field synthesis are potentially well suited to be complemented by listening room equalization . however , their typically large number of reproduction channels makes this task challenging for both computational and algorithmic reasons . wave-domain adaptive filtering was proposed earlier and is especially well-suited to adap-tive filtering tasks in the context of wave field synthesis . in this paper , we propose to generalize the model originally used for wave-domain adaptive filtering to allow an adaptive lre for a broader range of reproduction scenarios , while maintaining the advantages of the original adaptive lre . the proposed adaptive lre is evaluated for filtering structures of varying complexity along with considering the robustness to varying listener positions .",0
1556,"This paper proposes reflection methods for user-friendly submodular optimization. Submodular functions have been widely used in machine learning, computer vision, and signal processing for tasks such as image segmentation and parameter tuning. However, discrete submodular minimization problems are often computationally expensive, and continuous best approximation problems are not user-friendly. The proposed reflection methods provide a more user-friendly approach to submodular optimization by leveraging the de-composability of submodular functions. The paper also discusses the applicability of the proposed methods",1
1557,"recently , it has become evident that submodularity naturally captures widely occurring concepts in machine learning , signal processing and computer vision . consequently , there is need for efficient optimization procedures for submodu-lar functions , especially for minimization problems . while general submodular minimization is challenging , we propose a new method that exploits existing de-composability of submodular functions . in contrast to previous approaches , our method is neither approximate , nor impractical , nor does it need any cumbersome parameter tuning . moreover , it is easy to implement and parallelize . a key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections , and its solution can be easily thresholded to obtain an optimal discrete solution . this method solves both the continuous and discrete formulations of the discrete submodular minimization problem , and therefore has applications in learning , inference , and reconstruction . in our experiments , we illustrate the benefits of our method on two image segmentation tasks .",0
1558,"This paper investigates the relevance of bandwidth extension for speaker verification tasks. In particular, it explores the use of short-time spectral parameterizations such as mel-frequency cepstral coefficients in conjunction with covariance matrix based verification systems. The study examines the trade-off between detection errors and bandwidth extension using detection error trade-off curves. The results demonstrate that the bandwidth-extension algorithm can significantly improve the performance of the verification system, especially for narrow-band speech. The study also explores the impact of model order on the performance of the system. The findings suggest that the use of bandwidth extension can improve the accuracy of speaker verification systems when applied appropriately. The study uses microphone and ISDN speech databases for its analysis and demonstrates the effectiveness of short-time spectral parameterizations in improving the performance of verification systems.",1
1559,"in this paper , we consider the effect of a bandwidth extension of narrow-band speech signals -lrb- 0.3-3 .4 khz -rrb- to 0.3-8 khz on speaker verification . using covariance matrix based verification systems together with detection error trade-off curves , we compare the performance between systems operating on narrow-band , wide-band -lrb- 0-8 khz -rrb- , and bandwidth-extended speech . the experiments were conducted using different short-time spectral parameterizations derived from microphone and isdn speech databases . the studied bandwidth-extension algorithm did not introduce artifacts that affected the speaker verification task , and we achieved improvements between 1 and 10 percent -lrb- depending on the model order -rrb- over the verification system designed for narrow-band speech when mel-frequency cepstral coefficients for the short-time spectral parameterization were used .",0
1560,"This paper proposes a sparse representation approach for improving the perceptual quality of separated speech signals. The study evaluates the perceptual evaluation of speech quality of binary-masked noisy speech using speech quality measures. The paper proposes a method for speech separation based on time-frequency masking, which is used in conjunction with binary masks for speech separation. The paper explores the performance of various reconstruction approaches for speech separation, with a particular focus on the intelligibility of speech signals. The proposed sparse-representation approach is used to improve the quality of separated speech signals. The study evaluates the speech quality of the binary-masked noisy speech and the reconstruction approaches for speech separation. The results show that the proposed approach can significantly improve the perceptual quality of the separated speech signal, leading to better speech quality measures. The paper discusses the overlap-and-add synthesis technique used for speech reconstruction and the effectiveness of the proposed sparse-representation approach in improving the perceptual quality of separated speech signals. The study concludes that the proposed method can effectively improve the quality of separated speech signals and provides a promising direction for future research in speech signal processing.",1
1561,"speech separation based on time-frequency masking has been shown to improve intelligibility of speech signals corrupted by noise . a perceived weakness of binary masking is the quality of separated speech . in this paper , an approach for improving the perceptual quality of separated speech from binary masking is proposed . our approach consists of two stages , where a binary mask is generated in the first stage that effectively performs speech separation . in the second stage , a sparse-representation approach is used to represent the separated signal by a linear combination of short-time fourier transform magnitudes that are generated from a clean speech dictionary . overlap-and-add synthesis is then used to generate an estimate of the speech signal . the performance of the proposed approach is evaluated with the perceptual evaluation of speech quality , which is a standard objective speech quality measure . the proposed algorithm offers considerable improvements in speech quality over binary-masked noisy speech and other reconstruction approaches .",0
1562,"This paper presents ""It Makes Sense,"" a wide-coverage word sense disambiguation (WSD) system for free text. The study evaluates the system's performance on senseval and semeval tasks, which are used to evaluate WSD systems. The paper describes the use of preprocessing tools and features, including knowledge-based features, for the system. The paper uses supervised learning, specifically linear support vector machines, to train classifiers for the WSD task. The study discusses the importance of the sense-val and semeval workshops and their role in the evaluation of WSD systems. The paper demonstrates that the proposed system outperforms existing WSD systems on sense-val and semeval tasks. The study evaluates the system's performance on IMS, a common benchmark dataset for WSD, and shows that the system achieves state-of-the-art performance. The paper discusses the importance of feature selection and the role of knowledge-based features in improving classifier performance. The study concludes that the proposed system is an effective tool for WSD in free text and provides a promising direction for future research in this field.",1
1563,"word sense disambiguation -lrb- word sense disambiguation systems -rrb- systems based on supervised learning achieved the best performance in sense-val and semeval workshops . however , there are few publicly available supervised english all-words wsd system . this limits the use of word sense disambiguation systems in other applications , especially for researchers whose research interests are not in word sense disambiguation systems . in this paper , we present ims , a supervised english all-words wsd system . the flexible framework of ims allows users to integrate different preprocessing tools , additional features , and different classifiers . by default , we use linear support vector machines as the classifier with multiple knowledge-based features . in our implementation , ims achieves state-of-the-art results on several senseval and semeval tasks .",0
1564,"This paper proposes a method for shape classification through structured learning of matching measures. The proposed method is based on a structured prediction setting, where point correspondences between shapes are used to compute shape similarity scores. The study uses shape databases to evaluate the proposed method and demonstrates that it outperforms existing methods in terms of classification accuracy. The paper formulates the problem as a max-margin formulation, which is a common approach in structured learning techniques. The study discusses the importance of the classification loss and its impact on the performance of the classifier. The paper also examines various similarity measures and their effectiveness in shape classification. The proposed method uses a learning-based approach that enables the system to learn from examples and improve its performance over time. The study shows that the proposed method achieves state-of-the-art performance on several benchmark datasets. The paper concludes that the proposed method is an effective tool for shape classification and provides a promising direction for future research in this field. The study highlights the importance of structured learning techniques and their potential to improve the accuracy and efficiency of shape classification systems.",1
1565,"many traditional methods for shape classification involve establishing point correspondences between shapes to produce matching scores , which are in turn used as similarity measures for shape classification . learning techniques have been applied only in the second stage of this process , after the matching scores have been obtained . in this paper , instead of simply taking for granted the scores obtained by matching and then learning a classifier , we learn the matching scores themselves so as to produce shape similarity scores that minimize the classification loss . the solution is based on a max-margin formulation in the structured prediction setting . experiments in shape databases reveal that such an integrated learning algorithm substantially improves on existing methods .",0
1566,"This paper proposes a novel distributed algorithm for principal component analysis (PCA) on networks using directed Gaussian graphical models. The algorithm employs online principal subspace estimation and global principal subspace estimation to achieve efficient distributed anomaly detection in real-world computer networks. The method utilizes structured sparsity and message passing to reduce communication overhead and computational complexity, with each node performing local computation using the Cholesky factor. The proposed approach outperforms existing methods in terms of accuracy and scalability, as demonstrated through extensive simulations. The structured sparsity is used for both the inverse covariance matrix and Cholesky factor computations, while message passing and local computation are employed for efficient distributed PCA on large-scale networks.",1
1567,"we introduce an efficient algorithm for performing distributed principal component analysis on directed gaussian graphical models . by exploiting structured sparsity in the cholesky factor of the inverse covariance matrix , our proposed ddpca algorithm computes global principal subspace estimation through local computation and message passing . we show significant performance and computation/communication advantages of ddpca for online principal subspace estimation and distributed anomaly detection in real-world computer networks .",0
1568,"This paper presents a panel discussion on the topic of good spelling in Vietnamese texts as one aspect of computational linguistics in Vietnam. The discussion focuses on the challenges and opportunities of developing spelling correction systems for Vietnamese, which utilizes the unique Telex code used for Vietnamese typing. The panel also highlights the importance of a comprehensive spelling database for accurate text processing and introduces the Vietnamese language processing tool, Vietools, as a useful resource for Vietnamese computational linguistics. Through the discussion, the panelists provide valuable insights and recommendations for advancing research and development in the field of Vietnamese language processing, with a particular focus on improving spelling accuracy in Vietnamese texts. The paper concludes with a call to action for researchers and practitioners in the field to collaborate and contribute to the advancement of computational linguistics in Vietnam.",1
1569,"there are many challenging problems for vietnamese language processing . it will be a long time before these challenges are met . even some apparently simple problems such as spelling correction are quite difficult and have not been approached systematically yet . in this paper , we will discuss one aspect of this type of work : designing the so-called vietools to detect and correct spelling of vietnamese texts by using a spelling database based on telex code . vietools is also extended to serve many purposes in vietnamese language processing .",0
1570,"This paper proposes practical tables for Wyner-Ziv coding of Laplacian sources. The Wyner-Ziv coding problem is addressed by considering a general encoding model, which includes multi-level coset codes and transform coefficients. The proposed tables provide rate-distortion trade-offs for different encoding parameters and quantizer families, allowing for efficient encoding and decoding of correlated side-information in practical applications. The regular codec is used for encoding the side-information Laplacians, while the decoder utilizes the correlated side-information to improve the quality of the reconstructed signal. The proposed method is evaluated using simulations, demonstrating its effectiveness in achieving good rate-distortion performance for a range of encoding parameters and quantizer families. The practical tables presented in this paper provide a useful resource for researchers and practitioners in the field of source and channel coding, particularly in the context of Wyner-Ziv coding of Laplacian sources with correlated side-information.",1
1571,"many practical coding scenarios deal with sources with transform coefficients that are well modeled as laplacians . for the wyner-ziv coding problem for such sources when correlated side-information is available at the decoder , the side-information is modeled as obtained by independent additive laplacian or gaussian innovation on the source . this paper deals with the optimal choice of encoding parameters for practical wyner-ziv coding in such scenarios , using the same quantizer family as in the regular codec to cover a range of rate-distortion trade-offs , given the variances of the source and innovation . using our prior analysis of a general encoding model based on multi-level coset codes combining source and channel coding , we present comprehensive tables with optimal encoding parameters . these tables can be readily incorporated into a practical codec to read off the encoding parameters .",0
1572,"This paper proposes a personalized tag recommendation system using nonlinear tensor factorization with a Gaussian kernel. The proposed method is a nonlinear extension of the canonical decomposition used in linear tensor factorization. It leverages the power of tensor factorization techniques to capture the complex relationships between users, tags, and items in a personalized way. The Gaussian radial basis function is utilized to enhance the performance of the system by modeling the similarity between different users and tags. The proposed method achieves linear time complexity through the use of the Tucker decomposition. The personalized tag recommendation system is evaluated using real datasets and features, demonstrating its effectiveness in improving the accuracy of online recommendations compared to linear tensor factorization and canonical decomposition-based methods. The proposed method provides a valuable contribution to the field of personalized tag recommendation systems, enabling more accurate and efficient recommendations for users in various applications. The Gaussian kernel-based nonlinear tensor factorization technique presented in this paper is a promising approach for improving the performance of recommendation systems.",1
1573,"personalized tag recommendation systems recommend a list of tags to a user when he is about to annotate an item . it exploits the individual preference and the characteristic of the items . tensor factorization techniques have been applied to many applications , such as tag recommendation . models based on tucker decomposition can achieve good performance but require a lot of computation power . on the other hand , models based on canonical decomposition can run in linear time and are more feasible for online recommendation . in this paper , we propose a novel method for personal-ized tag recommendation , which can be considered as a nonlinear extension of canonical decomposition . different from linear tensor factorization , we exploit gaus-sian radial basis function to increase the personalized tag recommendation systems 's capacity . the experimental results show that our proposed method outperforms the state-of-the-art methods for tag recommendation on real datasets and perform well even with a small number of features , which verifies that our models can make better use of features .",0
1574,"This paper presents a novel approach to highlight removal in single images by utilizing illumination-based constraints and image in-painting techniques. The method involves estimation of the underlying diffuse color, recovery of shading and textures, and removal of highlight pixels. The illumination color uniformity and highlight color analysis are used in conjunction with illumination constraints to achieve effective highlight removal even in occluded image regions. The proposed approach also considers pixel colors and their relationship with highlight color analysis and illumination constraints to improve the accuracy of recovery of shading and textures. Experimental results demonstrate that the proposed single-image highlight removal method outperforms state-of-the-art methods in terms of both visual quality and quantitative evaluation metrics.",1
1575,"we present a single-image highlight removal method that incorporates illumination-based constraints into image in-painting . unlike occluded image regions filled by traditional image in-painting , highlight pixels contain some useful information for guiding the image in-painting . illumination constraints provided by observed pixel colors , highlight color analysis and illumination color uniformity are employed in our single-image highlight removal method to improve estimation of the underlying diffuse color . the inclusion of these illumination constraints allows for better recovery of shading and textures by image in-painting . experimental results are given to demonstrate the performance of our single-image highlight removal method .",0
1576,"This paper proposes an adaptive lighting correction method for matched-texture coding that effectively reduces illumination artifacts in natural images. The method employs an adaptive Poisson lighting correction technique to address incomplete boundary conditions in the encoded blocks, which can cause structural distortion and compromise the quality of structurally lossless compression. The proposed approach integrates a side-matching algorithm and a structural texture similarity metric to improve the matching accuracy of the textures and optimize the encoding process. The effectiveness of the proposed method is demonstrated through experimental results on various natural images, which show significant improvement in image quality and compression performance compared to existing methods. The paper concludes that the proposed adaptive lighting correction method is a promising approach for enhancing matched-texture coding techniques and improving the quality of image coding.",1
1577,"matched-texture coding is a novel image coder that utilizes the self-similarity of natural images that include textures , in order to achieve structurally lossless compression . the key to a high compression ratio is replacing large image blocks with previously encoded blocks with similar structure . adjusting the lighting of the replaced block is critical for eliminating illumination artifacts and increasing the number of matches . we propose a new adaptive lighting correction method that is based on the poisson equation with incomplete boundary conditions . in order to fully exploit the benefits of the adaptive poisson lighting correction , we also propose modifications of the side-matching algorithm and structural texture similarity metric . we show that the resulting adaptive lighting correction method achieves better coding performance .",0
1578,"This paper presents a novel approach to robust learning of Gaussian mixture state emission densities for Hidden Markov Models (HMMs), which can effectively address the challenges of insufficient training data and inaccurate estimation of state emission densities. The proposed method utilizes an ensemble framework that combines the Baum-Welch algorithm with gradient descent search to optimize the learning of Gaussian mixture densities in the function space. By incorporating boosting techniques, the proposed approach can improve the accuracy of probability density estimation and achieve more robust and accurate models. The effectiveness of the proposed approach is evaluated in the context of emotion recognition, where the proposed method outperforms existing methods in terms of classification accuracy and robustness to different free parameters. The paper concludes that the proposed method is a promising approach to robust learning of Gaussian mixture state emission densities for HMMs and has potential applications in various fields.",1
1579,"one important class of state emission densities of the hidden markov model is the gaussian mixture densities . the classical baum-welch algorithm often fails to reliably learn the gaussian mixture densities when there is insufficient training data , due to the large number of free parameters present in the model . in this paper , we propose a novel strategy for robustly and accurately learning the gaussian mixture state emission densities of the hidden markov model . the strategy is based on an ensemble framework for probability density estimation in which the learning of the gaussian mixture densities is formulated as a gradient descent search in a function space . the resulting learning algorithm is called '' the boosting baum-welch algorithm . '' our preliminary experiment results on emotion recognition from speech show that the proposed algorithm outperforms the original baum-welch algorithm on this task .",0
1580,"This paper defends the use of the Random Sample Consensus (RANSAC) algorithm for outlier rejection in deformable registration, which is an essential task in accurately estimating non-rigid deformations. The paper proposes a novel RANSAC-driven deformable registration technique that utilizes both synthetic and real data to achieve accurate outlier rejection and optimize fully deformable models. The proposed approach is evaluated on both synthetic and real data, and experimental results demonstrate that the RANSAC-driven technique outperforms existing methods in terms of accuracy and robustness to outliers. The paper also provides theoretical analysis and error bounds to demonstrate the effectiveness of the proposed method in accurately estimating physical deformations. The proposed approach is based on feature correspondences and utilizes low-dimensional hyperplane manifold to optimize the fully deformable models. The paper concludes that the RANSAC-driven deformable registration technique is a promising approach for accurate and robust outlier rejection in deformable registration and has potential applications in various fields.",1
1581,"this paper concerns the robust estimation of non-rigid deformations from feature correspondences . we advance the surprising view that for many realistic physical deformations , the error of the mismatches -lrb- outliers -rrb- usually dwarfs the effects of the curvature of the manifold on which the correct matches -lrb- inliers -rrb- lie , to the extent that one can tightly enclose the manifold within the error bounds of a low-dimensional hy-perplane for accurate outlier rejection . this justifies a simple ransac-driven deformable registration technique that is at least as accurate as other methods based on the optimisation of fully deformable models . we support our ideas with comprehensive experiments on synthetic and real data typical of the deformations examined in the literature .",0
1582,"This paper proposes a robust talking face video verification system using joint factor analysis (JFA) and sparse representation on Gaussian mixture models (GMM) mean shifted supervectors. The system employs block-wise local features and a GMM-ZTnorm baseline model with a posteriori adapted model to address session variabilities. The proposed system achieves a relative error reduction and a lower error rate than the GMM-ZTnorm baseline. The system also incorporates over-complete dictionaries and L1-minimization with quadratic constraints to achieve robustness against recording devices and lighting conditions. The Banca talking face video database is used to evaluate the proposed system's performance in terms of complexity and robustness. The experimental results show that the proposed system can accurately verify video-based talking faces in the presence of facial expressions, session variabilities, and environmental factors.",1
1583,"it has been previously demonstrated that systems based on block wise local features and gaussian mixture models are suitable for video based talking face verification due to the best trade-off in terms of complexity , robustness and performance . in this paper , we propose two methods to enhance the robustness and performance of the gmm-ztnorm baseline system . first , joint factor analysis is performed to compensate the session variabilities due to different recording devices , lighting conditions , facial expressions , etc. . second , the difference between the universal background model and the maximum a posteriori adapted model is mapped into the gmm mean shifted supervector whose over-complete dictionary becomes more incoherent . then , for verification purpose , the sparse representation computed by l 1-minimization with quadratic constraints is employed to model these gmm mean shifted su-pervectors . experimental results show that the proposed system achieved 8.4 % -lrb- group 1 -rrb- and 10.5 % -lrb- group 2 -rrb- equal error rate on the banca talking face video database following the p protocol and outperformed the gmm-ztnorm baseline by yielding more than 20 % relative error reduction .",0
1584,"This paper proposes an algorithmic strategy for regularized stochastic online learning on large or streaming data sets. Specifically, the paper focuses on the dual averaging algorithm, which uses approximate subgradient directions to optimize a loss function subject to a nonsmooth regularization term. The authors show that the set of near-optimal points of this algorithm is contained in a low-dimensional manifold, and they provide an iterative method for identifying this manifold. The paper discusses the advantages of identifying this manifold, including faster convergence rates and reduced computational cost, and demonstrates the effectiveness of the proposed method on various stochastic learning problems. Overall, this work provides a novel approach to regularized stochastic online learning and highlights the importance of considering the intrinsic dimension of the problem when designing optimization algorithms.",1
1585,"iterative methods that take steps in approximate subgradient directions have proved to be useful for stochastic learning problems over large or streaming data sets . when the objective consists of a loss function plus a nonsmooth regularization term , whose purpose is to induce structure -lrb- for example , spar-sity -rrb- in the solution , the solution often lies on a low-dimensional manifold along which the regularizer is smooth . this paper shows that a regularized dual averaging algorithm can identify this manifold with high probability . this observation motivates an algorith-mic strategy in which , once a near-optimal manifold is identified , we switch to an regularized dual averaging algorithm that searches only in this manifold , which typically has much lower intrinsic dimension than the full space , thus converging quickly to a near-optimal point with the desired structure . computational results are presented to illustrate these claims .",0
1586,"This paper presents a statistical segment-based approach for spoken language understanding (SLU) in a spoken dialog system. The proposed method utilizes a corpus of unaligned pairs of sentences and variable-length word segments to model semantic units for the understanding component. Decoding of user utterances is performed using statistical language understanding models that provide a semantic representation of the input. The approach is demonstrated on a Spanish railway information retrieval task, and results show that corpus-based approaches outperform rule-based methods. The proposed segment-based approach offers a promising framework for SLU that can be extended to other languages and domains.",1
1587,"in this paper we propose an algorithm to learn statistical language understanding models from a corpus of unaligned pairs of sentences and their corresponding semantic representation . specifically , it allows to automatically map variable-length word segments with their corresponding semantic units and thus , the decoding of user utterances to their corresponding meanings . in this way we avoid the time consuming work of manually associate semantic labels to words , process which is needed by almost all the corpus-based approaches . we use the algorithm to learn the understanding component of a spoken dialog system for railway information retrieval in spanish . experiments show that the results obtained with the proposed method are very promising , whereas the effort employed to obtain the models is not comparable with this of manually segment the training corpus .",0
1588,"This paper proposes a multi-cue visual tracking system that uses a robust feature-level fusion technique based on joint sparse representation. The system employs fusion-based trackers that integrate multiple cues to improve tracking performance. A joint sparse representation model is used to effectively combine the cues and handle unreliable features in the video sequence. The model is applied to both pose and illumination estimation tasks, and is shown to improve robustness to feature occlusion. Experimental results demonstrate the effectiveness of the proposed method on a variety of challenging tracking scenarios.",1
1589,"the use of multiple features for tracking has been proved as an effective approach because limitation of each feature could be compensated . since different types of variations such as illumination , occlusion and pose may happen in a video sequence , especially long sequence videos , how to dynamically select the appropriate features is one of the key problems in this approach . to address this issue in multi-cue visual tracking , this paper proposes a new joint sparse representation model for robust feature-level fusion . the proposed joint sparse representation model dynamically removes unreliable features to be fused for tracking by using the advantages of sparse representation . as a result , robust tracking performance is obtained . experimental results on publicly available videos show that the proposed joint sparse representation model outperforms both existing sparse representation based and fusion-based trackers .",0
1590,This paper proposes a Super-resolution Person re-identification (SR-ReID) method using Semi-coupled Low-rank Discriminant Dictionary Learning (SLD2L). The proposed method leverages the intrinsic feature spaces of high-resolution (HR) and low-resolution (LR) images for person re-identification task. The approach employs a multi-view SLD2L approach to learn type-specific dictionary pairs for both HR and LR training images. The low-resolution SR person re-identification task is addressed by converting LR features to HR features using mapping matrices. A discriminative capability is incorporated through discriminant term and low-rank regularization term. The proposed method is evaluated on publicly available datasets and outperforms the state-of-the-art methods in various illumination and identification scenarios.,1
1591,"person re-identification has been widely studied due to its importance in surveillance and forensics applications . in practice , gallery images are high-resolution -lrb- hr -rrb- while probe images are usually low-resolution in the identification scenarios with large variation of illumination , weather or quality of cameras . person re-identification in this kind of scenarios , which we call super-resolution person re-identification , has not been well studied . in this paper , we propose a semi-coupled low-rank discriminant dictionary learning -lrb- sld2l -rrb- approach for sr person re-identification task . with the hr and lr dictionary pair and mapping matrices learned from the features of hr and lr training images , sld2l can convert the features of lr probe images into hr features . to ensure that the converted features have favorable discriminative capability and the learned dictionaries can well characterize intrinsic feature spaces of hr and lr images , we design a discriminant term and a low-rank regularization term for sld2l . moreover , considering that low resolution results in different degrees of loss for different types of visual appearance features , we propose a multi-view sld2l approach , which can learn the type-specific dictionary pair and mappings for each type of feature . experimental results on multiple publicly available datasets demonstrate the effectiveness of our proposed approaches for the sr person re-identification task .",0
1592,"This paper presents an emotional Malay speech synthesis system that integrates rule-based and template-based approaches. The system uses prosody manipulation principles and prosody parametric manipulation to synthesize emotional speech in Malay. The emotional synthesizer manipulates the prosody of the text input to generate emotion-specific intonation patterns and duration for sadness and anger. The system also employs prosody templates to generate synthetic speech with emotional features. The duration and intonation patterns are modified based on the prosody parametric manipulation principle, while the prosody templates are used to synthesize speech with the desired emotional features. The results demonstrate that the proposed system can synthesize emotional speech with high quality and naturalness.",1
1593,"the manipulation of prosody , including pitch , duration and intensity , is one of the leading approaches in synthesizing emotion . this paper reports work on the development of a malay emotional synthesizer capable of expressing four basic emotions , namely happiness , anger , sadness and fear for any form of text input with various intonation patterns using the prosody manipulation principle . the malay emotional synthesizer makes use of prosody templates and prosody parametric manipulation for different types of sentence structure .",0
1594,"This paper proposes a production domain modeling approach for the pronunciation of visual speech recognition. The work explores the use of feature-based and viseme-based units for automatic speech recognition in the visual modality. The study involves the use of inter-feature asynchrony and dynamic Bayesian network for isolated-word visual speech recognition tasks. The approach also utilizes support vector machine feature classifiers to model visually-salient features. The paper presents the articulatory feature models and its relevance in the phone-based models of speech, as well as its impact on the automatic speech recognition community. The study results indicate that the feature-based model is useful for visual speech recognition tasks.",1
1595,"articulatory feature models have been proposed in the automatic speech recognition community as an alternative to phone-based models of speech . in this paper , we extend this approach to the visual modality . specifically , we adapt a recently proposed feature-based model of pronunciation variation to visual speech recognition using a set of visually-salient features . the model uses a dynamic bayes-ian network to represent the evolution of the feature streams . a bank of svm feature classifiers , with outputs converted to likelihoods , provides input to the visual speech recognition . we present preliminary experiments on an isolated-word vsr task , comparing feature-based and viseme-based units and studying the effects of modeling inter-feature asynchrony .",0
1596,"This paper presents ParaQuery, a tool designed to help make sense of pivoted paraphrase collections. Pivoted paraphrase collections have been shown to be useful in a range of natural language processing (NLP) applications. However, it can be challenging to understand the structure and content of these collections, particularly when dealing with large bilingual parallel corpora. ParaQuery addresses this challenge by providing a user-friendly interface for exploring pivoted paraphrase collections and lexical similarity resources. The tool includes features for filtering and sorting paraphrases based on various criteria, as well as visualizations for exploring the structure of the collection. Overall, ParaQuery aims to facilitate the use of pivoted paraphrase collections in NLP research and applications.",1
1597,"pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition . although such pivoted paraphrase collections have been successfully used to improve the performance of several different nlp applications , it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections . we present paraquery , a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection , analyze its utility for a particular domain , and compare it to other popular lexical similarity resources -- all within a single interface .",0
1598,"This paper proposes a new method for the computation of the dual frame of oversampled filter banks. The method uses forward and backward Greville formulas for the computation of the transform coefficients. The paper shows that the backward Greville formula is useful for robust transmission over erasure channels, and the forward Greville formula is useful for post-filtering structures. The paper also discusses the application of the method to the Laplacian pyramid and row-subband oversampled filter banks. The results show that the proposed method outperforms existing methods in terms of computational efficiency and numerical stability. The proposed method has potential applications in image and signal processing.",1
1599,"we study the computation of the dual frame for oversampled filter banks by exploiting greville 's formula , which was derived in 1960 to compute the pseudo inverse of a matrix when a new row is appended . in this paper , we first develop the backward greville formula to handle the case of row deletion . based on greville 's formula , we then study the dual frame computation of the laplacian pyramid . through the backward greville formula , we investigate ofbs for robust transmission over erasure channels . the necessary and sufficient conditions for ofbs robust to one erasure channel are derived . a post-filtering structure is also presented to implement the dual frame when the transform coefficients in one subband are completely lost .",0
1600,"The paper ""When VLAD Met Hilbert"" proposes a new approach for image and video recognition tasks using a combination of vectors of locally aggregated descriptors (VLAD) and Hilbert space representations. The authors introduce the use of nonlinear VLAD descriptors to handle non-vector descriptors such as covariance descriptors, which are common in visual recognition tasks. The proposed method utilizes a kernelized version of the VLAD framework to encode the local descriptors into a manifold-valued data representation, allowing for more effective classification schemes using linear classifiers. The paper also presents an approximate formulation for the coding process, which makes use of the Hilbert space structure to better handle the nonlinearities present in the data. The experimental results demonstrate the effectiveness of the proposed method on several benchmark datasets.",1
1601,"vectors of locally aggregated descriptors have emerged as powerful image/video representations that compete with or even outperform state-of-the-art approaches on many challenging visual recognition tasks . in this paper , we address two fundamental limitations of vectors of locally aggregated descriptors : its requirement for the local descriptors to have vector form and its restriction to linear classifiers due to its high-dimensionality . to this end , we introduce a kernelized version of vectors of locally aggregated descriptors . this not only lets us inherently exploit more sophisticated classification schemes , but also enables us to efficiently aggregate non-vector descriptors -lrb- e.g. , tensors -rrb- in the vlad framework . furthermore , we propose three approximate formulations that allow us to accelerate the coding process while still benefiting from the properties of kernel vlad . our experiments demonstrate the effectiveness of our approach at handling manifold-valued data , such as covariance descriptors , on several visual recognition tasks . our results also evidence the benefits of our nonlinear vlad descriptors against the linear ones in euclidean space using several standard benchmark datasets .",0
1602,"This paper proposes a fast and efficient method for sampling determinantal point processes (DPPs) with application to kernel methods and the Nyström method. DPPs are used for landmark selection, and the Nyström method is used to approximate kernel matrices. The proposed method uses a Markov chain approach to sample from DPPs and achieves linear time complexity in the number of landmarks. The authors provide a theoretical analysis of the method and show that it has cubic complexity in the Nyström landmarks. The method is applied to kernel ridge regression and tested on several datasets, showing that it outperforms previous methods while also being faster. The paper also discusses the approximation errors and the impact of the number of landmarks on the performance. Overall, the proposed method provides a fast and accurate way to sample DPPs for landmark selection in kernel methods.",1
1603,"the nyström method has long been popular for scaling up kernel methods . its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected . we study landmark selection for nyström using determi-nantal point processes , discrete probability models that allow tractable generation of diverse samples . we prove that landmarks selected via determi-nantal point processes guarantee bounds on approximation errors ; subsequently , we analyze implications for kernel ridge regression . contrary to prior reservations due to cubic complexity of dpp sampling , we show that -lrb- under certain conditions -rrb- markov chain dpp sampling requires only linear time in the size of the data . we present several empirical results that support our theoretical analysis , and demonstrate the superior performance of dpp-based landmark selection compared with existing approaches .",0
1604,"This paper focuses on the automated generation of contingency plans using Markov decision processes. The goal is to derive optimal and understandable plans that can be executed in case of unexpected events. The paper proposes an anytime algorithm that uses heuristic techniques to generate such plans efficiently. The algorithm generates plans in a sequential manner, allowing the user to stop the generation process at any time to obtain a partial plan. The paper demonstrates the effectiveness of the proposed approach in generating understandable contingency plans.",1
1605,"markov decision processes -lrb- markov decision processes -rrb- and contingency planning are two widely used approaches to planning under uncertainty . markov decision processes are attractive because the model is extremely general and because many algorithms exist for deriving optimal plans . in contrast , contingency planning is normally performed using heuristic techniques that do not guarantee op-timality , but the resulting plans are more compact and more understandable . the inability to present contingency planning in a clear , intuitive way has limited their applicability in some important domains . we introduce an anytime algorithm for deriving contingency plans that combines the advantages of the two approaches .",0
1606,"This paper proposes a method for mining search engine clickthrough logs to improve web search ranking by extracting matching n-gram features. The authors leverage limited user click data to identify exact match click features and query-url n-gram features, which are then used to generate a model that can accurately predict user clicks on unseen words and URLs. The proposed approach addresses the challenge of sparsity in clickthrough logs and leverages the regularities in URLs to improve performance. The research is of interest to the NLP community and offers a promising way to improve web ranking tasks.",1
1607,"user clicks on a url in response to a query are extremely useful predictors of the url 's relevance to that query . exact match click features tend to suffer from severe data sparsity issues in web ranking . such sparsity is particularly pronounced for new urls or long queries where each distinct query-url pair will rarely occur . to remedy this , we present a set of straightforward yet informative query-url n-gram features that allows for generalization of limited user click data to large amounts of unseen query-url pairs . the query-url n-gram features is motivated by techniques leveraged in the nlp community for dealing with unseen words . we find that there are interesting regularities across queries and their preferred destination urls ; for example , queries containing '' form '' tend to lead to clicks on urls containing '' pdf '' . we evaluate our set of new query-url n-gram features on a web search ranking task and obtain improvements that are statistically significant at a p-value < 0.0001 level over a strong baseline with exact match clickthrough features .",0
1608,"This paper proposes a new move-making algorithm for adaptive graph-cut-based image labeling problems. The algorithm, called the expansion-move algorithm, uses local primal-dual gaps to guide alpha-expansion, a well-known technique for energy minimization in image labeling. The algorithm makes use of pre-defined moves such as expansion and swap to efficiently search the space of possible labelings. Experimental results show that the proposed algorithm outperforms state-of-the-art move-making algorithms on object segmentation and stereo tasks. Furthermore, the paper demonstrates the effectiveness of the proposed algorithm in dealing with the sparsity of user clicks and the regularities of URLs in web ranking tasks.",1
1609,"this paper presents a new adaptive graph-cut based move-making algorithm for energy minimization . traditional move-making algorithms such as expansion and swap operate by searching for better solutions in some pre-defined moves spaces around the current solution . in contrast , our adaptive graph-cut based move-making algorithm uses the primal-dual interpretation of the expansion-move algorithm to adaptively compute the best move-space to search over . at each step , it tries to greedily find the move-space that will lead to biggest decrease in the primal-dual gap . we test different variants of our adaptive graph-cut based move-making algorithm on a variety of image labelling problems such as object segmentation and stereo . experimental results show that our adaptive graph-cut based move-making algorithm significantly outper-forms the conventional expansion-move algorithm , in some cases cutting the runtime by 50 % .",0
1610,"This paper presents an evaluation of texture segmentation algorithms using region-based and pixel-based performance metrics. The control scheme of texture segmentation is analyzed using various feature extraction methods, such as gray level co-occurrence matrix and Laws' texture energy. Unsupervised texture segmentation algorithms are evaluated on real scene images using manually-specified ground truth segmentation algorithms. Modular processes like split-and-merge and square-error clustering are also compared, and Gabor multi-channel filtering is used for feature extraction. Fuzzy C-means clustering is also used in conjunction with square-error clustering for evaluating the segmentation algorithms. The results show the effectiveness of different feature values in homogeneous regions of real images, and the paper provides insights into the evaluation of texture segmentation algorithms.",1
1611,"this paper presents a method of evaluating unsu-pervised texture segmentation algorithms . the control scheme of texture segmentation has been conceptualized as two modular processes : -lsb- l -rrb- feature computation and -lrb- 2 -rrb- segmentation of homogeneous regions based on the feature values . three feature extraction methods are considered : gray level co-occurrence matrax , laws ' texture energy and gabor multi-channel filtering . three segmentation algorithms are considered : fuzzy c-means clustering , square-error clustering and split-and-merge . a set of 35 real scene images with manually-specified ground truth was compiled . performance is measured against ground truth on real images using region-based and pixel-based performance met-rics .",0
1612,"This paper proposes a method called Filtered Variation (FV) for denoising and sparse signal processing. The FV problem is formulated as an optimization problem that aims to recover a signal from its noisy and sparse measurements, using a convex sets approach and total variation regularization. The method involves alternating projections and discrete-time filters to achieve denoising and recovery of sparse signals. The paper also discusses applications of sparse signal processing and demonstrates the effectiveness of the proposed FV method in denoising and sparse signal processing.",1
1613,"we propose a new framework , called filtered variation -lrb- fv -rrb- , for de-noising and sparse signal processing applications . these problems are inherently ill-posed . hence , we provide regularization to overcome this challenge by using discrete time filters that are widely used in signal processing . we mathematically define the fv problem , and solve it using alternating projections in space and transform domains . we provide a globally convergent algorithm based on the projections onto convex sets approach . we apply to our algorithm to real denoising problems and compare it with the total variation recovery .",0
1614,"This paper proposes a canonicalization process for feature parameters in automatic speech recognition systems. The goal is to improve robustness by accounting for variation in speaking rate, gender, and acoustic environment. The proposed approach uses acoustic feature extractors to compute acoustic feature vectors, which are then transformed into discriminant feature parameters (DPF) using a multilayer neural network. The DPF spaces are then canonicalized to remove variability due to hidden variables such as speaking rate and gender. The canonicalized DPF is used as input to an HMM-based classifier, which improves recognition performance. The paper evaluates the proposed method on a robust automatic speech recognition system and shows that the canonicalization process reduces computation time and improves performance compared to previous methods.",1
1615,"acoustic models -lrb- acoustic models -rrb- of an hmm-based classifier include various types of hidden variables such as gender type , speaking rate , and acoustic environment . if there exists a canonicalization process that reduces the influence of the hidden variables from the acoustic models , a robust automatic speech recognition system can be realized . in this paper , we describe the configuration of a canonicalization process targeting gender type as a hidden variable . the proposed canonicalization process is composed of multiple distinctive phonetic feature extractors corresponding to the hidden variable and a dpf selector in which the distance between input dpf and acoustic models is compared . in a dpf extraction stage , an input sequence of acoustic feature vectors is mapped onto three dpf spaces corresponding to male , female , and neutral voice by using three multilayer neural networks -lrb- mlns -rrb- . experiments are carried out by comparing -lrb- a -rrb- the combination of the canonicalized dpf and a single hmm classifier , and -lrb- b -rrb- the combination of a single acoustic feature and multiple hmm classifiers . the result shows that the proposed canonicalization process outperforms both of the conventional robust automatic speech recognition system with acoustic feature and a single hmm and the robust automatic speech recognition system with multiple hmms in spite of less memories and computation time .",0
1616,"This paper presents a novel approach to sparse spectral factorization, which has important applications in signal processing and communications. The authors prove the unicity of the one-dimensional sparse signal reconstruction problem, and propose an iterative algorithm for solving it. They also demonstrate the usefulness of their approach through numerical simulations, and show how it can be applied in phase retrieval and x-ray crystallography. The paper highlights the significance of sparse spectral factorization as a classical tool in signal processing and communications, and its potential for solving problems related to autocorrelation and time-shift.",1
1617,"spectral factorization is a classical tool in signal processing and communications . it also plays a critical role in x-ray crystallography , in the context of phase retrieval . in this work , we study the problem of sparse spectral factorization , aiming to recover a one-dimensional sparse signal from its autocorrelation . we present a sufficient condition for the recovery to be unique , and propose an iterative algorithm that can obtain the original signal -lrb- up to a sign change , time-shift and time-reversal -rrb- . numerical simulations verify the effectiveness of the proposed algorithm .",0
1618,"This paper proposes a statistical inference process using Segmental Hidden Markov Models (HMM) for view-based sport video analysis. The proposed approach employs a generative model approach that captures intrinsic semantic structures in American football games and other sport videos. The model utilizes a two-layer observation model to account for the variability of visual features across camera views. The Segmental HMM is a powerful extension of the traditional HMM that is capable of modeling complex structures in video data. The proposed method is evaluated through numerical simulations, and the results demonstrate its effectiveness in view-based shot classification and video mining tasks. The generative model approach and the Segmental HMM provide a promising framework for analyzing sport videos and other visual data that involve complex semantic structures.",1
1619,"we present a generative model approach to explore intrinsic semantic structures in sport videos , e.g. , the camera view in american football games . we will invoke the concept of semantic space to explicitly define the semantic structure in the video in terms of latent states . a generative model approach is used to govern the transition between states , and an generative model approach is developed to characterize visual features pertaining to different states . then the problem is formulated as a statistical inference process where we want to infer latent states -lrb- i.e. , camera views -rrb- from observations -lrb- i.e. , visual features -rrb- . two generative models , the hidden markov model and the segmental hmm , are involved in this research . in the hidden markov model , both latent states and visual features are shot-based , and in the segmental hmm , latent states and visual features are defined for shots and frames respectively . both generative model approach provide promising performance for view-based shot classification , and the segmental hmm outper-forms the hidden markov model by involving a two-layer observation model to accommodate the variability of visual features . this generative model approach is also applicable to other video mining tasks .",0
1620,This paper presents a novel method for sequential source localization in a wireless distributed sensor network using particle filter. The traditional maximum likelihood source localization algorithm is improved by introducing a non-Gaussian probability density function for acoustic energy-based source localization. The proposed sequential source localization method employs a state transition equation and an observation equation to estimate the source location with high accuracy. The particle filter is used to track the source location by iteratively updating the particle positions based on acoustic signal measurements from the distributed acoustic sensors. The proposed method can handle multiple-target locations and provides reliable location estimates even in the presence of parameter perturbation. Numerical simulations are conducted to demonstrate the effectiveness of the proposed method.,1
1621,"a sequential source localization method using particle filter is presented to estimate and track multiple-target locations . this sequential source localization method is designed to make use of acoustic signal measured at multiple acoustic sensors randomly deployed in a wireless distributed sensor network . by using the particle filter , non-gaussian probability density function of the target locations are represented by a discrete set of '' particles '' . the positions of these particles are propagated sequentially using known state transition equation , and updated using new location estimates via the observation equation . compared to a previously proposed maximum likelihood source localization algorithm , this new sequential source localization method is computationally effective and more robust to parameter perturbation .",0
1622,"This paper introduces a robust kernel density estimator (KDE) based on the m-estimator objective function, which ensures the estimator's robustness to outliers. The proposed estimator is developed in the reproducing kernel Hilbert space framework and uses a positive semi-definite kernel. The paper proves the representer theorem for the KDE, which leads to a convex optimization problem that can be solved using the kernelized iteratively reweighted least squares (IRWLS) algorithm. The paper also introduces the influence function of the estimator and shows that it characterizes the estimator's robustness. The proposed estimator is evaluated for density estimation and anomaly detection tasks and compared to the classical m-estimation method. The numerical results demonstrate the superior performance of the proposed estimator in terms of robustness and accuracy. The paper concludes that the proposed KDE has potential applications in various fields where robust nonparametric density estimation is required.",1
1623,"we propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample . this method achieves robustness by combining a traditional kernel density estimator with ideas from classical m-estimation . we interpret the kernel density estimator based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel hilbert space . since the sample mean is sensitive to outliers , we estimate kernel density estimator robustly via m-estimation , yielding a robust kernel density estimator . an kernel density estimator can be computed efficiently via a kernelized iteratively re-weighted least squares -lrb- irwls -rrb- algorithm . necessary and sufficient conditions are given for kernelized irwls to converge to the global minimizer of the m-estimator objective function . the robustness of the kernel density estimator is demonstrated with a representer theorem , the influence function , and experimental results for density estimation and anomaly detection .",0
1624,"Entity disambiguation is an essential task in natural language processing that aims to identify the correct meaning of a named entity mention in a given context. In this paper, we propose a novel approach for entity disambiguation using a Markov-Logic Network (MLN). Our approach utilizes a knowledge base of entries and interweaved constraints to create formulae/features for disambiguation. We also employ EL approaches and filtering techniques to improve the accuracy of our disambiguation results. Our experimental results show that our approach outperforms state-of-the-art methods on several benchmark datasets. Our work demonstrates the effectiveness of using MLNs for entity disambiguation, and highlights the importance of incorporating background knowledge and constraints into the model.",1
1625,"* entity linking -lrb- el -rrb- is the task of linking a textual named entity mention to a knowledge base entry . it is a difficult task involving many challenges , but the most crucial problem is entity ambiguity . traditional el approaches usually employ different constraints and filtering techniques to improve performance . however , these constraints are executed in several different stages and can not be used interactively . in this paper , we propose several disambiguation formulae/features and employ a markov logic network to model interweaved constraints found in one type of el , gene mention linking . to assess our systems effectiveness in different applications , we adopt two evaluation schemes : article-wide and instance-based precision/recall/f-measure . experimental results show that our system outperforms the baseline systems and state-of-the-art systems under both evaluation schemes .",0
1626,"This paper proposes a novel burst-based text representation model, called BurstVSM, for scalable event detection that incorporates temporal aspects of documents. Unlike classic text representation models, BurstVSM captures both semantic and temporal information by representing text as a series of bursts, each containing words that co-occur within a certain time window. The proposed model is evaluated on a 10-year news archive, and compared with the traditional vector space model. Results show that BurstVSM outperforms the vector space model in terms of accuracy and efficiency for mining retrospective events. The proposed model also has the advantage of having a sparse representation, as only the bursts with non-zero entries are used, making it suitable for processing large text streams with bursty features.",1
1627,"mining retrospective events from text streams has been an important research topic . classic text representation model -lrb- i.e. , vector space model -rrb- can not model temporal aspects of documents . to address burstvsm , we proposed a novel burst-based text representation model , denoted as burstvsm . burstvsm corresponds dimensions to bursty features instead of terms , which can capture semantic and temporal information . meanwhile , burstvsm significantly reduces the number of non-zero entries in the representation . we test burstvsm via scalable event detection , and experiments in a 10-year news archive show that our burstvsm are both effective and efficient .",0
1628,This paper proposes a convex solution to the correspondence problem by spatially regularizing the data consistency energy function. The goal is to find a globally optimal solution to this problem using a convex minimization approach. The proposed method uses geometric measure theory to define a minimal two-dimensional surface that satisfies certain constraints. A convex formulation is then developed to ensure the spatial regularity of the solution. The method is implemented using a primal-dual algorithm to solve the convex optimization problem. The effectiveness of the proposed method is demonstrated through experiments on two-dimensional surfaces and 2-vector fields. The results show that the proposed method achieves a globally optimal solution with improved accuracy compared to existing methods.,1
1629,"we propose a convex formulation of the correspondence problem between two images with respect to an energy function measuring data consistency and spatial regularity . to this end , we formulate the general correspondence problem as the search for a minimal two-dimensional surface in r 4 . we then use tools from geometric measure theory and introduce 2-vector fields as a representation of two-dimensional surfaces in r 4 . we propose a discretization of this convex formulation that gives rise to a convex minimization problem and compute a globally optimal solution using an efficient primal-dual algorithm .",0
1630,This paper proposes a modeling approach for negotiation subdialogues in task-oriented interactions. The approach utilizes a robust natural language understanding system and a multi-strength belief model to process the communicative actions in negotiation subdialogues. The model considers knowledge sources and a process model to incorporate expressions of doubt and effectively handle the discourse actions. A processing strategy is applied to ensure the system's robustness in interpreting the negotiated propositions. The proposed approach is expected to improve the efficiency and effectiveness of negotiation in multi-agent activities.,1
1631,"this paper presents a robust natural language understanding system that handles negotiation subdialogues by inferring both the communicative actions that people pursue when speaking and the beliefs underlying these actions . we contend that recognizing the complex discourse actions pursued in negotiation subdialogues -lrb- e.g. , expressing doubt -rrb- requires both a multi-strength belief model and a process model that combines different knowledge sources in a unified framework . we show how our robust natural language understanding system identifies the structure of negotiation subdialogues , including recognizing expressions of doubt , implicit acceptance of communicated propositions , and negotiation subdialogues embedded within other negotiation subdialogues . 1 introduction since negotiation is an integral part of multi-agent activity , a robust natural language understanding system must be able to handle subdi-alogues in which participants negotiate what has been claimed in order to try to come to some agreement about those claims . to handle such dialogues , the robust natural language understanding system must be able to recognize when a dialogue participant has initiated a negotiation subdialogue and why the participant began the negotiation -lrb- i.e. , what beliefs led the participant to start the negotiation -rrb- . this paper presents a robust natural language understanding system of task-oriented interactions that assimilates negotiation subdialogues by inferring both the communicative actions that people pursue when speaking and the beliefs underlying these actions . we will argue that recognizing the complex discourse actions pursued in negotiation subdialogues -lrb- e.g. , expressing doubt -rrb- requires both a multi-strength belief model and a processing strategy that combines different knowledge sources in a unified framework , and we will show how our robust natural language understanding system incorporates these and recognizes the structure of negotiation subdialogues .",0
1632,"This paper proposes a method for abandoned object detection using relative attributes. The goal is to reduce false alarms in large-scale video surveillance by ranking abandoned objects based on their attributes. Low-level spatial and temporal features are used to extract attributes such as staticness, foregroundness, and abandon-ment. A linear ranking algorithm is employed to assign a score to each attribute, and a ranking function is learned to combine these scores into a final ranking. The proposed method is evaluated on public datasets, and the results show that it outperforms classic approaches in terms of detection accuracy. The use of relative attributes enables the detection of abnormal events of interest, such as people and light artifacts, and allows for a large-scale deployment of the system.",1
1633,"effective reduction of false alarms in large-scale video surveillance is rather challenging , especially for applications where abnormal events of interest rarely occur , such as abandoned object detection . we develop an approach to prioritize alerts by ranking them , and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy . our approach benefits from a novel representation of abandoned object alerts by relative attributes , namely staticness , foregroundness and abandon-ment . the relative strengths of these relative attributes are quantified using a ranking function -lsb- 19 -rsb- learnt on suitably designed low-level spatial and temporal features.these attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts , but also computationally efficient for large-scale deployment . with these features , we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user . we test the effectiveness of our approach on both public data sets and large ones collected from the real world .",0
1634,"This paper proposes a new algorithm, called Remix-UCB, for addressing both stationary and restless mixing bandit problems. The restless bandit problem considers a scenario where the payoff distributions of arms change over time, while the stationary ϕ-mixing processes are a class of stochastic processes that satisfy a certain mixing condition. The proposed algorithm is based on the well-known UCB algorithm and utilizes a waiting arm and a suboptimal arm to balance exploration and exploitation. The Remix-UCB algorithm is proven to achieve a logarithmic regret bound in both the stationary and restless settings. The ϕ-mixing coefficients are also analyzed to show the efficacy of the algorithm in different environments. Experimental results demonstrate the effectiveness of Remix-UCB compared to existing algorithms.",1
1635,"we study the restless bandit problem where arms are associated with stationary ϕ-mixing processes and where rewards are therefore dependent : the question that arises from this setting is that of carefully recovering some independence by ` ig-noring ' the values of some rewards . as we shall see , the restless bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off , which we do by considering the idea of a waiting arm in the new remix-ucb algorithm , a generalization of improved-ucb for the problem at hand , that we introduce . we provide a regret analysis for this restless bandit problem ; two noticeable features of remix-ucb are that i -rrb- regret analysis reduces to the regular improved-ucb when the ϕ-mixing coefficients are all 0 , i.e. when the i.i.d scenario is recovered , and ii -rrb- when ϕ -lrb- n -rrb- = o -lrb- n − α -rrb- , regret analysis is able to ensure a controlled regret of order θ ∆ -lrb- α − 2 -rrb- / α * log 1 / α t , where ∆ * encodes the distance between the best arm and the best suboptimal arm , even in the case when α < 1 , i.e. the case when the ϕ-mixing coefficients are not summable .",0
1636,"This paper presents a novel approach for solving the permutation problem in frequency domain Blind Source Separation (BSS) by using a coupled Hidden Markov Model (HMM). The proposed algorithm utilizes psychoacoustic characteristics of signals and a cross-power spectrum-based cost function with a non-unitary penalty term to estimate the mixing matrix. The state transitions of the HMM are associated with the frequency bins of the input signal, and the joint diagonalization algorithm is used to estimate the mixing matrix in each state. Simulation studies demonstrate that the proposed approach outperforms existing methods in terms of separation quality and robustness to permutation effect. The coupled HMM-based approach provides a promising solution for frequency domain convolutive BSS.",1
1637,"permutation of the outputs at different frequency bins remains as a major problem in the convolutive blind source separation . in this work a coupled hidden markov model effectively exploits the psychoacoustic characteristics of signals to mitigate such permutation . a joint diagonalization algorithm for convolutive bss , which incorporates a non-unitary penalty term within the cross-power spectrum-based cost function in the frequency domain , has been used . the proposed coupled hidden markov model couples a number of conventional hmms , equivalent to the number of outputs , by making state transitions in each model dependent not only on its own previous state , but also on some aspects of the state of the other models . using this joint diagonalization algorithm the permutation effect has been substantially reduced , and demonstrated using a number of simulation studies .",0
1638,This paper proposes criteria for direct blind deconvolution of multi-input multi-output (MIMO) finite impulse response (FIR) systems driven by white non-Gaussian source signals. The so-called independent and identically distributed (i.i.d.) condition of the source signals is assumed. The paper focuses on adaptive algorithms that are able to perform blind deconvolution by maximizing certain criteria. The proposed maximization criteria allow for the direct computation of the MIMO FIR system impulse response from the observations without resorting to matrix inversions. The paper also discusses some simulation results to illustrate the performance of the proposed approach.,1
1639,"this paper addresses the blind deconvolution of multi-input -- multi-output fir systems driven by white non-gaussian source signals . first , we present a weaker condition on source signals than the so-called i.i.d. condition so that blind deconvolution is possible . then , under this condition , we provide a necessary and sufficient condition for blind deconvolution of mimo fir systems . finally , based on this result , we propose two maximization criteria for blind deconvolution of mimo fir systems . these maximization criteria are simple enough to be implemented by adaptive algorithms .",0
1640,"This paper proposes a novel method called Attribute2Image for conditional image generation from visual attributes. The method uses a layered generative model with disentangled latent variables to generate natural images of faces that are conditioned on visual attributes. The approach involves posterior inference of latent variables and uses an energy minimization algorithm to optimize the model. The paper demonstrates the effectiveness of the approach through experiments that show high-quality attribute-conditioned image reconstruction. The proposed method is compared to other generative models such as the variational auto-encoder and is shown to produce superior results. The paper concludes that Attribute2Image has the potential to be used in a wide range of applications, including image generation and completion.",1
1641,"this paper investigates a novel problem of generating images from visual attributes . we model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder . we experiment with natural images of faces and birds and demonstrate that the proposed layered generative model are capable of generating realistic and diverse samples with disentangled latent representations . we use a general energy minimization algorithm for posterior inference of latent variables given novel images . therefore , the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion .",0
1642,"This paper proposes an adaptation-guided case-base maintenance approach for case-based reasoning. The approach utilizes adaptation knowledge and competence-based deletion strategies to guide case retention decisions. Specifically, the system utilizes case base densities to identify cases that contribute the most to the system competence, and applies compacting and case difference heuristic techniques to reduce the case base size while preserving the most informative cases. The proposed approach is evaluated on numerical prediction tasks and compared to other case-base maintenance approaches. Results show that the adaptation-guided approach outperforms other methods in terms of accuracy and size of the case base. The study demonstrates the effectiveness of incorporating adaptation knowledge and competence-based strategies in case-base maintenance for improving the performance of case-based reasoning systems.",1
1643,"in case-based reasoning , problems are solved by retrieving prior cases and adapting their solutions to fit ; learning occurs as new cases are stored . controlling the growth of the case base is a fundamental problem , and research on case-base maintenance has developed methods for compacting case bases while maintaining system competence , primarily by competence-based deletion strategies assuming static case adaptation knowledge . this paper proposes adaptation-guided case-base maintenance , a case-base maintenance approach exploiting the ability to dynamically generate new adaptation knowledge from cases . in adaptation-guided case-base maintenance , case retention decisions are based both on cases ' value as base cases for solving problems and on their value for generating new adaptation rules . the paper illustrates the method for numerical prediction tasks -lrb- case-based regression -rrb- in which adaptation rules are generated automatically using the case difference heuristic . in comparisons of adaptation-guided case-base maintenance to five alternative methods in four domains , for varying case base densities , adaptation-guided case-base maintenance outperformed the alternatives in all domains , with greatest benefit at high compression .",0
1644,"This paper proposes an estimator for the eigenvalues of the system matrix of a periodic-reference LMS algorithm. The algorithm is designed for the adaptation of linear time-periodic systems and is analyzed in terms of convergence. The monodromy matrix is used to characterize the behavior of the system over time, and the proposed estimator is compared with this matrix. The estimator is evaluated using stochastic signals, and the results show that it provides accurate estimates of the eigenvalues. The paper concludes that the proposed estimator can be a useful tool for analyzing and designing periodic-reference LMS algorithms for linear time-periodic systems.",1
1645,"the convergence analysis of the least mean square algorithm has been conventionally based on stochastic signals and describes thus only the average behavior of the least mean square algorithm . it has been shown previously that a periodic-reference lms system can be regarded as a linear time-periodic system whose stability can be determined from the monodromy matrix . generally , the monodromy matrix can only be solved numerically and does not thus reveal the actual factors behind the dynamics of the periodic-reference lms system . this paper derives an estimator for the eigenvalues of the monodromy matrix . the estimator is easy to calculate , and estimator also reveals the underlying reason for the bad convergence of the least mean square algorithm in some special cases . the estimator is confirmed by comparing estimator to the precise eigenvalues of the monodromy matrix . the estimator is found to be accurate for the eigenvalues close to unity .",0
1646,"This paper presents an effective perceptual weighting model for videophone coding, which aims to improve the coding quality of videophone-like sequences. The model takes into account various factors related to the human visual system, including cognition-driven factors such as luminance adaptation and skin color, as well as stimulus-driven factors. By incorporating these factors, the proposed model is able to achieve better rate control and perceptual coding quality for videophone applications. Experimental results show that the proposed model outperforms existing models in terms of both objective and subjective quality metrics.",1
1647,"in this paper , a perceptual weighting model is proposed for effective rate control so as to enhance perceptual coding quality of videophone , by exploiting two categories of factors affecting the perception of the human visual system : stimulus-driven factors and cognition-driven factors . in order to achieve a simple but effective perceptual weighting model , we use luminance adaptation and texture masking as the stimulus-driven factors , while skin color serves as the cognition-driven factor in the videophone application . both objective and subjective quality evaluations of videophone-like sequences in h. 263 platform validate the effectiveness of our perceptual weighting model .",0
1648,"This paper presents an HMM-based singing voice synthesis system that generates smooth and natural-sounding singing voice. The system is based on a corpus of singing voice and uses context-dependent HMMs to model the musical information, such as lyrics, durations, and singing style, and generate high-quality singing voice. The quality of the singing voice is evaluated based on the voice quality and the results show that the proposed system outperforms other corpus-based singing voice synthesis systems. The HMM-based approach also allows for the synthesis of speech, making it a versatile tool for speech and singing voice synthesis.",1
1649,"the present paper describes a corpus-based singing voice synthesis system based on hidden markov models . this corpus-based singing voice synthesis system employs the hmm-based speech synthesis to synthesize singing voice . musical information such as lyrics , tones , durations is modeled simultaneously in a unified framework of the context-dependent hmm . corpus-based singing voice synthesis system can mimic the voice quality and singing style of the original singer . results of a singing voice synthesis experiment show that the proposed corpus-based singing voice synthesis system can synthesize smooth and natural-sounding singing voice .",0
1650,"This paper investigates the pitch pattern variations in three regional varieties of American English. The study focuses on the southeastern Wisconsin dialect, the central Ohio dialect, and the southern (North Carolina) dialect. The research examines the melodic component of these varieties and analyzes the differences in the realization of nuclear pitch accents, the spectral dynamics of F0 contours, and the relative location of F0 contours with respect to voiced syllable codas. The study shows that there are significant dialect-related differences in the pitch patterns of these regional accents, particularly in the realization of southern vowels, Ohio or Wisconsin vowels, and flat F0 contours. The findings suggest that the variation in pitch patterns among these regional varieties of American English can be attributed to differences in the realization of vowels, unstressed vowels, and pitch patterns in the melodic component of the regional accents.",1
1651,"this acoustic study explored dialect effects on realization of nuclear pitch accents in three regional varieties of american english spoken in central ohio , southeastern wisconsin and western north carolina . fundamental frequency -lrb- f0 -rrb- change from vowel onset to offset in the most prominent syllable in a sentence was examined along four parameters : maximum f0 change , relative location of f0 maximum , f0 offset and f0 fall from maximum to offset . a robust finding was that the f0 contours in the southern -lrb- north carolina -rrb- variants were significantly distinct from the two midwestern varieties whose contours did not differ significantly from one another . the southern vowels had an earlier f0 rise , a greater f0 fall and a lower f0 offset than either ohio or wisconsin vowels . there was a sharper f0 drop preceding a voiceless than a voiced syllable coda . no significant dialect-related differences were found for flat f0 contours in unstressed vowels , which were also examined in the study . this study contributes the finding that dynamic variations in pitch are greater for vowels which also exhibit a greater amount of spectral dynamics . the interaction of these two sets of cues contributes to the melodic component associated with a specific regional accent .",0
1652,"This paper discusses the issue of saddle points in high-dimensional non-convex optimization problems, which are common in deep or recurrent neural network training and other high dimensional problems of practical interest. The proliferation of saddle points in continuous, high-dimensional spaces can lead to slow convergence or stagnation in optimization. This paper identifies the saddle point problem and discusses approaches for attacking it, including second-order optimization, local methods, and quasi-Newton methods. The paper also explores the connection between saddle points and local minima, as well as the global minimum, and draws on insights from statistical physics, random matrix theory, and neural network theory to inform its analysis. The findings have important implications for the science and engineering of optimization and for improving the performance of non-convex error functions in high-dimensional spaces.",1
1653,"a central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous , high dimensional spaces . gradient descent or quasi-newton methods are almost ubiquitously used to perform such minimizations , and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum . here we argue , based on results from statistical physics , random matrix theory , neural network theory , and empirical evidence , that a deeper and more profound difficulty originates from the proliferation of saddle points , not local minima , especially in high dimensional problems of practical interest . such saddle points are surrounded by high error plateaus that can dramatically slow down learning , and give the illusory impression of the existence of a local minimum . motivated by these arguments , we propose a new approach to second-order optimization , the saddle-free newton method , that can rapidly escape high dimensional saddle points , unlike gradient descent and quasi-newton methods . we apply this algorithm to deep or recurrent neural network training , and provide numerical evidence for its superior optimization performance .",0
1654,"This paper investigates the computational complexity of a projection problem onto the k-cosparse set, where k-sparse vectors are sought using ternary or bipolar coefficients in a sparse optimization setting. The authors show that the problem is NP-hard, meaning that it is computationally intractable in the worst-case scenario. The analysis sheds light on the difficulty of finding sparse representations and highlights the importance of developing efficient algorithms for solving related problems. The results are relevant to a wide range of applications in signal processing, machine learning, and optimization.",1
1655,"the computational complexity of a problem arising in the context of sparse optimization is considered , namely , the projection onto the set of k-cosparse vectors w.r.t. some given matrix ω . it is shown that this projection problem is -lrb- strongly -rrb- np-hard , even in the special cases in which the matrix ω contains only ternary or bipolar coefficients . interestingly , this is in contrast to the projection onto the set of k-sparse vectors , which is trivially solved by keeping only the k largest coefficients .",0
1656,"This paper proposes an adaptive learning rate for stochastic variational inference (SVI) to address the issue of hand-tuning learning rates for large text corpora. Specifically, the authors focus on latent Dirichlet allocation (LDA) as an application of SVI, where the optimization of the variational objective is typically performed with hand-tuned rates. The proposed adaptive learning rate method uses the natural gradient of the objective function and decreases the learning rate over time as the optimization converges. The authors demonstrate that the proposed method outperforms hand-tuned rates on LDA inference for different sizes of subsamples of text corpora. The proposed method can be applied to other probabilistic models that use SVI for stochastic inference.",1
1657,"stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets . it optimizes the vari-ational objective with stochastic optimization , following noisy estimates of the natural gradient . operationally , stochastic inference iteratively subsamples from the data , analyzes the subsample , and updates parameters with a decreasing learning rate . however , the algorithm is sensitive to that rate , which usually requires hand-tuning to each application . we solve this problem by developing an adaptive learning rate for stochastic inference . our method requires no tuning and is easily implemented with computations already made in the algorithm . we demonstrate our approach with latent dirichlet allocation applied to three large text corpora . inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates .",0
1658,"In this paper, we investigate the performance of Empirical Mode Decomposition (EMD) based on genetic algorithm optimization schemes. EMD is a powerful signal processing technique that decomposes non-stationary signals into a finite set of intrinsic mode functions (IMFs). We propose a genetic algorithm framework to optimize the interpolation points used in the EMD process. By employing piecewise interpolating polynomials, we can better approximate the underlying signal and obtain more accurate IMF components. We evaluate the performance of our approach through a series of experiments and show that our proposed method outperforms existing EMD algorithms. Our investigation demonstrates the effectiveness of the genetic algorithm approach in improving the performance of EMD for various applications.",1
1659,"empirical mode decomposition -lrb- empirical mode decomposition -rrb- has lately received much attention due to the many interesting features that exhibits . however it lacks a strong theoretical basis which would allow a performance analysis and hence the enhancement and optimization of the method in a systematic way . in this paper , an investigation of empirical mode decomposition is attempted in an alternative way : the interpolation points and the piecewise interpolating polynomials for the formation of the upper and lower envelopes of the signal are optimized based on a genetic algorithm framework revealing important characteristics of the method which where previously hidden . as a result , novel directions for both the performance enhancement and the theoretical investigation of the method are unveiling .",0
1660,"This paper proposes a novel approach for learning Bayesian Belief Networks (BBNs) using Artificial Neural Networks (ANNs) as probability estimators. The method involves training ANNs to estimate probabilities of the variables in the BBN, which can be either discrete or continuous. The performance of the learning scheme is analyzed using accuracy as the evaluation metric. The proposed ANN estimators use different architectures and are shown to outperform other state-of-the-art estimators. The results demonstrate the effectiveness of the approach in learning complex models and highlight the potential of using ANNs for BBN learning.",1
1661,"in this paper we propose a method for learning bayesian belief networks from data . the method uses artificial neural networks as probability estimators , thus avoiding the need for making prior assumptions on the nature of the probability distributions governing the relationships among the participating variables . this new method has the potential for being applied to domains containing both discrete and continuous variables arbitrarily distributed . we compare the learning performance of this new method with the performance of the method proposed by cooper and herskovits in -lsb- 7 -rsb- . the experimental results show that , although the learning scheme based on the use of ann estimators is slower , the learning accuracy of the two methods is comparable . category : algorithms and architectures .",0
1662,"This paper presents a study on ranking human and machine summarization systems. Both human and machine evaluation systems were used to assess the performance of various summarization systems. The results were compared using average scores, and the study found that the performance of the machine summarization systems was not as good as that of the human systems. The paper highlights the importance of continuing to develop machine summarization systems to improve their accuracy and effectiveness.",1
1663,the text analysis conference -lrb- tac -rrb- ranks summarization systems by their average score over a collection of document sets . we investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems .,0
1664,"This paper investigates the effectiveness of manifold learning algorithms for speech analysis compared to classical linear dimensionality reduction methods. The high dimensional speech signals are projected into a low dimensional space while preserving the manifold structure. The performance of manifold learning algorithms is compared to classical methods using vowel data and speech apparatus data. Results show that manifold learning algorithms outperform classical methods in capturing the low dimensional structure of the data, particularly for vowels and speech sounds. This suggests that manifold learning algorithms are effective for analyzing speech data.",1
1665,"due to the physiological constraints of articulatory motion the speech apparatus has limited degrees of freedom . as a result , the range of speech sounds a human is capable of producing may lie on a low dimensional submanifold of the high dimensional space of all possible sounds . in this study a number of manifold learning algorithms are applied to speech data in an effort to extract useful low dimensional structure from the high dimensional speech signal . the ability of these manifold learning algorithms to separate vowels in a low dimensional space is evaluated and compared to a classical linear dimensionality reduction method . results indicate that manifold learning algorithms outperform classical methods in low dimensions and are capable of discovering useful manifold structure in speech data .",0
1666,"This paper describes PATHS, a system designed for accessing cultural heritage collections. The system utilizes natural language processing techniques to enable users to navigate and explore European Library and Europeana content. Specifically, PATHS supports navigation in several European languages, including English and Spanish. The system aims to provide a comprehensive and user-friendly interface for accessing and exploring cultural heritage collections.",1
1667,"this paper describes a system for navigating large collections of information about cultural heritage which is applied to eu-ropeana , the european library . euro-peana contains over 20 million artefacts with meta-data in a wide range of euro-pean languages . the system currently provides access to europeana content with meta-data in english and spanish . the paper describes how natural language processing is used to enrich and organise this meta-data to assist navigation through eu-ropeana and shows how this information is used within the system .",0
1668,"This paper proposes a globally optimal method for finding shortest paths in thin, elongated structures with higher-order curve properties, such as curvature and torsion. The method is specifically designed for medical images and multi-view reconstruction problems, where the regularization of curvature and torsion is critical. The proposed method uses line graphs and discretization to solve small-scale problems and is evaluated on medical images for the effectiveness of the regularization. The results show that the method is able to achieve good performance in terms of accuracy and efficiency, making it suitable for various applications in medical imaging and beyond.",1
1669,"this paper describes a method of finding thin , elongated structures in images and volumes . we use shortest paths to minimize very general functionals of higher-order curve properties , such as curvature and torsion . our globally optimal method uses line graphs and its runtime is polynomial in the size of the discretization , often in the order of seconds on a single computer . to our knowledge , we are the first to perform experiments in three dimensions with curvature and torsion regularization . the largest graphs we process have almost one hundred billion arcs . experiments on medical images and in multi-view reconstruction show the significance and practical usefulness of regularization based on curvature while torsion is still only tractable for small-scale problems .",0
1670,"This paper proposes a novel approach for image segmentation using networks of variable states. The system utilizes a neural net architecture to segment complex images that contain two-dimensional geometrical shapes with scale variation under different lighting conditions. The proposed model is implemented using a digital signal processor and software model, and is executed on a processor board or a workstation. The system is tested on video images of railroad cars, and the results demonstrate the effectiveness of the proposed approach for image segmentation in various scenarios. The experimental evaluation shows that the neural net architecture is a crucial component of the system, and that it can be used to segment complex images in conjunction with a digital signal processor on a workstation. Overall, this paper presents a comprehensive solution for image segmentation that can be used in a wide range of applications.",1
1671,"jan ben we developed a neural net architecture for segmenting complex images , i.e. , to localize two-dimensional geometrical shapes in a scene , without prior knowledge of the objects ' positions and sizes . a scale variation is built into the neural net architecture to deal with varying sizes . this neural net architecture has been applied to video images of railroad cars , to find their identification numbers . over 95 % of the characlers were located correctly in a data base of 300 images , despile a large variation in lighting conditions and often a poor quality of the characters . a part of the neural net architecture is executed on a processor board containing an analog neural net chip -lrb- graf et ai . 1991 -rrb- . while the rest is implemented as a software model on a workstation or a digital signal processor .",0
1672,"This paper presents a novel approach for selective partial updating in the normalised constant modulus algorithm (NCMA) to improve the convergence speed and implementation cost of fractionally-spaced equalisation. The proposed approach utilises a block selection criterion to update a subset of the block of equaliser parameters, while maintaining the convergence performance of the NCMA. The system solves a constrained minimisation problem to find the optimal subset of parameters to update. The computational complexity of the proposed method is evaluated and compared with its full-update counterpart, showing a significant reduction in the number of multiplications required to update the equaliser parameters. The proposed method is also compared with the soft criterion satisfaction version of NCMA, demonstrating superior convergence speed and lower computational complexity. The experimental results indicate that the proposed approach is an effective method for selective partial updating of equaliser parameters in NCMA, which can be implemented at a lower cost without compromising the performance of the system.",1
1673,"a reduced complexity realisation for the normalised constant mod-ulus algorithm and its soft criterion satisfaction version is proposed based on selective partial updating . the computational complexity of normalised constant mod-ulus algorithm and soft criterion satisfaction version is reduced by updating a block of equaliser parameters at every iteration rather than the entire equaliser . this results in a smaller number of multiplications for updating the equaliser parameters . a simple block selection criterion is derived from the solution of a constrained minimisation problem that underpins the development of normalised constant mod-ulus algorithm . in fractionally-spaced equalisation , the proposed selective partial updating is shown to be capable of maintaining comparable convergence speed to its full-update counterpart . this implies a significant reduction in implementation cost without necessarily pe-nalising the convergence speed .",0
1674,"This paper proposes an improved consensus-like method for minimum Bayes risk decoding and lattice combination in speech recognition systems. The proposed method combines the consensus algorithm, also known as confusion network decoding, with the rover technique to reduce the word error rate. The system utilises a lattice of alternative outputs generated by the minimum Bayes risk decoding to obtain a set of candidate transcriptions. The proposed method combines the candidate transcriptions using a lattice-based system combination, and applies e-m consensus and lattice rescoring to refine the combination. The confusion network combination and rover techniques are integrated into the proposed method to improve the performance of the system. The experimental results show that the proposed method outperforms the baseline method in terms of word error rate reduction. The proposed method achieves state-of-the-art performance on several benchmark datasets, demonstrating its effectiveness in practical speech recognition applications. Overall, this paper presents an improved consensus-like method that combines several techniques to enhance the accuracy of lattice-based system combination and minimum Bayes risk decoding.",1
1675,"in this paper we describe a method for minimum bayes risk decoding for speech recognition . this is a technique similar to consensus a.k.a. confusion network decoding , in which we attempt to find the hypothesis that minimizes the bayes ' risk with respect to the word error rate , based on a lattice of alternative outputs . our method is an e-m like technique which makes approximations which we believe are less severe than the approximations made in consensus , and our experimental results show an improvement in e-m both for lattice rescoring and lattice-based system combination , versus baselines such as consensus , confusion network combination and rover .",0
1676,"This paper presents an analysis of speech under physical task stress and its perception by listeners. The study investigates the effect of physical task stress on speech content, speech features, and acoustic correlates such as fundamental frequency and utterance duration. The speech data is collected from subjects who perform a physical task under controlled laboratory conditions. The glottal waveform features are also extracted from the speech signal to provide additional information on the effect of physical stress on speech production. The speech data is analysed using statistical methods to identify the differences between speech produced under physical task stress and normal conditions. Listener tests are conducted to evaluate the perception of speech under physical stress. The results show that physical task stress affects speech production, leading to changes in fundamental frequency, utterance duration, and glottal waveform features. The listener tests demonstrate that the perception of speech under physical stress is different from that of speech produced under normal conditions. The study provides insights into the effect of physical stress on speech production and its perception, which can be useful in designing speech systems for individuals who experience physical stress in their daily lives.",1
1677,"it is known that speech under physical task stress degrades speech system performance . therefore , an analysis of speech under physical task stress is performed across several parameters to identify acoustic correlates . formal listener tests are also performed to determine the relationship between acoustic correlates and perception . to verify the statistical significance of all results , student-t statistical tests are applied . it was found that fundamental frequency decreases for many speakers , that utterance duration increases for some speakers and decreases for others , and that the glottal waveform is quantifiably different for many speakers . perturbation of two speech features , fundamental frequency and the glottal waveform , is applied in listener tests to quantify the degree to which these features convey physical stress content in speech . finally , the enhanced understanding of physical task stress speech provided here is discussed in the context of speech system .",0
1678,"This paper presents biologically inspired defenses against computer viruses, which are based on the concept of a computer immune system. The proposed system utilizes intelligent agents to detect and respond to computer viruses in a manner similar to how the human immune system detects and responds to pathogens. The system is designed to be adaptive and capable of learning from past experiences. A neural network virus detector is used to classify infected and un-infected programs, which is a critical component of the system. The proposed biologically inspired anti-virus techniques aim to improve the effectiveness of anti-virus technology by emulating the natural defense mechanisms of the human immune system. The paper also discusses the limitations of traditional anti-virus technology and how the proposed system can overcome these limitations. The experimental results demonstrate that the proposed system can effectively detect and respond to computer viruses. Overall, this paper presents a novel approach to computer virus defense, which is inspired by biological systems and offers an alternative to traditional anti-virus technology.",1
1679,"today 's anti-virus technology , based largely on analysis of existing viruses by human experts , is just barely able to keep pace with the more than three new computer viruses that are written daily . in a few years , intelligent agents navigating through highly connected networks are likely to form an extremely fertile medium for a new breed of viruses . at ibm , we are developing novel , biologically inspired anti-virus techniques designed to thwart both today 's and tomorrow 's viruses . here we describe two of these : a neural network virus detector that learns to discriminate between infected and un-infected programs , and a computer immune system that identifies new viruses , analyzes them automatically , and uses the results of its analysis to detect and remove all copies of the virus that are present in the computer immune system . the biologically inspired anti-virus techniques has been incorporated into ibm 's commercial anti-virus product ; the computer immune system is in prototype .",0
1680,"This paper presents a method for optical flow computation using mixture models. Optical flow computation is the process of estimating the motion of objects in a scene based on changes in their position over time. The proposed method models multiple motions in the scene using a mixture of probability distributions. The computation of optical flow values is based on component velocity measurements, which are estimated using a maximum likelihood estimate. The image patch merging process is used to improve the accuracy of the optical flow computation. The method also takes into account occlusion boundaries and transparency in the scene, which can be challenging to handle using traditional optical flow computation methods. The proposed method uses the EM algorithm to estimate the motion parameters and handle outliers in the data. Experimental results demonstrate that the proposed method outperforms traditional optical flow computation methods in terms of accuracy and robustness to occlusion and transparency. Overall, this paper presents a novel approach to optical flow computation using mixture models that can handle challenging scenarios in computer vision.",1
1681,"the computation of optical ow relies on merging information available over an image patch to form an estimate of 2d image velocity a t a p o i n t . this merging process raises a host of issues , which include the treatment of outliers in component v elocity measurements and the modeling of multiple motions within a patch which arise from occlusion boundaries or transparency . w e present a new approach which allows us to deal with these issues within a common framework . our approach is based on the use of a probabilistic mixture m o del to explicitly represent m ultiple motions within a patch . we use a simple extension of the em-algorithm to compute a maximum likelihood estimate for the various motion parameters . preliminary experiments indicate that this approach is computationally eecient and can provide robust estimates of the optical ow v alues in the presence of outliers and multiple motions . the basic approach can also be applied to other problems in computational vision , such as the computation of 3d relative motion , which require the integration of several partial constraints to obtain a desired quantity .",0
1682,"This paper presents UMRAO, a chess endgame tutor based on a knowledge-based approach. UMRAO uses strategy graphs to represent the knowledge of bishop-pawn endgames, and employs a minimax search algorithm to choose moves. The system provides real-time tutoring for chess players by presenting strategic concepts and moves to the user. The prototype chess tutor is designed to solve the endgame problem using a knowledge-based approach. The paper describes the development of UMRAO and its effectiveness in improving players' endgame skills. The results of tests conducted on UMRAO show that it significantly improves players' performance in bishop-pawn endgames, demonstrating the usefulness of the knowledge-based chess tutoring system.",1
1683,"most research in computer chess has focussed on creating an excellent chess player , with relatively little concern given to modelling how humans play chess player . the research reported in this paper is aimed at investigating knowledge-based chess in the context of building a prototype chess tutor , umrao , which helps students learn how to play bishop-pawn endgames . in tutoring it is essential to take a knowledge-based approach , since students must learn how to manipulate strategic concepts , not how to carry out minimax search . umrao uses an extension of michic 's advice language to represent expert and novice chess player plans . for any given endgame the umrao is able to compile the plans into a strategy graph , which elaborates strategies -lrb- both well-formed and ill-formed -rrb- that students might use as strategy graphs solve the endgame problem . strategy graphs can be compiled `` off-line '' so that strategy graphs can be used in real time tutoring . we show that the normally rigid `` model tracing '' tutoring paradigm can be used in a flexible way in this domain .",0
1684,"This paper proposes a new method for time-frequency analysis, based on regionally optimized kernels for bilinear time-frequency distributions. The method utilizes a finite mixture model of time-varying Gaussian distributions to construct a set of time-frequency kernels that are regionally optimized for closely-spaced components in the signal. The proposed algorithm uses a combination of functional merging and the expectation-maximization algorithm to minimize the computational burden and expense of computing the time-frequency plane. Experimental results demonstrate the effectiveness of the proposed method in reducing the interference between closely spaced components in the time-frequency plane, leading to improved signal analysis and interpretation.",1
1685,"ideally , kernels used to generate bilinear time-frequency distributions should be signal-dependent , and optimised independently at every location in the time-frequency plane . this poses an extremely severe computational burden . a compromise is proposed in this paper : time-varying kernels are optimised for specific time-varying kernels in the time-frequency plane . the time-varying kernels , designed to isolate separate components comprising the signal , are determined by modelling the bilinear time-frequency distributions using a finite mixture model of gaussian distributions . the parameters of the model are estimated using a combination of the expectation-maximisation algorithm and functional merging . the regional optimisation provides improved separation and resolution of closely-spaced components when compared to methods using a solely time-varying kernel , without incurring an overwhelming computational expense .",0
1686,"This paper presents a multi-body factorization method for motion analysis. The proposed method utilizes a mathematical construct of object shapes and a shape interaction matrix to analyze object motions in a given scene. The method involves selecting appropriate coordinate systems for each object and segmenting features from the input image. By representing the shape interaction matrix in a canonical form, the method factorizes the interaction between multiple objects into individual components, which can be used to estimate the motion of each object. The proposed method addresses the structure-from-motion problem and overcomes the limitations of traditional approaches by allowing for the inclusion of shape information in the analysis. Experimental results show the effectiveness of the proposed method in accurately estimating the motion of multiple objects from image features.",1
1687,"the structure-from-motion problem has been extensively studied in the field of computer vision . yet , the bulk of the existing work assumes that the scene contains only a single moving object . the more realistic case where an unknown number of objects move in the scene has received little attention , especially for its theoretical treatment . in this paper we present a new method for separating and recovering the motion and shape of multiple independently moving objects in a sequence of images . the method does not require prior knowledge of the number of objects , nor is dependent on any grouping of features into an object at the image level . for this purpose , we introduce a mathematical construct of object shapes , called the shape interaction matrix , which is invariant to both the object motions and the selection of coordinate systems . this shape interaction matrix is computable solely from the observed trajectories of image features without grouping them into individual objects . once the shape interaction matrix is computed , shape interaction matrix allows for segmenting features into objects by the process of transforming shape interaction matrix into a canonical form , as well as recovering the shape and motion of each object .",0
1688,"This paper proposes a bit-rate scalable local feature descriptor called PQ-WGLOH for image matching and retrieval tasks in mobile visual search applications. The descriptor is designed to be compact yet discriminative, allowing for efficient wireless query transmission and low complexity image matching. The proposed descriptor is based on log-polar location grid table lookup operations and product quantization, and uses 128-byte SIFT and CHOg descriptors. The MPEG compact descriptor is also used for visual search standardization. Experimental results show that PQ-WGLOH outperforms other state-of-the-art descriptors in terms of matching accuracy and retrieval efficiency.",1
1689,"in this paper , we propose a compact yet discriminative local descrip-tor which tackles the wireless query transmission latency in mobile visual search . the compact yet discriminative local descrip-tor captures gradient statistics of canoni-cal patches over a log-polar location grid whose parameters are optimized using training samples . we quantize the resulting compact yet discriminative local descrip-tor using product quantization . the compact yet discriminative local descrip-tor achieves about 95 % bits reduction compared with 128-byte sift and allows adaptation of descriptor lengths to support user required performance . moreover , accurate matching of descriptors with low complexity is allowed within several table lookup operations . we perform a comprehensive comparison with sift , chog and chog in the context of image retrieval , image matching and object localization . we achieve competing matching and retrieval performance with sift , chog with much fewer bits . in particular , the compact yet discriminative local descrip-tor outperforms chog at the same bits on eight data sets contributed to mpeg compact de-scriptor for visual search standardization .",0
1690,"This paper proposes a new method called Doubly Stochastic Variational Bayes for solving non-conjugate Bayesian inference problems. The method is based on variational inference algorithm and stochastic optimization techniques. It is designed to work in continuous parameter spaces and handle variable selection in the presence of non-conjugate priors. The proposed algorithm uses gradient information and stochastic approximation to optimize the joint density model. Illustrative examples are provided to show the effectiveness of the proposed method, including kernel hyperparameters and logistic regression. The method is particularly useful for solving problems such as Gaussian process regression where the priors are non-conjugate.",1
1691,we propose a simple and effective variational inference algorithm based on stochastic optimi-sation that can be widely applied for bayesian non-conjugate inference in continuous parameter spaces . this variational inference algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density . we demonstrate these properties using illustrative examples as well as in challenging and diverse bayesian inference problems such as variable selection in logistic regression and fully bayesian inference over kernel hyperparameters in gaussian process regression .,0
1692,"This paper proposes a new parsing method called ""Top-Down Nearly-Context-Sensitive Parsing"" which improves upon existing top-down parsers by incorporating nearly-context-sensitive constraints. The method uses dynamic programming algorithms to generate parse hypotheses and fully-connected parse trees, allowing for cognitive fact evaluation and error reduction. The paper evaluates the new method using statistical parsers and demonstrates its effectiveness in improving parse accuracy. The proposed method can be used in a variety of applications, including syntactic parsing and error reduction in parsers.",1
1693,"we present a new syntactic parser that works left-to-right and top down , thus maintaining a fully-connected parse tree for a few alternative parse hypotheses . all of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence . thus they only find a complete fully connected parse at the very end . in contrast , both subjective and experimental evidence show that people understand a sentence word-to-word as they go along , or close to it . the constraint that the syntactic parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact . our syntactic parser achieves a new best result for top-down parsers of 89.4 % , a 20 % error reduction over the previous single-parser best result for parsers of this type of 86.8 % -lrb- roark , 2001 -rrb- . the improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming .",0
1694,"This paper proposes a novel approach for semantic parsing using paraphrasing. The goal of semantic parsing is to convert natural language expressions into machine-readable logical forms. The proposed method uses a candidate generation model to produce a set of possible logical forms, and then applies a paraphrasing model to generate new candidate logical forms that are semantically equivalent to the original set. The paraphrasing model is based on a vector space association model that maps raw text to logical forms, and is trained on question-answer pairs and a knowledge base. The resulting set of candidate logical forms are ranked by a canonical realization model that selects the most likely logical form. The proposed method is evaluated on question-answering datasets and achieves state-of-the-art accuracies compared to existing semantic parsing methods.",1
1695,"a central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed . traditionally , semantic parsers are trained primarily from text paired with knowledge base information . our goal is to exploit the much larger amounts of raw text not tied to any knowledge base . in this paper , we turn semantic parsing on its head . given an input utterance , we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each . then , we use a paraphrase model to choose the realization that best paraphrases the input , and output the corresponding logical form . we present two simple paraphrase model , an association model and a vector space model , and train paraphrase model jointly from question-answer pairs . our system parasempre improves state-of-the-art accuracies on two recently released question-answering datasets .",0
1696,"This paper investigates the effects of lexical resource quality on preference violation detection. The authors focus on detecting selectional preference violations using lexical resources such as WordNet and VerbNet. They use annotated corpora to measure the F1-measure of their preference violation detector and analyze parser outputs to evaluate the algorithmic improvements. The study demonstrates that the quality of lexical resources has a significant impact on the performance of preference violation detection in natural language processing tasks, such as treebanks and verb classification. The results highlight the need for high-quality lexical resources in NLP tasks and motivate further research in this area.",1
1697,"lexical resources such as wordnet and verbnet are widely used in a multitude of nlp tasks , as are annotated corpora such as treebanks . often , the resources are used as-is , without question or examination . this practice risks missing significant performance gains and even entire techniques . this paper addresses the importance of resource quality through the lens of a challenging nlp task : detecting selec-tional preference violations . we present david , a simple , lexical resource-based preference violation detector . with as-is lexical resources , david achieves an f 1-measure of just 28.27 % . when the resource entries and parser outputs for a small sample are corrected , however , the f 1-measure on that sample jumps from 40 % to 61.54 % , and performance on other examples rises , suggesting that the algorithm becomes practical given refined resources . more broadly , this paper shows that resource quality matters tremendously , sometimes even more than algorithmic improvements .",0
1698,"This paper presents a method for detecting vowel onset points (VOPs) in continuous speech using autoassociative neural network (AANN) models. The goal is to recognize consonant-vowel (CV) units and to spot subword units. The AANN models are trained on a corpus of CV utterances, and the detection of VOPs is achieved by identifying the minimum point of error in the reconstruction of the speech signal. The method is evaluated on several datasets, and the results demonstrate the effectiveness of AANN models for the detection of VOPs in continuous speech. The proposed method has potential applications in speech recognition and other natural language processing tasks.",1
1699,"detection of vowel onset points -lrb- detection of vowel onset points -rrb- is important for spotting subword units in continuous speech . for consonant-vowel utterances , vop is the instant at which the consonant part ends and the vowel part begins . accurate detection of detection of vowel onset points is important for recognition of cv units in continuous speech . in this paper , we propose an approach for detection of detection of vowel onset points using autoasso-ciative neural network models . a pair of aann models are trained for each cv class to capture the characteristics of speech signal in the consonant and vowel regions of that class . the trained aann models are then used to detect detection of vowel onset points in continuous speech . the results of studies show that the proposed approach leads to significantly less number of spurious hypotheses .",0
1700,"This paper presents a well-founded semantics for basic logic programs that include arbitrary abstract constraint atoms. The paper introduces the concept of aggregate logic programs, which allows the specification of polynomial time rules that use aggregate functions. The well-founded semantics ensures that the evaluation of programs with arbitrary abstract constraint atoms is grounded in a stable set of facts. The paper also discusses the relationship between answer set semantics and the proposed well-founded semantics. The approach is compared to other approaches, such as description logic programs, and its advantages are discussed. The paper concludes that the proposed semantics provides a solid foundation for reasoning with abstract constraint atoms in logic programs.",1
1701,"logic programs with abstract constraint atoms proposed by marek and truszczynski are very general description logic programs . they are general enough to capture aggregate logic programs as well as recently proposed description logic programs . in this paper , we propose a well-founded semantics for basic description logic programs with arbitrary abstract constraint atoms , which are sets of rules whose heads have exactly one atom . we show that similar to the well-founded semantics of normal description logic programs , it has many desirable properties such as that it can be computed in polynomial time , and is always correct with respect to the answer set semantics . this paves the way for using our well-founded semantics to simplify these description logic programs . we also show how our semantics can be applied to aggregate logic programs and description logic programs , and compare it to the well-founded semantics already proposed for these description logic programs .",0
1702,This paper proposes an entirely data-driven approach for hand pose estimation using a feedback loop. The method involves using a convolutional neural network to estimate the 3D pose of the hand from depth images. A feedback loop is trained using the estimated pose and the 3D model of the hand to refine the pose estimation. The input data is used to train deep networks and the training is performed on a GPU. The proposed method achieves high accuracy and demonstrates the effectiveness of the feedback loop for improving the hand pose estimation.,1
1703,"we propose an entirely data-driven approach to estimating the 3d pose of a hand given a depth image . we show that we can correct the mistakes made by a convolutional neural network trained to predict an estimate of the 3d pose by using a feedback loop . the components of this feedback loop are also deep networks , optimized using training data . they remove the need for fitting a 3d model to the input data , which requires both a carefully designed fitting function and algorithm . we show that our entirely data-driven approach outperforms state-of-the-art methods , and is efficient as our implementation runs at over 400 fps on a single gpu .",0
1704,"This paper proposes a cluster-based approach for optimizing color spaces for conversion of multispectral and multiprimary data. The goal is to address color transformation problems such as gamut mapping, natural mapping, and information loss, particularly for color deficient viewers. The approach uses tristimulus colors and image optimization techniques to achieve bijective color transformations. The proposed method is compared to traditional techniques and evaluated based on its effectiveness for printing, grayscale conversion, and metamerism. Results show that the cluster-based approach is superior in terms of natural mapping and information preservation.",1
1705,"transformations between different color spaces and gamuts are ubiquitous operations performed on images . often , these transformations involve information loss , for example when natural '' mapping from color to grayscale for printing , from multispectral or multiprimary data to tristimulus spaces , or from one color gamut to another . in all these applications , there exists a straightforward '' natural '' mapping from the source space to the target space , but the natural '' mapping is not bijective , resulting in information loss due to metamerism and similar effects . we propose a cluster-based approach for optimizing the transformation for individual images in a way that preserves as much of the information as possible from the source space while staying as faithful as possible to the natural mapping . our cluster-based approach can be applied to a host of color transformation problems including color to gray , gamut mapping , conversion of multispectral and multipri-mary data to tristimulus colors , and image optimization for color deficient viewers .",0
1706,"This paper presents an evaluation of a singing voice conversion method based on many-to-many eigenvoice conversion. The goal of the method is to convert the voice timbre of a singing voice, using nonparallel singing voice data. The singing-to-singing synthesis system is evaluated using parallel data sets and compared to a speaking voice conversion method. The probabilistic model is trained using many-to-many eigenvoice conversion and the input data includes voice timbre. The results show that the singing voice conversion method performs well and is able to produce high-quality singing voice conversion. The method has potential for use in various applications such as music production and voice modification.",1
1707,"in this paper , we evaluate our proposed singing voice conversion method from various perspectives . to enable singers to freely control their voice timbre of singing voice , we have proposed a singing voice conversion method based on many-to-many eigenvoice conversion that enables to convert the voice timbre of an arbitrary source singer into that of another arbitrary target singer using a probabilistic model . furthermore , to easily develop training data consisting of multiple parallel data sets between a single reference singer and many other singers , a technique for efficiently and effectively generating the parallel data sets from nonparallel singing voice data sets of many singers using a singing-to-singing synthesis system have been proposed . however , we have never conducted sufficient investigations into the effectiveness of these proposed singing voice conversion method . in this paper , we conduct both objective and subjective evaluations to carefully investigate the effectiveness of proposed singing voice conversion method . moreover , the differences between singing voice conversion and speaking voice conversion are also analyzed . experimental results show that our proposed singing voice conversion method succeeds in enabling people to control their own voice timbre by using only an extremely small amount of the target singing voice .",0
1708,"This paper explores the feasibility of source separation in the frequency domain. It investigates the use of the L-point discrete Fourier transform and non-linear filtering to identify signals in the spectral kurtosis domain. The paper also examines the efficiency and convergence speed of the algorithm in source separation, as well as the reconstruction of the signals. The feasibility of the approach is tested using QARMA processes and the Gaussianity of signals in the L spectra. The results suggest that source separation in the frequency domain is a viable method for identifying and separating signals.",1
1709,"we focus on the feasibility of the source separation in the frequency domain . first , it is linked with the convergence speed towards gaussianity of signals after l-point discrete fourier transform . we test here a distance to gaussianity thanks to the spectral kurtosis . we analyse the influence of l , of the duration of the source tricorrelations and of a non linear filtering . we mainly develop the case of qarma processes . the second point consists in the reconstruction of the spectra of the estimated sources from the signals identified at each frequency bin . indeed , the source associated to the ith identified signal is not necessarily the same from one frequency bin to another . the algorithm efficiency is then illustrated on qarma processes , including the procedures of separation and reconstruction .",0
1710,"This paper presents a novel approach for 3D rotation estimation using discrete spherical harmonic oscillator transforms. The proposed method utilizes spherical harmonics related algorithms and angle estimation algorithms to accurately estimate the rotation of a 3D object. The rotated signal is represented using discrete shots and processed on Cartesian grids using Wigner-D matrix. The precision, noise tolerance, accuracy, and robustness of the proposed method are evaluated and compared to other angle estimation algorithms. The experimental results show that the proposed method outperforms existing techniques in terms of accuracy and robustness, making it a promising approach for 3D rotation estimation in various applications.",1
1711,"this paper presents an approach to 3d rotation estimation using discrete spherical harmonic oscillator transforms -lrb- discrete shots -rrb- . discrete shots not only have simple and fast implementation methods but also are compatible with the existing angle estimation algorithms related to spherical harmonics . discrete shots of the rotated signal follow the same formulation to the wigner-d matrix as spherical harmonics transforms . thus , the spherical harmonics related algorithms could be utilized to discrete shots without modification . furthermore , compared to some existing methods , our approach with discrete shots exhibits higher accuracy , higher precision and improved robustness to noise if the input signal is sampled uniformly on cartesian grids . the phenomenon results from no interpolations in discrete shots .",0
1712,"This paper examines the consistency of the ℓ1-regularized maximum-likelihood estimator for compressive Poisson regression in the high-dimensional setting. The estimator is evaluated in both the n p and compressive sensing settings, and its consistency is analyzed using variable selection and sample complexity. The canonical link function is utilized for Poisson regression analysis of count data, particularly in the field of electrical engineering, such as transmission tomography estimation. The experimental results show that the proposed estimator achieves consistency in the high-dimensional setting, making it a promising approach for compressive Poisson regression with count data.",1
1713,"we consider poisson regression with the canonical link function . this poisson regression is widely used in regression analysis involving count data ; one important application in electrical engineering is transmission tomography . in this paper , we establish the variable selection consistency and estimation consistency of the 1-regularized maximum-likelihood esti-mator in this poisson regression , and characterize the asymp-totic sample complexity that ensures consistency even under the compressive sensing setting -lrb- or the n p setting in high-dimensional statistics -rrb- .",0
1714,"This paper proposes a new approach called ""skewing"" as an efficient alternative to lookahead for greedy decision tree induction. The traditional lookahead approach incurs a constant run-time penalty and increases the time complexity of greedy decision tree learners. Skewing is an alternative approach that aims to address these limitations by utilizing the concept of parity functions to identify problematic functions and apply a specific skewing operation to the data. The experimental results show that the proposed skewing approach achieves similar accuracy to lookahead while significantly reducing the time complexity and run-time penalty, making it a promising alternative for decision tree induction in various applications. Additionally, the study discusses the relationship between parity functions and problematic functions in the context of decision tree induction algorithms.",1
1715,"this paper presents a novel , promising approach that allows greedy decision tree induction algorithms to handle problematic functions such as parity functions . lookahead is the standard approach to addressing difficult functions for greedy decision tree learners . nevertheless , this approach is limited to very small problematic functions or subfunc-tions -lrb- 2 or 3 variables -rrb- , because the time complexity grows more than exponentially with the depth of lookahead . in contrast , the approach presented in this paper carries only a constant run-time penalty . experiments indicate that the approach is effective with only modest amounts of data for problematic functions or subfunctions of up to six or seven variables , where the examples themselves may contain numerous other -lrb- irrelevant -rrb- variables as well .",0
1716,"This paper proposes an efficient piecewise training approach for deep structured models to improve semantic segmentation accuracy. The proposed approach uses deep convolutional neural networks (CNNs) with multi-scale image input and sliding pyramid pooling to capture both patch-background context and patch-patch context. The CNN-based pairwise potential functions are used to improve semantic correlations, and the contextual information is captured using conditional random fields (CRF) inference. The study evaluates the proposed approach on two benchmark semantic segmentation datasets, Pascal VOC 2012 and NYUDv2, and compares it to existing methods, such as SIFT-Flow and Pascal-Context. The experimental results show that the proposed approach achieves state-of-the-art performance in terms of intersection-over-union score and outperforms existing methods. The study also discusses the impact of network design and back propagation on the performance of",1
1717,"recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neu-ral networks for the task . we show how to improve semantic segmentation through the use of contextual information . specifically , we explore ` patch-patch ' context and ` patch-background ' context with deep cnns . for learning the patch-patch context between image regions , we formulate conditional random fields with cnn-based pairwise potential functions to capture semantic correlations between neighboring patches . efficient piecewise training of the proposed deep convolutional neu-ral networks is then applied to avoid repeated expensive crf inference for back propagation . in order to capture the patch-background context , we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance . our experiment results set new state-of-the-art performance on a number of popular semantic segmentation datasets , including nyudv2 , pascal voc 2012 , pascal-context , and sift-flow . particularly , we achieve an intersection-over-union score of 77.8 on the challenging pascal voc 2012 dataset .",0
1718,"This paper presents a novel method for decoding auditory attention using electroencephalography (EEG) recordings and noisy acoustic reference signals. In a cocktail-party scenario, where multiple speakers are present, the proposed approach employs a spatio-temporal filter design with regularization parameters to extract the target speech signal from the noisy acoustic reference signals. The clean speech signals are then used to decode auditory attention. The study evaluates the decoding accuracy using a least-squares method for different noise types. The experimental results show that the proposed method outperforms existing methods in decoding auditory attention. The study also analyzes the effect of spatio-temporal filter and regularization parameters on decoding accuracy. The findings suggest that the proposed method can be used for real-time decoding of auditory attention in noisy environments.",1
1719,"to decode auditory attention from electroencephalography recordings in a cocktail-party scenario with two competing speakers a least-squares method has recently been proposed , showing a promising decoding accuracy . this method however requires the clean speech signals of both the attended and the unattended speaker to be available as reference signals , which is difficult to achieve from the noisy recorded microphone signals in practice . in addition , optimizing the parameters involved in the spatio-temporal filter design is of crucial importance in order to reach the largest possible decoding performance . in this paper , the influence of noisy acoustic reference signals and the spatio-temporal filter and regularization parameters on the decoding performance is investigated . the results show that to some extent the decoding performance is robust to noisy acoustic reference signals , depending on the noise type . furthermore , we demonstrate the crucial influence of several parameters on the decoding performance , especially when the acoustic reference signals used for decoding have been corrupted by noise .",0
1720,"This paper presents an expert lexicon approach to identifying English phrasal verbs, which is a challenging problem in natural language processing (NLP). The proposed method utilizes a finite state approach to tackle the phrasal verb identification problem, and incorporates a lexicon module to handle the complex morpho-syntactic interaction between phrasal verbs and their constituents. The study also employs shallow parsing and deep parsing techniques to extract relevant features from phrasal verbs and their contexts. The proposed method is evaluated on a dataset of English phrasal verbs and outperforms existing approaches in terms of accuracy and efficiency. The study also highlights the importance of the lexicon module and the role of morpho-syntactic interaction in identifying phrasal verbs. Furthermore, the paper discusses the relevance of the proposed approach to NLP frameworks and English parsers, and the potential for future research in this area. Overall, the expert lexicon approach shows promise for improving the identification of phrasal verbs in the English language.",1
1721,"phrasal verbs are an important feature of the english language . properly identifying phrasal verbs provides the basis for an english parser to decode the related structures . phrasal verbs have been a challenge to natural language processing because phrasal verbs sit at the borderline between lexicon and syntax . traditional nlp frameworks that separate the lexicon module from the english parser make it difficult to handle this problem properly . this paper presents a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho-syntactic interaction . with precision/recall combined performance benchmarked consistently at 95.8 % -97.5 % , the phrasal verb identification problem has basically been solved with the presented finite state approach .",0
1722,"This paper proposes a method for answering definition questions using temporally-anchored text snippets. The method utilizes a lightweight extraction approach to identify relevant text snippets from the web and leverages lexical resources and processing modules to generate answers. The proposed method is evaluated on a set of test question sets, and the results demonstrate the effectiveness of using temporally-anchored text snippets in answering definition questions. The study highlights the importance of the web as a source of information and the utility of lightweight extraction methods in identifying relevant text snippets. The paper also discusses the potential for future research in this area, particularly in improving the accuracy and efficiency of the proposed method. Overall, the proposed approach provides a promising solution for answering definition questions using temporally-anchored text snippets, and shows potential for practical applications in various domains.",1
1723,"a lightweight extraction method derives text snippets associated to dates from the web . the snippets are organized dynamically into answers to definition questions . experiments on standard test question sets show that temporally-anchored text snippets allow for efficiently answering definition questions at accuracy levels comparable to the best systems , without any need for complex lexical resources , or specialized processing modules dedicated to finding definitions .",0
1724,"This paper presents a method for detecting system-directed utterances in spoken dialogue systems using dialogue-level features. The proposed approach utilizes a feature set that includes utterance length, utterance timing, and dialogue status to classify user utterances as system-directed or not. The study evaluates the proposed method on a dataset of audio inputs and achieves high classification accuracy, demonstrating the effectiveness of the proposed feature set in detecting system-directed utterances. The paper also discusses the importance of feature selection and the potential for future research in this area. Logistic regression is employed to classify the user utterances using the selected features. The proposed method also highlights the utility of transcription for analyzing spoken dialogue and extracting relevant features. Overall, the proposed method provides a promising solution for detecting system-directed utterances in spoken dialogue systems, and has potential for practical applications in various domains.",1
1725,"we have developed a method to determine whether a user utterance is directed at the system or not . a spoken dialogue system should not respond to audio inputs that are not directed at it -lrb- i.e. , a user 's mutter -rrb- , and it therefore needs to detect such inputs to avoid unsuitable responses . we classify the two cases by logistic regression based on a feature set including utterance timing , utterance length , and dialogue status . we conducted experiments using 5395 user utterances for both transcription and automatic speech recognition results . results showed that the classification accuracy improved by 11.0 and 4.1 points , respectively . we also discuss which features are effective in the classification .",0
1726,"This paper proposes a multi-image focus of attention mechanism for rapid site model construction. The proposed approach utilizes a structural salience measure to identify the salient regions in aerial image scenarios and construct a virtual, horizontal plane for the structured background clutter. The method then employs a space-sweep stereo method to back-project the gradient orientations of the scene locations and construct a volume of space. The study evaluates the proposed approach on a dataset of raised objects, features, buildings, and edges, and demonstrates the effectiveness of the proposed structural salience measure in identifying salient features. The paper also discusses the potential for future research in this area, particularly in improving the accuracy and efficiency of the proposed method. Overall, the proposed approach provides a promising solution for rapid site model construction using a multi-image focus of attention mechanism and has potential for practical applications in various domains, such as urban planning, disaster response, and reconnaissance.",1
1727,"a multi-image focus of attention mechanism has been developed that can quickly distinguish raised objects like buildings from structured background clutter typical to many aerial image scenarios . the underlying approach is the space-sweep stereo method , in which features from multiple images are backprojected onto a virtual , horizontal plane that is methodically swept through the scene . back-projected gradient orientations from multiple images are highly correlated when back-projected gradient orientations come from scene locations containing structural edges that are roughly horizontal , like building roofs and terrain ; otherwise , back-projected gradient orientations tend to be uniformly distributed . these observations are used to define a structural salience measure that can determine whether a given volume of space contains a statistically significant number of structural edges , without first performing precise reconstruction of those edges . the utility of structural salience for computing focus of attention regions is illustrated on sample data from ft.hood , texas .",0
1728,"This paper presents a responsive information architect that enables context-sensitive information seeking by users. The proposed approach aims to develop a full-fledged, context-sensitive information system that supports context-sensitive multimodal input interpretation and automated multimedia output generation. The system adopts an innovative context-sensitive interaction paradigm that leverages intelligent user interaction technologies to facilitate user data requests and enable context-sensitive information retrieval. The study evaluates the proposed approach using a dataset of context-sensitive information scenarios and demonstrates the effectiveness of the proposed approach in enabling responsive and efficient information seeking. The paper also discusses the potential for future research in this area, particularly in improving the accuracy and efficiency of the proposed method, as well as extending the scope of the system to support more complex information-seeking tasks. Overall, the proposed responsive information architect provides a promising solution for enabling context-sensitive information seeking and has potential for practical applications in various domains, such as e-commerce, healthcare, and education.",1
1729,"information seeking is an important but often difficult task especially when involving large and complex data sets . we hypothesize that a context-sensitive interaction paradigm can greatly assist users in their information seeking . such a context-sensitive interaction paradigm allows a system to both understand user data requests and present the requested information in context . driven by this hypothesis , we have developed a suite of intelligent user interaction technologies and integrated intelligent user interaction technologies in a full-fledged , context-sensitive information system . in this paper , we review two sets of key technologies : context-sensitive multimodal input interpretation and automated multimedia output generation . we also share our evaluation results , which indicate that our context-sensitive interaction paradigm are capable of supporting context-sensitive information seeking for practical applications .",0
1730,"In this paper, we propose a method for automatically extracting social networks from literary text. Our approach is applied to the novel Alice in Wonderland, and we use an un-weighted gold network and network measures to evaluate the performance of our method. We detect social events using news corpus and employ support vector machines and tree kernels for social network extraction. We evaluate the quality of the extracted social networks using f-measure, achieving good results with our approach. Our work provides a valuable tool for analyzing social networks in literary text and can contribute to the study of social dynamics in literature.",1
1731,"in this paper we present results for two tasks : social event detection and social network extraction from a literary text , al-ice in wonderland . for the first task , our system trained on a news corpus using tree kernels and support vector machines beats the baseline systems by a statistically significant margin . using this system we extract a social network from al-ice in wonderland . we show that while we achieve an f-measure of about 61 % on social event detection , our extracted un-weighted network is not statistically dis-tinguishable from the un-weighted gold network according to popularly used network measures .",0
1732,This paper presents a method for finding glass in real images using visual cues. The authors propose a framework based on consistent support regions and glass edges to detect transparent objects in the presence of complex backgrounds. They develop classifiers that use the background texture and glass surfaces to distinguish between glass and non-glass regions. The proposed method achieves high accuracy in detecting glass in a range of real-world scenarios. The results show that visual cues can be effectively used for finding glass and other transparent objects in challenging image environments.,1
1733,"this paper addresses the problem of finding glass objects in images . visual cues obtained by combining the systematic distortions in background texture occurring at the boundaries of transparent objects with the strong highlights typical of glass surfaces are used to train a hierarchy of classifiers , identify glass edges , and find consistent support regions for these edges . qualitative and quantitative experiments involving a number of different classifiers and real images are presented .",0
1734,"This paper proposes a PAC-Bayes learning algorithm for the classification of gene-expression data. The algorithm is designed to handle datasets consisting of DNA micro-array data sets with single real-valued attributes. The proposed approach is based on the use of soft greedy '' learning algorithm that is able to learn conjunctions of features. The algorithm also utilizes a PAC-Bayes risk bound to minimize the generalization error of the classifiers. The experimental evaluation of the proposed algorithm is conducted on a number of real-world gene-expression data sets, and the results demonstrate its effectiveness in improving classification accuracy compared to other state-of-the-art methods.",1
1735,"we propose a '' soft greedy '' learning algorithm for building small conjunctions of simple threshold functions , called rays , defined on single real-valued attributes . we also propose a pac-bayes risk bound which is minimized for classifiers achieving a non-trivial tradeoff between sparsity -lrb- the number of rays used -rrb- and the magnitude of the separating margin of each ray . finally , we test the soft greedy '' learning algorithm on four dna micro-array data sets .",0
1736,"This paper explores the equivalence between two techniques in frequency domain signal processing: blind source separation (BSS) and adaptive null beamforming (ANB). Specifically, the paper investigates the relationship between frequency domain BSS and frequency domain ANB in the context of microphone array processing. The study finds that the filter coefficients used in BSS can be viewed as a part of the ANB process. Moreover, the paper proposes a sense BSS update equation and shows that the resulting unmixing matrix is equivalent to the ANB weights. The study provides insights into the connections between different techniques in frequency domain processing and their applications in areas such as jamming suppression.",1
1737,"frequency domain blind source separation -lrb- frequency domain blind source separation -rrb- is shown to be equivalent to two sets of frequency domain adaptive microphone arrays , i.e. , adaptive null beam-formers . the minimization of the off-diagonal components in the bss update equation can be viewed as the minimization of the mean square error in the frequency domain blind source separation . the unmixing matrix of the frequency domain blind source separation and the filter coefficients of the frequency domain blind source separation converge to the same solution in the mean square error sense if the two source signals are ideally independent . therefore , we can conclude that the performance of the frequency domain blind source separation is upper bounded by that of the frequency domain blind source separation . this understanding clearly explains the poor performance of the frequency domain blind source separation in a real room with long reverberation . the fundamental difference exists in the adaptation period when they should adapt . that is , the frequency domain blind source separation can adapt in the presence of a jammer but the absence of a target , whereas the frequency domain blind source separation can adapt in the presence of a target and jammer , and also in the presence of only a target .",0
1738,"This paper discusses learning from labeled and unlabeled data on a directed graph. Many real-world web classification problems have directed graphs, and it is important to understand the directionality of the edges when learning from the data. The authors propose a spectral clustering method that uses both labeled and unlabeled data to construct a graph Laplacian matrix, which is then used to cluster the data. They show that the method is effective and has good time complexity using numerical techniques. The authors also explore the differences between directed and undirected graphs and demonstrate the importance of considering directionality in the learning process. Overall, this paper provides valuable insights into learning on directed graphs and the benefits of using both labeled and unlabeled data.",1
1739,"we propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered . the time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical techniques . in the absence of labeled instances , this framework can be utilized as a spectral clustering method for directed graphs , which generalizes the spectral clustering approach for undirected graphs . we have applied our framework to real-world web classification problems and obtained encouraging results .",0
1740,This paper explores the design of kernel-based combinatorial auctions and examines the issues of stability and incentive,1
1741,"we present the design and analysis of an approximately incentive-compatible combinatorial auction . in just a single run , the auction is able to extract enough value information from bidders to compute approximate truth-inducing payments . this stands in contrast to current auction designs that need to repeat the allocation computation as many times as there are bidders to achieve incentive compatibility . the auction is formulated as a kernel method , which allows for flexibility in choosing the price structure via a kernel function . our main result characterizes the extent to which our auction is incentive-compatible in terms of the complexity of the chosen kernel function . our analysis of the auction 's properties is based on novel insights connecting the notion of stability in statistical learning theory to that of universal competitive equilibrium in the auction literature .",0
1742,"This paper presents SCALPEL, a novel segmentation method that combines bottom-up segmentation models with localized shape priors and mid- to high-level information to generate accurate segmentation proposals. SCALPEL employs a flexible method for the segmentation process and utilizes a stopping criterion to improve efficiency. The method also incorporates rich region-merging cues to refine the segmentation proposals. The approach is evaluated on the Pascal VOC2010 dataset and achieves state-of-the-art results in object segmentation. Furthermore, SCALPEL provides a re-ranking class for the segmentation proposals and is able to improve object layout. The results demonstrate the effectiveness of SCALPEL as a powerful segmentation tool that can be used for a wide range of applications.",1
1743,"we propose scalpel , a flexible method for object seg-mentation that integrates rich region-merging cues with mid-and high-level information about object layout , class , and scale into the segmentation process . unlike competing approaches , scalpel uses a cascade of bottom-up segmentation models that is capable of learning to ignore boundaries early on , yet use them as a stopping criterion once the object has been mostly segmented . furthermore , we show how such scalpel can be learned efficiently . when paired with a novel method that generates better localized shape priors than our competitors , our method leads to a concise , accurate set of segmentation proposals ; these proposals are more accurate on the pascal voc2010 dataset than state-of-the-art methods that use re-ranking to filter much larger bags of proposals . the code for our algorithm is available online .",0
1744,"This paper proposes a method for learning lexicalized reordering models from reordering graphs. The reordering relations of adjacent phrases are extracted from NIST Chinese-English test sets and used to construct a structure named the ""reordering graph."" The graph provides a useful representation of the lexicalized reordering models, which are then learned from the word-aligned bilingual corpus. The proposed method is evaluated on phrase-based translation systems and achieves state-of-the-art results in reordering models. The results demonstrate that the use of the reordering graph can effectively improve the accuracy of the phrase segmentations and thus the quality of the translation output. This approach provides a useful tool for natural language processing tasks that involve reordering relations between phrases.",1
1745,"lexicalized reordering models play a crucial role in phrase-based translation systems . they are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases . instead of just checking whether there is one phrase adjacent to a given phrase , we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models . we propose to use a structure named reordering graph , which represents all phrase segmentations of a sentence pair , to learn lex-icalized reordering models efficiently . experimental results on the nist chinese-english test sets show that our approach significantly outperforms the baseline method .",0
1746,"This paper presents a comparison of different grapheme-to-phoneme (G2P) methods on large pronunciation dictionaries and large vocabulary continuous speech recognition (LVCSR) tasks. G2P conversion is a crucial component of many speech and language processing systems, as it maps written text to its phonetic representation. The evaluation is conducted on several open source software tools that support end user customization of G2P models. The evaluation metrics include phoneme error rate and G2P accuracy on n-best pronunciation variants. The results show that different G2P methods have varying performance on different tasks, and that some methods perform better than others in terms of accuracy and efficiency. This study provides insights into the strengths and limitations of different G2P methods and can help guide the selection of the most appropriate method for a given ASR system.",1
1747,"grapheme-to-phoneme conversion -lrb- g2p -rrb- is usually used within every state-of-the-art asr system to generalize beyond a fixed set of words . although the performance is typically already quite good -lrb- < 10 % phoneme error rate -rrb- and pronunciations of important words are checked by a linguist , further improvements are still desirable , especially for end user customization . in this work , we present and compare five methods/tools to tackle the g2p task . although most of the methods have already been published and/or are available as open source software , the reported experiments are done on large state-of-the-art tasks and the used software is from the actual publications . besides an experimental comparison on text data for a range of languages -lrb- i.e. measuring the g2p accuracy only -rrb- , our focus in this paper is measuring the effect of improved grapheme-to-phoneme conversion on lvcsr performance for a challenging g2p task . additionally , the effect of using n-best pronunciation variants instead of single best is investigated briefly .",0
1748,This paper presents a non-linear spectral contrast stretching method for in-car speech recognition. The method is based on the adaptation of the auditory system and is designed to improve the robustness of speech recognition systems under adverse conditions such as noise and processing artifacts. The proposed approach is evaluated on the CENSREC-2 in-car database and compared to a baseline MFCC system. The evaluation includes feature normalization methods and cepstral post-processing techniques. The results show that the non-linear contrast stretching method significantly improves noise robustness and outperforms the baseline MFCC system. The method also employs log mel-filterbanks and two-dimensional filters in the log-scaled spectral domain to improve processing efficiency. This study demonstrates the effectiveness of non-linear spectral contrast stretching for in-car speech recognition and provides insights into the potential of adaptation of the auditory system in improving speech recognition performance.,1
1749,"in this paper , we present a novel feature normalization method in the log-scaled spectral domain for improving the noise robustness of speech recognition front-ends . in the proposed feature normalization method , a non-linear contrast stretching is added to the outputs of log mel-filterbanks to imitate the adaptation of the auditory system under adverse conditions . this is followed by a two-dimensional filter to smooth out the processing artifacts . the proposed feature normalization method perform remarkably well on censrec-2 in-car database with an average relative improvement of 29.3 % compared to baseline mfcc system . it is also confirmed that the proposed processing in log mfb domain can be integrated with conventional cepstral post-processing techniques to yield further improvements . the proposed feature normalization method is simple and requires only a small extra computation load .",0
1750,"This paper presents a new dialogue control method based on the human listening process to construct an interface for ascertaining a user's inputs. The proposed method focuses on naturally controlled dialogues and stress-free voice input to improve the user experience. The method is based on human dialogue analysis and pretense-type recognition to achieve real-time responses. The system recognizes and analyzes user input to provide appropriate responses, which are generated based on the user's intention and the context of the conversation. The system's recognition processes are optimized to improve accuracy and efficiency. The evaluation of the system shows that it outperforms traditional dialogue control methods in terms of user satisfaction and accuracy of recognition. This study demonstrates the potential of using the human listening process as a model for dialogue control and provides insights into the design of natural and effective interfaces for user inputs.",1
1751,"this paper describes a new dialogue control method that utilizes new recognition processes called '' presupposition-type recognition '' and '' pretense-type recognition '' that we propose based on human dialogue analysis . this dialogue control method provides users with stress-free voice input through real-time responses , comprising a naturally controlled dialogue to obtain information in order to winnow candidates comprehensively .",0
1752,"This paper proposes an adaptive image fusion method using Independent Component Analysis (ICA) bases. The method aims to combine multiple modalities of sensor images into a composite image that contains the most informative features from each modality. The proposed ICA-based fusion framework allows for the construction of an adaptive fusion scheme that can automatically adjust to the input data, and effectively remove redundant and irrelevant information from the final composite image. The method involves decomposing the input images into independent components using ICA, followed by a fusion process that selects the most relevant components and combines them into a composite image. The proposed method is evaluated on various datasets, and the results demonstrate its effectiveness in achieving high-quality fusion images with improved visual clarity and information content compared to traditional fusion methods. The proposed adaptive image fusion method has potential applications in various fields such as remote sensing, medical imaging, and surveillance. This study provides insights into the importance of using ICA bases for adaptive image fusion and highlights its advantages over traditional fusion techniques.",1
1753,"image fusion can be viewed as a process that incorporates essential information from different modality sensors into a composite image . the use of bases trained using independent component analysis for image fusion has been highlighted recently . common fusion rules can be used in the ica fusion framework with promising results . in this paper , the authors propose an adaptive fusion scheme , based on the ica fusion framework , that maximises the sparsity of the fusion image in the transform domain .",0
1754,"This paper proposes a method for estimating and localizing internal delays in a network using network tomography. The proposed method uses an end-to-end delay measurement and an EM algorithm to estimate the internal delay distribution of the network. The internal delay distribution is then used to estimate the network traffic and network dynamics. The method employs a sequential Monte Carlo procedure to track non-stationary delay characteristics, which helps to accurately estimate the internal network performance. The paper also discusses the internal behavior of the network and how it can be analyzed using the proposed method. The results of the study demonstrate the effectiveness of the proposed approach in accurately estimating internal delays in a network, particularly in the presence of dynamic routing algorithms and non-stationary delay characteristics.",1
1755,"on-line , spatially localized information about internal network performance can greatly assist dynamic routing algorithms and traffic transmission protocols . however , it is impractical to measure network traffic at all points in the network . a promising alternative is to measure only at the edge of the network and infer internal behavior from these measurements . in this paper we concentrate on the estimation and localization of internal delays based on end-to-end delay measurements from sources to receivers . we develop an em algorithm for computing mles of the internal delay distributions in cases where the network dynamics are stationary over the observation period . for time-varying cases , we propose a sequential monte carlo procedure capable of tracking non-stationary delay characteristics . simulations are included to demonstrate the promise of these techniques .",0
1756,"This paper proposes a feature-centric cascade approach for fast and accurate face detection, utilizing a locally assembled binary (LAB) feature. The method combines the discriminating power of binary Haar features with the accumulated intensities of local binary pattern features to improve detection speed and reduce computational cost. The LAB feature is shown to have superior performance to traditional Haar features, and the feature-centric cascade approach outperforms previous methods in terms of detection accuracy. The proposed method also incorporates an ordinal relationship detection method to capture the internal structure of faces. Results demonstrate the effectiveness of the LAB feature and feature-centric cascade approach in improving face detection performance.",1
1757,"in this paper , we describe a novel type of feature-centric cascade for fast and accurate face detection . the feature-centric cascade is called locally assembled binary haar feature . locally assembled binary haar feature is basically inspired by the success of haar feature and local binary pattern for face detection , but it is far beyond a simple combination . in our locally assembled binary haar feature , haar features are modified to keep only the ordinal relationship -lrb- named by binary haar feature -rrb- rather than the difference between the accumulated intensities . several neighboring binary haar features are then assembled to capture their co-occurrence with similar idea to local binary pattern . we show that the feature-centric cascade is more efficient than haar feature and local binary pattern both in discriminating power and computational cost . furthermore , a novel efficient detection method called feature-centric cascade is proposed to build an efficient detector , which is developed from the feature-centric method . experimental results on the cmu+mit frontal face test set and cmu profile test set show that the proposed locally assembled binary haar feature can achieve very good results and amazing detection speed .",0
1758,This paper presents a method for simultaneous learning of a discriminative projection and prototypes for nearest-neighbor classification. The goal is to reduce the classification error probability in image recognition research by using dimensionality reduction techniques. The proposed method employs an iterative algorithm that learns a linear projection base and prototypes that are used in a nearest-neighbor classifier. The method achieves improved performance compared to standard dimensionality reduction classifiers.,1
1759,"computer vision and image recognition research have a great interest in dimensionality reduction techniques . generally these techniques are independent of the classifier being used and the learning of the classifier is carried out after the dimensionality reduction is performed , possibly discarding valuable information . in this paper we propose an iterative algorithm that simultaneously learns a linear projection base and a reduced set of prototypes optimized for the nearest-neighbor classifier . the iterative algorithm is derived by minimizing a suitable estimation of the classification error probability . the proposed iterative algorithm is assessed through a series of experiments showing a good behavior and a real potential for practical applications .",0
1760,"This paper proposes a regional CNN-LSTM model for dimensional sentiment analysis, which aims to predict the continuous numerical values of valence-arousal (VA) ratings of texts. Unlike traditional categorical approaches, the dimensional approach takes into account affective information and captures long-distance dependency, which are crucial for fine-grained sentiment analysis. The proposed model combines the strengths of both convolutional neural networks (CNN) and long short-term memory (LSTM) networks to exploit local information and capture temporal dynamics. Experimental results show that the proposed model outperforms existing nn-based methods in terms of va prediction and sentiment classification. The proposed model has the potential to be applied in various fields, such as social media analysis, customer feedback analysis, and product review analysis.",1
1761,"dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valence-arousal space . compared to the categorical approach that focuses on sentiment classification such as binary classification -lrb- i.e. , positive and negative -rrb- , the dimensional approach can provide more fine-grained sentiment analysis . this study proposes a regional cnn-lstm model consisting of two parts : dimensional approach and lstm to predict the va ratings of texts . unlike a conventional cnn which considers a whole text as input , the proposed dimensional approach uses an individual sentence as a region , dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the va prediction . such affective information is sequentially integrated across regions using lstm for va prediction . by combining the dimensional approach and lstm , both local information within sentences and long-distance dependency across sentences can be considered in the prediction process . experimental results show that the proposed regional cnn-lstm model outperforms lexicon-based , regression based , and nn-based methods proposed in previous studies .",0
1762,"This paper investigates the use of non-professional translators in crowdsourcing translation tasks and their ability to produce professional quality translations. The study focuses on the impact of various features such as calibration and country of residence on translation quality. The authors use Mechanical Turk to collect redundant translations and calculate edit rates and language model perplexity to assess translation quality. The results show that non-professional translators can produce high-quality translations, and the inclusion of calibration and considering country of residence can further improve translation quality. Overall, the study suggests that crowdsourcing translation can be a viable option for producing professional-quality translations.",1
1763,"naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent , low-quality results if no quality control is exercised . we demonstrate a variety of mechanisms that increase the translation quality to near professional levels . specifically , we solicit redundant translations and edits to them , and automatically select the best output among them . we propose a set of features that model both the translations and the translators , such as country of residence , lm perplexity of the translation , edit rate from the other translations , and -lrb- optionally -rrb- calibration against professional translators . using these features to score the collected translations , we are able to discriminate between acceptable and unacceptable translations . we recreate the nist 2009 urdu-to-english evaluation set with mechanical turk , and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators . the total cost is more than an order of magnitude lower than professional translation .",0
1764,"Title: Effective separation of sparse and non-sparse image features for denoising.

Entities: compressible and incompressible regions, posterior median-based denoising method, principal components analysis, canonical transformation, approximate classifier, natural images, modeling strategy, textured regions, undecimated wavelets, edges image.

Graph: <H> posterior median-based denoising method <R> USED-FOR <T> posterior median-based denoising method <H> principal components analysis <R> USED-FOR <T> approximate classifier.

Abstract: This paper presents an effective method for denoising images by separating sparse and non-sparse features. The method uses a modeling strategy that takes into account the compressible and incompressible regions of natural images. The proposed denoising method is based on a posterior median approach, which utilizes the principal components analysis to effectively separate sparse and non-sparse features. The method also includes a canonical transformation and an approximate classifier to further improve the quality of the denoised image. The proposed approach is evaluated on both textured regions and edges images using undecimated wavelets. The experimental results show that the proposed method is effective in separating sparse and non-sparse features and achieves better denoising performance than existing methods.",1
1765,"over-complete representations of images such as undecimated wa-velets have enjoyed immense popularity in recent years . though undecimated wa-velets are efficient for modeling singularities and edges , natural images also consist of textures that are difficult to capture with any canonical transformation . in this work , we develop a new modeling strategy with a rigorous treatment of textured regions . using principal components analysis as an approximate classifier for edges and textures , we partition an image into compressible and incompress-ible regions -- with corresponding posterior median-based denoising method matching their behaviors . a posterior median-based denoising method using these posterior median-based denoising method is described with preliminary results that demonstrate the effectiveness of this posterior median-based denoising method .",0
1766,"This paper proposes a novel Bayesian nonparametric approach for the identification of sparse dynamic linear systems. The approach uses stable spline kernels and exponential hyperpriors to learn the system's scale factors, impulse responses, and ARMAX models. To ensure BIBO stability, a prediction error minimization constraint is imposed on the sparse solutions. The proposed method is compared to parametric identification techniques, such as the group LAR algorithm and Gaussian processes, and is shown to outperform them in terms of accuracy and efficiency. Additionally, the paper demonstrates the effectiveness of the exponential hyperpriors in improving the sparsity of the learned models. Overall, the results indicate that the proposed method is a promising approach for learning sparse dynamic linear systems.",1
1767,"we introduce a new bayesian nonparametric approach to identification of sparse dynamic linear systems . the impulse responses are modeled as gaussian processes whose autocovariances encode the bibo stability constraint , as defined by the recently introduced '' stable spline kernel '' . sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels . numerical experiments regarding estimation of armax models show that this bayesian nonparametric approach provides a definite advantage over a group lar algorithm and state-of-the-art parametric identification techniques based on prediction error minimization .",0
1768,"This paper presents a simple, robust, and scalable semi-supervised learning method, called Expectation Regularization (ER), which is based on the conditional label-likelihood objective function for exponential family parametric models. ER is a label propagation algorithm that can be applied to data sets with non-independent features and model predictions. The method leverages both labeled and unlabeled data to estimate the label priors, tuning scales, and the model parameters simultaneously. ER is particularly suitable for large-scale data sets and can be easily deployed in various applications. The effectiveness of ER is demonstrated through extensive experiments on several benchmark data sets, where it outperforms several state-of-the-art semi-supervised methods in terms of accuracy and scalability. Overall, ER is a promising semi-supervised learning method that provides a powerful tool for tackling problems with limited labeled data.",1
1769,"although semi-supervised learning has been an active area of research , its use in deployed applications is still relatively rare because the methods are often difficult to implement , fragile in tuning , or lacking in scalability . this paper presents <i> expectation regularization </i> , a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations -- such as label priors . the semi-supervised learning method is extremely easy to implement , scales as well as logistic regression , and can handle non-independent features . we present experiments on five different data sets , showing accuracy improvements over other semi-supervised methods .",0
1770,"This paper proposes a memory-based particle filter (PF) for robust face pose tracking under complex dynamics. PF is a popular visual tracking technique that has been successfully applied in many applications. However, abrupt object movements and long-term dynamics can cause tracking failure, especially when the proper dynamics model is not available. To address this problem, we propose a new memory-based PF formulation that uses a nonlinear, time-variant PF framework to model systems with complex dynamics. Our method leverages magnetic sensors to estimate the face pose accurately, even in the presence of occlusions. The proposed method uses a prior distribution to predict the state of the system and a tracking loss function to update the state estimate. The memory-based approach allows us to use random sampling to generate the proposal distribution for each time step, thereby avoiding the Markov assumption used in conventional PF frameworks. The effectiveness of the proposed method is demonstrated through extensive experiments on several benchmark data sets, where it outperforms several state-of-the-art techniques in terms of accuracy and robustness. Overall, the proposed memory-based PF provides a promising tool for face pose estimation and visual tracking applications.",1
1771,"a novel particle filter , the memory-based particle filter , is proposed that can visually track moving objects that have complex dynamics . we aim to realize robust-ness against abrupt object movements and quick recovery from tracking failure caused by factors such as occlusions . to that end , we eliminate the markov assumption from the previous particle filtering framework and predict the prior distribution of the target state from the long-term dynamics . more concretely , memory-based particle filter stores the past history of the estimated target states , and employs a random sampling from the history to generate prior distribution ; memory-based particle filter represents a novel pf formulation.our method can handle nonlinear , time-variant , and non-markov dynamics , which is not possible within existing pf frameworks . accurate prior prediction based on proper dynamics model is especially effective for recovering lost tracks , because memory-based particle filter can provide possible target states , which can drastically change since the track was lost . we target the face pose of seated humans in this paper . quantitative evaluations with magnetic sensors confirm improved accuracy in face pose estimation and successful recovery from tracking loss . the proposed memory-based particle filter suggests a new paradigm for modeling systems with complex dynamics and so offers a various visual tracking applications .",0
1772,"This paper presents efficient methods for dealing with missing data in supervised learning. Missing inputs, or incomplete feature vectors, can occur for various reasons, such as sensor failure or incomplete data collection. These missing features can affect the performance of supervised learning algorithms, especially for complex models with arbitrary feedforward networks. We propose two approaches to address this problem: a weighted averaged backpropagation step and a closed form solution based on the input data distribution and Parzen windows. The former is a modification of the standard backpropagation step that allows for incomplete feature vectors. The latter provides a closed form solution for regression problems that can handle missing data more efficiently than traditional methods. Both methods are evaluated on several benchmark data sets, and the results show that they outperform several state-of-the-art techniques in terms of classification and recall. Furthermore, the proposed methods reduce the complexity of training and can handle missing data in a more efficient manner. Overall, this paper provides efficient methods for dealing with missing data in supervised learning, which can improve the performance of machine learning algorithms in various applications.",1
1773,"we present efficient algorithms for dealing with the problem of missing inputs -lrb- incomplete feature vectors -rrb- during training and recall . our approach is based on the approximation of the input data distribution using parzen windows . for recall , we obtain closed form solutions for arbitrary feedforward networks . for training , we show how the backpropagation step for an incomplete feature vectors can be approximated by a weighted averaged backpropagation step . the complexity of the closed form solutions for training and recall is independent of the number of missing features . we verify our theoretical results using one classification and one regression problem .",0
1774,"This paper presents a novel approach to extract statistically independent components from high-dimensional multi-sensory input streams using spiking neurons. The goal is to perform blind source separation by extracting independent components from the input data without prior knowledge of the sources. We propose an information bottleneck optimization method based on unsupervised learning principles, specifically the independent component analysis (ICA) and information bottleneck method. The concrete learning rules of the proposed method are based on the processing strategy of spiking neurons. Our method utilizes the internal predictions and proprioceptive feedback to extract independent components from high-dimensional input streams. We demonstrate the effectiveness of our approach on several benchmark data sets, and the results show that it outperforms several state-of-the-art techniques in terms of accuracy and speed. Furthermore, our method provides a novel framework for abstract information optimization principles and concrete learning rules for spiking neuron-based unsupervised learning. Overall, this paper presents a new method for the extraction of statistically independent components from high-dimensional multi-sensory input streams using spiking neurons, which can be applied in various applications that require efficient sensory processing and information extraction.",1
1775,"the extraction of statistically independent components from high-dimensional multi-sensory input streams is assumed to be an essential component of sensory processing in the brain . such independent component analysis -lrb- or blind source separation -rrb- could provide a less redundant representation of information about the external world . another powerful processing strategy is to extract preferentially those components from high-dimensional input streams that are related to other information sources , such as internal predictions or proprioceptive feedback . this processing strategy allows the optimization of internal representation according to the information bottleneck method . however , concrete learning rules that implement these general unsupervised learning principles for spiking neurons are still missing . we show how both information bottleneck optimization and the extraction of independent components can in principle be implemented with stochastically spiking neurons with refractoriness . the new concrete learning rules that achieves this is derived from abstract information optimization principles .",0
1776,"This paper presents an exploration of subjective natural language problems and their motivations, applications, characterizations, and implications. Subjective natural language problems refer to the challenges associated with interpreting and analyzing subjective language expressions, such as opinions and sentiments, in computational linguistics. The paper discusses problem-solving methods for addressing these challenges, including sentiment analysis and opinion mining. A holistic approach to analyzing subjective language is emphasized, and the paper proposes characterizations of subjective natural language problems. The implications of these problems are also discussed, including their impact on decision-making processes and the need for improved natural language processing techniques.",1
1777,"this opinion paper discusses subjective natural language problems in terms of their motivations , applications , characterizations , and implications . it argues that such subjective natural language problems deserve increased attention because of their potential to challenge the status of theoretical understanding , problem-solving methods , and evaluation techniques in computational linguistics . the author supports a more holis-tic approach to such subjective natural language problems ; a view that extends beyond opinion mining or sentiment analysis .",0
1778,"This paper proposes a novel approach for extracting local features from images, called Phase-Based Local Features. The method is based on the use of complex-valued steerable filters, which can effectively capture common image deformations, including 2-D rotation, and differential invariants. The feature extraction is robust to common brightness and illumination changes, as well as scale changes and noise. The resulting feature vectors contain phase data, which provides identity information of the underlying image structure. The paper provides a detailed description of the method and its advantages over other feature extraction methods. The proposed approach can be applied to a wide range of computer vision applications, such as image classification and object recognition.",1
1779,"we introduce a new type of phase-based local feature based on the phase and amplitude responses of complex-valued steerable filters . the design of this phase-based local feature is motivated by a desire to obtain feature vectors which are semi-invariant under common image deformations , yet distinctive enough to provide useful identity information . a recent proposal for such phase-based local feature involves combining differential invariants to particular image deformations , such as rotation . our approach differs in that we consider a wider class of image deformations , including the addition of noise , along with both global and local brightness variations . we use steerable filters to make the feature robust to rotation . and we exploit the fact that phase data is often locally stable with respect to scale changes , noise , and common brightness changes . we provide empirical results comparing our phase-based local feature with one based on differential invariants . the results show that our phase-based local feature leads to better performance when dealing with common illumination changes and 2-d rotation , while giving comparable effects in terms of scale changes .",0
1780,"This paper proposes a method for generating robust Gram embeddings for natural language processing (NLP) applications. Gram embeddings are a type of vectorial word representation used in NLP tasks such as word similarity. The authors formulate a regularized embedding formulation to improve the generalization abilities of the embeddings. The proposed method addresses the challenge of working with finite data, small datasets, and human similarities, which can lead to overfitting and limit the complexity of the embeddings. The results show that the proposed method outperforms previous approaches on word similarity tasks and context embeddings. The robust Gram embeddings have potential applications in various NLP tasks.",1
1781,"word embedding word embedding models learn vectorial word representations that can be used in a variety of nlp applications . when training data is scarce , these word embedding models risk losing their generalization abilities due to the complexity of the word embedding models and the overfitting to finite data . we propose a regularized embedding formulation , called robust gram , which penalizes overfitting by suppressing the disparity between target and context embeddings . our experimental analysis shows that the robust gram trained on small datasets generalizes better compared to alternatives , is more robust to variations in the training set , and correlates well to human similarities in a set of word similarity tasks .",0
1782,"I apologize, but the provided graph is insufficient to generate a paper abstract as it only contains one node and edge. Could you please provide additional information or a new graph?",1
1783,"in order to overcome several limitations of structured light 3d acquisition methods , the colors , intensities , and shapes of the projected patterns are adapted to the scene . based on a crude estimate of the scene geometry and reflectance characteristics , the local intensity ranges in the projected patterns are adapted , in order to avoid over-and under-exposure in the image . this avoids the infamous specularity problems and generally increases accuracy . the estimated geometry also helps to limit the effect of aliasing caused by the sampling of foreshortened patterns . furthermore , the approach also acounts for the adverse effects that small motions during scanning would normally have . moreover , the approach yields a confidence measure at every pixel of the range image . last but not least , the scanner consists of consumer products only , and therefore is cheap .",0
1784,"This paper proposes a spectral conversion approach for feature denoising and speech enhancement. The approach is based on an iterative Kalman filter that utilizes the spectral conversion technique to denoise noisy speech features and enhance the overall quality of speech signals. The proposed method is evaluated using the average segmental output signal-to-noise ratio and is shown to be effective for both feature denoising and speech enhancement problems. The spectral conversion technique is used to transform noisy speech features into clean speech features, which are then further denoised using a feature denoising method based on the Kalman filter. The results demonstrate that the proposed approach outperforms existing methods in terms of clean speech signal reconstruction, and it can be used in applications such as speech recognition, teleconferencing, and hearing aids.",1
1785,"in this paper we demonstrate that spectral conversion can be successfully applied to the speech enhancement problem as a feature denoising method . the enhanced spectral conversion can be used in the context of the kalman filter for estimating the clean speech signal . in essence , instead of estimating the clean speech features and the clean speech signal using the iterative kalman filter , we show that is more efficient to initially estimate the clean speech features from the noisy speech features using spectral conversion -lrb- using a training speech corpus -rrb- and then apply the standard kalman filter . our results show an average improvement compared to the iterative kalman filter that can reach 6 db in the average segmental output signal-to-noise ratio , in low input snr 's .",0
1786,"This paper presents a study on the use of orthogonal frequency-division multiplexing (OFDM) for underwater acoustic communications. Specifically, the paper proposes an adaptive synchronization approach that uses a non-uniform frequency offset compensation method and a low-complexity channel estimation technique based on sparse channel estimation. The proposed synchronization method enables efficient phase synchronization and improves the accuracy of channel impulse response estimation. The proposed low-complexity channel estimation method operates in the time domain and employs a decision-directed operation to further reduce complexity. The adaptive synchronization approach is shown to outperform existing synchronization methods in terms of accuracy and robustness, making it suitable for underwater acoustic communication applications. The proposed sparse channel estimation method also demonstrates superior performance compared to traditional channel estimation methods. The results suggest that the proposed approach can significantly improve the reliability and efficiency of OFDM-based underwater acoustic communication systems.",1
1787,"a phase synchronization method , which provides non-uniform frequency offset compensation needed for wideband ofdm -lsb- 1 -rsb- , is coupled with low-complexity channel estimation in the time domain . sparsing of the channel impulse response leads to an improved performance , while adaptive synchronization supports decision-directed operation and yields low overhead . system performance is demonstrated using experimental data transmitted over a 1 km shallow water channel in the 19 khz-31 khz band .",0
1788,"This paper presents a three-stage solution for flexible vocabulary speech understanding, which addresses the challenges of recognizing out-of-vocabulary words and dealing with unseen data. The proposed approach involves using a column-bigram finite-state transducer, natural language processor, and angie sublexical models to generate syllable-level lexical units with tighter linguistic constraints. The system also incorporates instantaneous sound-to-letter capability and grapheme information to improve recognition accuracy. An iterative procedure is used to refine the word network, which is evaluated using the Jupiter implementation. The results demonstrate the effectiveness of the three-stage approach, which significantly improves recognition accuracy for unknown words and unseen data.",1
1789,"this paper discusses our three-stage approach to a flexible vocabulary speech understanding system , which can detect out-of-vocabulary words , and hypothesize their phonetic and or-thographic transcriptions . in the first stage , we introduce the column-bigram finite-state transducer which , while embedding angie sublexical models , also supports previously unseen data from unknown words . secondly , the angie sublexical models utilize grapheme information , providing tighter linguistic constraint as well as instantaneous sound-to-letter capability during recognition . thirdly , the syllable-level lexical units of the first stage are automatically derived via an iterative procedure to optimize performance . the three-stage approach employs angie sublexical models to output a word network which is parsed by tina , our natural language processor , in stage three . experiments with a jupiter implementation of this three-stage approach are described in -lsb- 1 -rsb- .",0
1790,"This paper proposes a novel method, PETRELS, for subspace estimation and tracking from partial observations in a data stream. The method builds on state-of-the-art batch algorithms for sequential low-rank matrix completion problems and parallel estimation and tracking of a low-dimensional linear subspace matrix. PETRELS uses a recursive procedure based on recursive least squares estimation to estimate the subspace in an online fashion. The long-term behavior of the data stream is taken into account by modeling the subspace as evolving over time. Numerical examples are provided to illustrate the effectiveness of the proposed method, including direction-of-arrival estimation applications. The results demonstrate that PETRELS outperforms other methods in terms of both estimation accuracy and computational efficiency.",1
1791,"we consider the data stream of reconstructing a data stream from a small subset of its entries , where the data stream is assumed to lie in a low-dimensional linear subspace , possibly corrupted by noise . it is also important to track the change of underlying subspace for many applications . this data stream can be viewed as a sequential low-rank matrix completion problem in which the subspace is learned in an online fashion . the proposed algorithm , called parallel estimation and tracking by recursive least squares , identifies the underlying low-dimensional subspace via a recursive procedure for each row of the subspace matrix in parallel , and then reconstructs the missing entries via least-squares estimation if required . recursive least squares outperforms previous approaches by discounting observations in order to capture long-term behavior of the data stream and be able to adapt to recursive least squares . numerical examples are provided for direction-of-arrival estimation and matrix completion , comparing recursive least squares with state of the art batch algorithms .",0
1792,"This paper proposes a novel approach for image segmentation and data clustering using nonlocal partial difference equations (PdES) on graphs. The proposed method combines nonlocal regularization functionals with continuous global active contours and nonlocal discrete perimeters to achieve accurate and efficient image segmentation and data clustering in high-dimensional settings. Specifically, the method employs nonlocal regularization functionals to encourage smoothness in the segmentation or clustering results, while the nonlocal discrete perimeters provide a way to encode the connectivity information of the underlying graph. The proposed method also leverages the co-area formula and gradients on sub-graphs to enable efficient computation of the nonlocal global minimizers. Numerical experiments demonstrate that the proposed method outperforms state-of-the-art methods for image segmentation and data clustering on several benchmark datasets. Overall, the proposed approach shows great promise for a wide range of applications in computer vision and machine learning.",1
1793,"we propose a transcription on graphs of recent continuous global active contours proposed for image segmentation to address the problem of binary partitioning of data represented by graphs . to do so , using the framework of partial difference equations , we propose a family of nonlocal regular-ization functionals that verify the co-area formula on graphs . the gradients of a sub-graph are introduced and their properties studied . relations , for the case of a sub-graph , between the introduced nonlocal regularization functionals and nonlo-cal discrete perimeters are exhibited and the co-area formula on graphs is introduced . finally , nonlocal global minimiz-ers can be considered on graphs with the associated energies . experiments show the benefits of the approach for nonlocal image segmentation and high dimensional data clustering .",0
1794,"This paper presents a study on content-based recommender systems for spoken documents, focusing on heterogeneous information sources such as audio, music, and text domains. The proposed approach aims to improve spoken document retrieval by leveraging content-based features, non-linguistic aspects, preference ratings, and relevance judgments. The study evaluates several hybrid fusion techniques, including a bag-of-words baseline and a multisource approach, to combine the features from different sources effectively. The proposed method uses gender and speaker information to further refine the content-based features, and a corpus of public domain internet audio is utilized to demonstrate the effectiveness of the approach. The study also investigates the impact of preference ratings and document relevance on content-based recommendation, and the results show that the proposed approach outperforms the baseline methods in terms of precision and recall. Overall, the proposed content-based recommender system provides a promising solution for improving the retrieval of spoken documents, and it can be applied to various domains, including music recommender systems and other content-based recommendation scenarios.",1
1795,"content-based recommender systems use preference ratings and features that characterize media to model users ' interests or information needs for making future recommendations . while previously developed in the music and text domains , we present an initial exploration of content-based recommendation for spoken documents using a corpus of public domain internet audio . unlike familiar speech technologies of topic identification and spoken document retrieval , our recommendation task requires a more comprehensive notion of document relevance than bags-of-words would supply . inspired by music recommender systems , we automatically extract a wide variety of content-based features to characterize non-linguistic aspects of the audio such as speaker , language , gender , and environment . to combine these heterogeneous information sources into a single relevance judgement , we evaluate feature , score , and hybrid fusion techniques . our study provides an essential first exploration of the task and clearly demonstrates the value of a multisource approach over a bag-of-words baseline .",0
1796,"This paper proposes a labeling system, called Meeting Acts, for annotating high-level group interaction tags in meetings. The system utilizes the ICSI Meeting Recorder Corpus, which contains a vast amount of dialog act information and sequences, to extract meaningful insights into meeting styles and behaviors. The paper presents a detailed labeling process for assigning appropriate tags to different segments of the meeting, which can help capture the nuances and complexities of group interactions. The proposed Meeting Acts system provides a systematic and efficient way of analyzing meeting data, allowing researchers to gain valuable insights into the communication patterns and social dynamics of group interactions. The system can be used to analyze various aspects of meetings, including turn-taking, topic transitions, and participant roles. Overall, the proposed labeling system provides a useful tool for researchers and practitioners interested in studying group interactions and can help improve our understanding of effective communication and collaboration in meetings.",1
1797,"we describe a new system for labeling speech corpora with high-level group interaction tags , called '' meeting acts . '' the system was motivated by a need to assess work seeking to automatically detect meeting style using dialog act information . we present information about the relationships seen between dialog act sequences and meeting style to motivate the labeling process . we provide a summary of the annotation system and labeling procedure , as well as preliminary inter-annotator reliability statistics on the icsi meeting recorder corpus .",0
1798,"This paper introduces the concept of Limited Enquiry Negotiation Dialogues, which are designed to facilitate effective man-machine dialogues. The paper proposes a dialogue management strategy that is specific to Limited Enquiry Negotiation Dialogues, and outlines the standard components of the dialogue designer's toolbox, including menu-traversal and slot-filling. The behavior of the dialogue is analyzed and discussed, and its effectiveness is demonstrated through examples and experiments. The paper shows that Limited Enquiry Negotiation Dialogues are a promising approach to achieving effective man-machine communication, and that they offer a useful alternative to more traditional dialogue strategies.",1
1799,"we define a new dialogue management strategy limited enquiry negotiation dialogues designed for enabling simple man-machine dialogues in which the parameters -lrb- for which the user will supply values -rrb- of a query to a database are negotiated . the choice of which query to make next is also not pre-ordained . the dialogue management strategy limited enquiry negotiation dialogues is simple and intuitive but permits interestingly complex dialogue behaviour . we propose dialogue management strategy limited enquiry negotiation dialogues as an addition to a dialogue designer 's standard components toolbox along with other well-known ideas such as menu-traversal and slot-filling . we illustrate the dialogue management strategy limited enquiry negotiation dialogues by examining how dialogue management strategy limited enquiry negotiation dialogues accounts for interesting but by no means rare data in a wizard of oz corpus of business trip planning dialogues . finally , we discuss some more theoretical issues arising from the dialogue management strategy limited enquiry negotiation dialogues .",0
1800,"This paper presents a novel speaker verification system based on the lattice MLLR technique and the m-vector approach. The system uses a maximum likelihood linear regression approach to estimate super-vectors that represent the phonetic content of speech segments. The lattice word transcriptions and 1-best hypotheses from ASR are used to obtain uniform segmentation for the speech segments. The system is evaluated on the NIST SRE 2008 Core Condition dataset, and results show that the proposed system outperforms other state-of-the-art techniques. The paper provides detailed analysis of the results and demonstrates the effectiveness of the proposed system in handling ASR transcription errors.",1
1801,"the recently introduced m-vector approach uses maximum likelihood linear regression super-vectors for speaker verification , where maximum likelihood linear regression super-vectors are estimated with respect to a universal background model without any transcription of speech segments and speaker m-vectors are obtained by uniform segmentation of their maximum likelihood linear regression super-vectors . hence , this m-vector approach does not exploit the phonetic content of the speech segments . in this paper , we propose the integration of an automatic speech recognition -lrb- asr -rrb- based multi-class mllr transformation into the m-vector approach . we consider two variants , with maximum likelihood linear regression super-vectors computed either on the 1-best -lrb- hypothesis -rrb- or on the lattice word transcriptions . the former case is able to account for the risk of asr transcription errors . we show that the proposed m-vector approach outperform the conventional m-vector approach over various tasks of the nist sre 2008 core condition .",0
1802,"This paper explores the challenge of handling recordings acquired simultaneously over multiple channels in speaker recognition scenarios. Specifically, it focuses on the NIST SRE12 Core Condition dataset and the inter-session variability terms that come with it. To address this issue, the authors propose the use of a PLDA (Probabilistic Linear Discriminant Analysis) model to account for channel variability and phonetic content. The evaluation of the proposed approach is carried out using the minimum DCF (Detection Cost Function) metric on telephone speech, and the results demonstrate the effectiveness of the PLDA model in handling multiple channels and improving recognition performance. This study contributes to the advancement of speaker recognition research and provides insights into the use of PLDA models for handling recordings acquired simultaneously over multiple channels.",1
1803,"in some speaker recognition scenarios we find conversations recorded simultaneously over multiple channels . that is the case of the interviews in the nist sre dataset . to take advantage of that , we propose a modification of the plda model that considers two different inter-session variability terms . the first term is tied between all the recordings belonging to the same conversation whereas the second is not . thus , the former mainly intends to capture the variability due to the phonetic content of the conversation while the latter tries to capture the channel variability . we test this plda model on the nist sre12 core condition using multiple channels per interview to enroll the speakers . the proposed plda model improves the minimum dcf by 26 -- 29 % on telephone speech and by 1 -- 8 % on interviews compared to the standard plda -lrb- scored by the book -rrb- .",0
1804,"This paper proposes a novel approach to sequential data classification using a combination of the maximum entropy discrimination framework and nonparametric Bayesian inference. The proposed method, called Infinite Markov-Switching Maximum Entropy Discrimination Machines (IMS-MEDM), incorporates a switching mechanism to model complex temporal dynamics in the data. The IMS-MEDM model consists of a local nonlinearity component and a large-margin classification component, both of which are learned using a nonparametric Bayesian inference scheme. The use of stick-breaking priors enables efficient model training and allows for the construction of a Markov-switching model with an infinite number of components. The performance of the IMS-MEDM model is evaluated on several real-world datasets, demonstrating its effectiveness in modeling complex temporal dynamics and outperforming other state-of-the-art approaches. This study contributes to the advancement of sequential data classification research and provides insights into the use of Bayesian nonparametrics and large-margin classifiers in the maximum entropy discrimination framework.",1
1805,"in this paper , we present a method that combines the merits of bayesian nonparametrics , specifically stick-breaking priors , and large-margin kernel machines in the context of sequential data classification . the proposed model employs a set of -lrb- theoretically -rrb- infinite interdependent large-margin classifiers as model components , that robustly capture local nonlinearity of complex data . the employed large-margin classifiers are connected in the context of a markov-switching construction that allows for capturing complex temporal dynamics in the modeled datasets . appropriate stick-breaking priors are imposed over the component switching mechanism of our model to allow for data-driven determination of the optimal number of component large-margin classifiers , under a standard nonparametric bayesian inference scheme . efficient model training is performed under the maximum entropy discrimination framework , which integrates the large-margin principle with bayesian posterior inference . we evaluate our method using several real-world datasets , and compare it to state-of-the-art alternatives .",0
1806,"This paper addresses the problem of robust stability for linear time-variant systems represented by difference equations. Specifically, the focus is on the restricted parameter perturbations of the system's coefficients and the identification of the regions in the coefficient-space where robust stability can be guaranteed. The approach taken involves analyzing the system's difference equation and deriving sufficient conditions for robust stability in terms of the system's coefficients. The analysis is carried out using mathematical techniques such as Lyapunov functions and matrix inequalities. The results of the analysis provide a framework for identifying the regions in coefficient-space where robust stability can be guaranteed for the time-variant system under restricted parameter perturbations. This study contributes to the understanding of the stability properties of linear time-variant systems and provides insights into the analysis of difference equations.",1
1807,suppose rate of change of coecients of a linear time-variant system modeled via a dierence equation is restricted . the work presented herein is an attempt at developing an algorithm that determines regions in coe-cient-space where such a linear time-variant system is guaranteed to be globally asymptotically stable . such information can be extremely useful in many applications . some previously published related results are consolidated as well .,0
1808,"This paper presents a novel approach to beat tracking in the presence of highly predominant vocals by using voice suppression algorithms. The aim of the proposed method is to improve the accuracy of beat tracking estimations in music signals that have a high level of vocal activity. The approach involves applying low-pass filtering and generic voice suppression techniques to the audio, and then combining pairwise combinations of beat trackers with voice suppression methods. The evaluation of the proposed approach is carried out on a generic annotated collection of song excerpts with highly predominant vocals. The results show that the use of voice suppression algorithms improves the accuracy of beat tracking estimations in the presence of highly predominant vocals. This study contributes to the advancement of beat tracking research and provides insights into the use of voice suppression methods to improve beat tracking in challenging audio conditions.",1
1809,"beat tracking estimation from music signals becomes difficult in the presence of highly predominant vocals . we compare the performance of five state-of-the-art algorithms on two datasets , a generic annotated collection and a dataset comprised of song excerpts with highly predominant vocals . then , we use seven state-of-the-art audio voice suppression techniques and a simple low pass filter to improve beat tracking estimations in the later case . finally , we evaluate all the pairwise combinations between beat tracking and voice suppression methods . we confirm our hypothesis that voice suppression improves the mean performance of beat trackers for the predominant vocal collection .",0
1810,"This paper proposes a zero-resource approach to audio-only spoken term detection based on a combination of template matching techniques. The aim of the proposed method is to detect spoken term queries without the need for any linguistic resources or knowledge of the target language. The approach involves using French and English phonetic posteriorgrams as the basis for training speech templates, which are then used to detect the presence of known query words of interest. The method also incorporates raw MFCC features and Gaussian posteriorgrams for robustness and to handle speech variability. Dynamic time warping and self-similarity matrix comparison are used in combination with template matching to improve the accuracy of spoken term detection. The evaluation of the proposed approach is carried out on a dataset of spoken term queries, and the results show that the method achieves good performance in the absence of linguistic resources. This study contributes to the advancement of information retrieval tasks and provides insights into the use of template matching techniques for spoken term detection in audio-only scenarios.",1
1811,"spoken term detection is a well-known information retrieval task that seeks to extract contentful information from audio by locating occurrences of known query words of interest . this paper describes a zero-resource approach to such spoken term detection based on pattern matching of spoken term queries at the acoustic level . the template matching module comprises the cascade of a segmental variant of dynamic time warping and a self-similarity matrix comparison to further improve robustness to speech variability . this zero-resource approach notably differs from more traditional train and test methods that , while shown to be very accurate , rely upon the availability of large amounts of linguistic resources . we evaluate our zero-resource approach on different param-eterizations of the speech templates : raw mfcc features and gaussian posteriorgrams , french and english phonetic posteri-orgrams output by two different state of the art phoneme recog-nizers .",0
1812,"This paper introduces the concept of Collective AI, which is a framework for achieving context awareness in microrobot swarms via communication. The Collective AI framework utilizes local communication mechanisms and hardware to enable real microrobotic swarms to exchange information content with each other in order to achieve collective navigation. The paper presents a detailed description of the local communication mechanisms used in the Collective AI framework, and explains how they are used to enable microrobotic swarms to collectively navigate in complex environments. The authors also discuss the advantages of using the Collective AI approach, which include improved scalability, robustness, and adaptability. The evaluation of the proposed approach is carried out on a set of experiments involving microrobot swarms, and the results show that the Collective AI approach can significantly improve the performance of microrobotic swarms in navigation tasks. This study represents a significant contribution to the field of swarm robotics, and highlights the potential of Collective AI for enabling context awareness and collective decision-making in microrobotic swarms.",1
1813,"communication among participants -lrb- agents , robots -rrb- is central to an appearance of collective ai . in this work we deal with the development of local communication mechanisms for real microro-botic swarms . we demonstrate that despite of very limited capabilities of the microrobot , the specific construction of communication hardware and software allows very extended collective capabilities of the whole swarm . we propose local communication mechanisms providing information content and context for collective navigation , coordination and spatial perception in a group of microrobots .",0
1814,"This paper proposes a new method called Robust Fisher Discriminant Analysis (RFDA) to address the sensitivity problem of Fisher Linear Discriminant Analysis (FLDA) to data uncertainty. RFDA is based on a convex uncertainty model that can handle high-dimensional feature spaces, and it employs a robust kernel to enhance the performance of FLDA in the presence of data outliers. The proposed method can be formulated in a product form of convex optimization problems, which can be solved efficiently using off-the-shelf solvers. The effectiveness of the proposed method is demonstrated through extensive experiments on several benchmark datasets. The results show that RFDA outperforms several state-of-the-art methods in terms of classification accuracy and robustness to data uncertainty.",1
1815,"fisher linear discriminant analysis -lrb- lda -rrb- can be sensitive to the problem data . robust fisher lda can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst-case scenario under this model . the main contribution of this paper is show that with general convex uncertainty models on the problem data , robust fisher linear discriminant analysis can be carried out using convex optimization . for a certain type of product form uncertainty model , robust fisher linear discriminant analysis can be carried out at a cost comparable to standard fisher linear discriminant analysis . the method is demonstrated with some numerical examples . finally , we show how to extend these results to robust kernel fisher discriminant analysis , i.e. , robust fisher linear discriminant analysis in a high dimensional feature space .",0
1816,This paper proposes a method for reducing the dimensionality of spectrograms by imposing independence constraints. The method is based on regularized non-negative matrix factorization (NMF) and solves a non-square matrix factorization problem with a regularization term that enforces independence constraints. The proposed method is compared with non-negative independent component analysis (ICA) algorithms for observation streams. The results show that the proposed method achieves a lower reconstruction error and produces more compact representations. The low-dimensional decomposition provided by the proposed method can be useful for various applications that require a compressed representation of the original data.,1
1817,"we present an algorithm to find a low-dimensional decomposition of a spectrogram by formulating low-dimensional decomposition as a regularized non-negative matrix factorization problem with a regularization term chosen to encourage independence . this algorithm provides a better decomposition than standard nmf when the underlying sources are independent . it is directly applicable to non-square matrices , and it makes better use of additional observation streams than previous nonnegative ica algorithms .",0
1818,"This paper addresses the challenge of achieving speaker- and language-independent speech recognition in mobile communication systems. Traditional approaches rely on speaker-dependent and language-dependent models, which are not feasible for mobile devices with limited resources. To overcome this, the authors propose a system based on embedded multilingual speech recognition, language identification, and sub-optimal on-line text-to-phoneme mapping. The proposed system uses acoustic model adaptation techniques and sparse implementation resources to achieve speaker independence and recognition accuracy. The authors evaluate the system using dynamic vocabularies and show that it can achieve high recognition rates for multilingual acoustic models. The paper highlights the importance of automatic language identification and on-line pronunciation modeling for achieving language-independent ASR in logistic difficulties.",1
1819,"in this paper , we investigate the technical challenges that are faced when making a transition from the speaker-dependent to speaker-independent speech recognition technology in mobile communication devices . due to globalization as well as the international nature of the markets and the future applications , speaker independence implies the development and use of language-independent asr to avoid logistic difficulties . we propose here an architecture for embedded multilingual speech recognition systems . multilingual acoustic modeling , automatic language identification , and on-line pronunciation modeling are the key features which enable the creation of truly language-and speaker-independent asr applications with dynamic vocabularies and sparse implementation resources . our experimental results confirm the viability of the proposed architecture . while the use of multilingual acoustic models degrades the recognition rates only marginally , a recognition accuracy decrease of approximately 4 % is observed due to sub-optimal on-line text-to-phoneme mapping and automatic language identification . this performance loss can nevertheless be compensated by applying acoustic model adaptation techniques .",0
1820,"The paper proposes a statistical theory for estimating motion and structure parameters of planar surfaces by coupling rotation and translation. The estimation is based on the assumption of Gaussian noise and an unknown motion field, and takes into account the viewing direction, translation magnitude, and measurement noise. The proposed method provides an unbiased estimator and uncertainty bounds for the perceived plane, as well as the error sensitivity and covariance of the estimated parameters. The paper also explores the coupling between motion and geometry configuration, and derives a lower-bound-matrix for the estimation error. Overall, the proposed method offers a new approach for motion estimation in planar surfaces that takes into account both translation and rotation.",1
1821,"this paper studies the error sensitivity in the estimation of the 3d-motion and the normal of a planar surface from an instantaneous motion eld . we use the statistical theory of the cramer-rao lower bound for the error co-variance in the estimated motion and structure p arameters which enables the derivation of results valid for any unbi-ased estimator under the assumption of gaussian noise in the motion eld . the obtained lower-bound-matrix is studied analytically with respect to the measurement noise , size of the eld of view and the motion-geometry connguration . the main result of this analysis is the coupling between translation and rotation which is exacerbated if the eld of view and the slant of the plane become smaller and the deviation of the translation from the viewing direction becomes larger . by-products of this study are the relationships o f the uncertainty bounds for every unknown motion parameter to the angle between translation and the plane-normal , the size of the eld of view , the distance f r om the perceived plane and the translation magnitude .",0
1822,"This paper presents HARMONET, a neural network designed to harmonize chorales in the style of J.S. Bach. The goal is to solve a real-world problem in music processing by providing a powerful tool that captures musically relevant information. HARMONET is built using a hierarchical system of connectionist networks that uses a coding scheme for one-part melody and produces error backpropagation for four-part chorales. The system leverages the power of backpropagation and symbolic algorithms to train on Bach chorales and generate new harmonizations. The paper demonstrates the effectiveness of HARMONET's approach and its ability to produce harmonizations that sound musically plausible. The study highlights the potential of connectionist networks in music processing and the use of coding schemes for encoding musical practices into neural networks.",1
1823,"harmonet , a harmonet employing connectionist networks for music processing , is presented . after being trained on some dozen bach chorales using error backpropagation , the harmonet is capable of producing four-part chorales in the style of j . s.bach , given a one-part melody . our harmonet solves a musical real-world problem on a performance level appropriate for musical practice . harmonet 's power is based on -lrb- a -rrb- a new coding scheme capturing musically relevant information and -lrb- b -rrb- the integration of backpropagation and symbolic algorithms in a hierarchical system , combining the advantages of both .",0
1824,"This paper presents a method for large-scale video understanding by harnessing object and scene semantics. The proposed approach utilizes a fusion network to combine object and scene-based semantic representations of videos, incorporating semantic relationships and correlations between them. The network consists of a large-scale CNN object-detector and a CNN scene-detector, followed by a three-layer neural network for semantic fusion. The semantic representation is used for various video understanding tasks such as video categorization, large-scale action recognition, and zero-shot action/video classification. The method is evaluated on the large-scale datasets-ActivityNet and achieves state-of-the-art results in supervised activity clustering and video categorization tasks. The results demonstrate the power of the proposed method in utilizing semantic relationships between objects and scenes for large-scale video understanding.",1
1825,"large-scale action recognition and video categorization are important problems in computer vision . to address these problems , we propose a novel object-and scene-based semantic fusion network and representation . our object-and scene-based semantic fusion network combines three streams of information using a three-layer neural network : -lrb- i -rrb- frame-based low-level cnn features , -lrb- ii -rrb- object features from a state-of-the-art large-scale cnn object-detector trained to recognize 20k classes , and -lrb- iii -rrb- scene features from a state-of-the-art cnn scene-detector trained to recognize 205 scenes . the trained object-and scene-based semantic fusion network achieves improvements in supervised activity and video categorization in two complex large-scale datasets-activitynet and fcvid , respectively . further , by examining and back propagating information through the fusion network , semantic relationships -lrb- correlations -rrb- between video classes and objects/scenes can be discovered . these video class-object/video class-scene relationships can in turn be used as semantic representation for the video classes themselves . we illustrate effectiveness of this semantic representation through experiments on zero-shot action/video classification and clustering .",0
1826,This paper proposes a method for automatic data-driven learning of articulatory primitives from real-time MRI data using convolutive NMF with sparseness constraints. The proposed method is aimed at the gesture-based articulatory phonology framework and the articulatory recognition task. The method uses recently-acquired RT-MRI corpus data to learn inter-pretable dynamic articulatory primitives in a data-driven manner. The method works by applying sparseness constraints to the activation matrix of convolutive NMF to learn parameter values that correspond to specific articulatory primitives. The proposed method has been shown to be effective in learning articulatory primitives from real-time MRI data and has the potential to improve our understanding of the relationship between speech articulation and acoustic signals.,1
1827,"we present a procedure to automatically derive inter-pretable dynamic articulatory primitives in a data-driven manner from image sequences acquired through real-time magnetic resonance imaging . more specifically , we propose a convolutive nonnegative matrix factorization algorithm with sparseness constraints to decompose a given set of image sequences into a set of basis image sequences and an activation matrix . we use a recently-acquired rt-mri corpus of read speech -lrb- 460 sentences from 4 speakers -rrb- as a test dataset for this procedure . we choose the free parameters of the algorithm empirically by analyzing algorithm performance for different parameter values . we then validate the extracted basis sequences using an articulatory recognition task and finally present an interpretation of the extracted basis set of image sequences in a gesture-based articulatory phonology framework .",0
1828,"This paper introduces a novel approach to Multiple Instance Learning (MIL) called ""Manifold Bags"", which utilizes the geometric structure of data in a high dimensional feature space. MIL is a machine learning technique that learns from ""bags"" of instances, where each bag contains a set of instances, but the labels are only assigned to the bags, not the individual instances. In Manifold Bags, the finite sized bags are assumed to lie on a low dimensional manifold, which reduces the memory requirements of MIL problems. The approach is demonstrated on both image and audio data, and a heuristic for PAC-learnability is provided. The results show that Manifold Bags outperform traditional MIL methods on real-world data.",1
1829,"in many machine learning applications , labeling every instance of data is burdensome . multiple instance learning , in which training data is provided in the form of labeled bags rather than labeled instances , is one approach for a more relaxed form of supervised learning . though much progress has been made in analyzing mil problems , existing work considers bags that have a finite number of instances . in this paper we argue that in many applications of multiple instance learning -lrb- e.g. image , audio , etc. -rrb- the bags are better modeled as low dimensional manifolds in high dimensional feature space . we show that the geometric structure of such manifold bags affects pac-learnability . we discuss how a learning algorithm that is designed for finite sized bags can be adapted to learn from mani-fold bags . furthermore , we propose a simple heuristic that reduces the memory requirements of such algorithms . our experiments on real-world data validate our analysis and show that our approach works well .",0
1830,"This paper proposes a correspondence driven adaptation approach for human profile recognition, which is based on a convolutional neural network (CNN) system for gender and age estimation. The proposed method leverages incremental stochastic training and weak supervision of correspondences to improve estimation accuracy. The system is evaluated on video datasets and the FG-NET database, and the results show that the proposed method outperforms existing methods in recognizing human gender and age from successive frames of face images in a real-world environment. The paper highlights the importance of statistical learning models and visual recognition models, and demonstrates how object correspondences can be used to improve the performance of visual recognition systems in real-world scenarios.",1
1831,"visual recognition systems for videos using statistical learning models often show degraded performance when being deployed to a real-world environment , primarily due to the fact that training data can hardly cover sufficient variations in reality . to alleviate this issue , we propose to utilize the object correspondences in successive frames as weak supervision to adapt visual recognition models , which is particularly suitable for human profile recognition . specifically , we substantialize this new strategy on an advanced convolutional neural network based system to estimate human gender , age , and race . we enforce the system to output consistent and stable results on face images from the same trajectories in videos by using incremental stochastic training . our baseline system already achieves competitive performance on gender and age estimation as compared to the state-of-the-art algorithms on the fg-net database . further , on two new video datasets containing about 900 persons , the proposed supervision of correspondences improves the estimation accuracy by a large margin over the baseline .",0
1832,"This paper presents a modified composition algorithm for large-vocabulary speech recognition tasks, designed for the Google Android platform. The algorithm is based on finite-state transducers and takes into account a fine-grained trade-off between context-dependent phones and context-dependent lexicon. The proposed approach uses a pre-initialized composition of recognition transducer and language model, which improves the decoding performance by reducing the computation time required for decoding. Experimental results show that the proposed method achieves a significant improvement in recognition accuracy over existing approaches for large-vocabulary speech recognition on video datasets. The context-dependent lexicon and language model are shown to play a critical role in achieving this performance improvement.",1
1833,"this paper describes a modified composition algorithm that is used for combining two finite-state transducers , representing the context-dependent lexicon and the language model respectively , in large vocabulary speech recogntion . this modified composition algorithm is a hybrid between the static and dynamic expansion of the resultant transducer , which maps from context-dependent phones to words and is searched during decoding . the modified composition algorithm is to pre-compute part of the recognition transducer and leave the balance to be expanded during decoding . this modified composition algorithm allows for a fine-grained trade-off between space and time in recognition . for example , the time overhead of purely dynamic expansion can be reduced by over six-fold with only a 20 % increase in memory in a collection of large-vocabulary recognition tasks available on the google android platform .",0
1834,"This paper proposes a vanishing point-based global image descriptor for Manhattan scenes. The descriptor is compact and viewpoint-invariant, making it suitable for object matching in challenging scenarios such as occlusion, image distortions, cropping, and variations in illumination. The proposed method aggregates image statistics based on relative locations of local maxima in scale-displacement plots, edge strengths, and edge shapes, leveraging the scene geometry. The discriminative ability of the descriptor is evaluated on the Zurich buildings database, achieving low error rates. The results demonstrate the effectiveness of the proposed method in capturing the distinctive features of Manhattan scenes.",1
1835,"viewpoint-invariant object matching is challenging due to image distortions caused by several factors such as rotation , translation , illumination , cropping and occlusion . we propose a compact , global image descriptor for manhattan scenes that captures relative locations and strengths of edges along vanishing directions . to construct the descriptor , an edge map is determined per vanishing point , capturing the edge strengths over a range of angles measured at the vanishing point . for matching , descriptors from two scenes are compared across multiple candidate scales and displacements . the matching performance is refined by comparing edge shapes at the local maxima of the scale-displacement plots . the proposed global image descriptor achieves an equal error rate of 7 % for the zurich buildings database , indicating significant gains in discriminative ability over other global descriptors that rely on aggregate image statistics but do not exploit the underlying scene geometry .",0
1836,"This paper presents a Lorentzian-based Iterative Hard Thresholding (LIHT) algorithm for compressed sensing, which aims to reconstruct sparse signals in impulsive environments with impulsive noise. LIHT uses a robust iterative hard thresholding algorithm with a Lorentzian cost function to achieve better reconstruction quality than the traditional Iterative Hard Thresholding (IHT) algorithm. The proposed LIHT algorithm can handle heavy-tailed models and has better robustness to impulsive noise than IHT. The computational load of LIHT is similar to IHT, and it is evaluated and compared with other sparse reconstruction techniques in terms of robustness and reconstruction quality. The proposed LIHT algorithm is expected to provide a better solution for compressed sensing problems in impulsive environments.",1
1837,"in this paper we propose a robust iterative hard thresolding algorithm for reconstructing sparse signals in the presence of impulsive noise . to address this problem , we use a lorentzian cost function instead of the í µí ° ¿ 2 cost function employed by the traditional iht algorithm . the derived robust iterative hard thresolding algorithm is comparable in computational load to the least squares based iht . analysis of the proposed robust iterative hard thresolding algorithm demonstrates its robustness under heavy-tailed models . simulations show that the proposed robust iterative hard thresolding algorithm significantly outperform commonly employed sparse reconstruction techniques in impulsive environments , while providing comparable reconstruction quality in less demanding , light-tailed environments .",0
1838,"This paper analyzes the bit error probability of direct-sequence code division multiple access (CDMA) multiuser demodulators in the presence of additive Gaussian noise. The demodulators are based on the marginal posterior mode (MPM) and are used to decode binary phase-shift keying (BPSK) signals transmitted over a CDMA channel. The paper presents a mean-field approximation to the demodulator, and uses a replica analysis to study its performance. The analysis is based on an analog-valued Hopfield model and considers the finite-temperature decoding problem. The results show that the MPM demodulator has good performance at low noise levels and high information bit rates, and that it is a useful tool for multiuser demodulation in CDMA systems.",1
1839,"we analyze the bit error probability of multiuser demodulators for direct-sequence binary phase-shift-keying cdma channel with additive gaussian noise . the problem of multiuser demodulators is cast into the finite-temperature decoding problem , and replica analysis is applied to evaluate the performance of the resulting mpm -lrb- marginal posterior mode -rrb- demodulators , which include the optimal demodulator and the map demodulator as special cases . an approximate implementation of multiuser demodulators is proposed using analog-valued hopfield model as a naive mean-field approximation to the mpm -lrb- marginal posterior mode -rrb- demodulators , and its performance is also evaluated by the replica analysis . results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one , especially in the cases of small information bit rate and low noise level .",0
1840,"This paper proposes a new method for affine-invariant image classification tasks, called the Multiresolution Tangent Distance (MTD). The MTD is a similarity metric that can estimate the affine transformation between images, and is robust to local minima. The proposed method is based on a multiresolution setting, which allows it to perform well on images with different levels of detail. The method is evaluated on face and character recognition tasks and compared with other state-of-the-art methods. The results show that MTD outperforms existing methods in terms of accuracy and robustness to image transformations. The proposed method can be used in various applications that require robust estimation procedures, such as object recognition and computer vision.",1
1841,"the ability to rely on similarity metrics invariant to image transformations is an important issue for image classification tasks such as face or character recognition . we analyze an invariant metric that has performed well for the latter-the tangent distance-and study its limitations when applied to regular images , showing that the most significant among these -lrb- convergence to local minima -rrb- can be drastically reduced by computing the distance in a multiresolution setting . this leads to the multiresolution tangent distance , which exhibits significantly higher invariance to image transformations , and can be easily combined with robust estimation procedures .",0
1842,"This paper proposes the use of Non-negative Matrix Factorization (NMF) for visual coding. Specifically, the authors apply NMF with sparseness constraints to training face data from MIT-CBCL and show that it can provide a sparser representation than linear sparse coding. They also demonstrate that the use of generalized Kullback-Leibler divergence can provide a better approximation error than mean square error. The paper concludes that NMF is a useful technique for visual coding as it can provide a parts-based representation and can be optimized using multiplicative updates.",1
1843,"this paper combines linear sparse coding and non-negative matrix factorization into sparse non-negative matrix factorization . in contrast to non-negative matrix factorization , the new model can leam much sparser representation via imposing sparseness constraints explicitly ; in contrast to a close model-non-negative sparse coding , the new model can learn parts-based representation via fully multiplicative updates because of adapting a generalized kullback-leibler divergence instead of the conventional mean square error for approximation error . experiments on mit-cbcl training faces data demonstrate the effectiveness of the proposed method .",0
1844,"This paper presents an analysis of the Map Seeking Circuit (MSC) and its Monte Carlo extensions. The MSC is a model of parallel search in biological vision that can perform transformation discovery and ordering properties of superpositions. The paper investigates the resolution of parameter estimates and resource-constrained implementations of the MSC. Additionally, the authors explore Monte Carlo approaches to the MSC, which can be used for high-dimensional problems. The paper concludes with a discussion of natural tasks and collusions in signal processing and vision, such as inverse kinematics. Overall, this work provides a comprehensive understanding of the MSC and its extensions, and their potential applications in various domains.",1
1845,"the map seeking circuit has been suggested to address the inverse problem of transformation discovery as found in signal processing , vision , inverse kinematics and many other natural tasks . according to this idea , a parallel search in the transformation space of a high dimensional problem can be decomposed into parts efficiently using the ordering property of superpositions . deterministic formulations of the circuit have been suggested . here , we provide a proba-bilistic interpretation of the architecture whereby the superpositions of the circuit are seen as a series of marginalisations over parameters of the transform . based on this , we interpret the weights of the map seeking circuit as importance weights . the latter suggests the incorporation of monte-carlo approaches in the map seeking circuit , providing improved resolution of parameter estimates within resource constrained implementations . as a final contribution , we model mixed serial/parallel search strategies of biological vision to reduce the problem of collusions , a common problem in the standard map seeking circuit .",0
1846,"This paper presents the design of a high order binaural microphone array for hearing aids using a rigid spherical model. Binaural hearing aids with bilateral arrays are widely used to improve speech intelligibility. The proposed design uses artificial head and torso with a spherical head model to create free-field/spherical models. A speech-intelligibility weighted directivity index is used to evaluate the array's performance. The study compares the proposed spherical model with a free-field model and shows that the spherical model can provide better directional performance. Additionally, the paper discusses the use of wireless technology in microphone arrays for hearing aids. The proposed design has potential for improving the performance of binaural hearing aids.",1
1847,"wireless technology has allowed for a much wider variety in the design of microphone arrays for binaural hearing aids . to facilitate the design of these microphone arrays , this paper investigates the use of a spherical head model in the design of bilateral and binaural microphone arrays for hearing aids . the microphone arrays have been designed using a free-field model , a spherical head model , measurements on an artificial head , and measurements on an artificial head + torso . the results show that the free-field/spherical models overestimate the speech-intelligibility weighted directivity index of the bilateral and binaural arrays by respectively 0.9 / 0.4 and 0.8 / 0.5 db . furthermore the weights designed with the free-field/spherical model yield an sii-di that is 0.7 / 0.6 db lower for bilateral arrays and 0.9 / 0.9 db lower for binaural arrays than the optimal sii-di . although the results show that the spherical head model is better in predicting the di than the free-field model , the spherical head model does not design better weights .",0
1848,"This paper proposes a new approach called Multi-Conditional Learning for generative/discriminative training in clustering and classification tasks. The method involves combining generative topic models such as Latent Dirichlet Allocation and Exponential Family Harmonium with discriminative classifiers such as Conditional Random Fields and Logistic Regression. The regularization and training criterion are designed to take into account both the generative and discriminative aspects of the model. The latent structure and latent variables are also considered in the model. The proposed approach is evaluated on text datasets and achieves good results in terms of precision, recall, accuracy, and robustness. The paper concludes that Multi-Conditional Learning can be a useful approach for tasks that require both generative and discriminative learning.",1
1849,"this paper presents multi-conditional learning , a training criterion based on a product of multiple conditional likelihoods . when combining the traditional conditional probability of '' label given input '' with a generative probability of '' input given label '' the later acts as a surprisingly effective regularizer . when applied to models with latent variables , multi-conditional learning combines the structure-discovery capabilities of generative topic models , such as latent dirichlet allocation and the exponential family harmonium , with the accuracy and robustness of discriminative classifiers , such as logistic regression and conditional random fields . we present results on several standard text data sets showing significant reductions in classification error due to mcl regularization , and substantial gains in precision and recall due to the latent structure discovered under multi-conditional learning .",0
1850,"This paper presents a multilinear generalization of the Common Spatial Pattern (CSP) algorithm, which is widely used in brain-computer interfaces (BCIs) for EEG classification. The proposed method enables simultaneous optimization of projection matrices in high-order tensor data, improving classification accuracy for multi-class motor imagery EEG. The multilinear formulation is based on tensor analysis theory and optimization criteria, and it is shown to outperform the original CSP algorithm. The classification accuracy of the proposed method is evaluated using EEG data, and the results demonstrate the effectiveness of the multilinear formulation in improving the performance of the CSP algorithm for BCI applications.",1
1851,"the common spatial patterns algorithm has been widely used in eeg classification and brain computer interface . in this paper , we propose a multilinear formulation of the common spatial patterns algorithm , termed as tensorcsp or common tensor discriminant analysis -lrb- ctda -rrb- for high-order tensor data . as a natural extension of common spatial patterns algorithm , the proposed multilinear formulation uses the analogous optimization criteria in common spatial patterns algorithm and a new framework for simultaneous optimization of projection matrices on each mode based on tensor analysis theory is developed . experimental results demonstrate that our proposed multilinear formulation is able to improve classification accuracy of multi-class motor imagery eeg .",0
1852,"This paper proposes a framework for active task selection in the context of lifelong machine learning. The goal is to optimize the learning time and accuracy by selecting the most relevant tasks to the current learning stage. The proposed framework includes lifelong learning algorithms, batch multi-task learning methods, curriculum selection methods, and transfer learning techniques. By integrating active task selection, the lifelong learning system can acquire knowledge efficiently and effectively. The paper discusses the challenges and opportunities of active task selection and provides experimental results on various learning tasks to demonstrate the effectiveness of the proposed framework.",1
1853,"in a lifelong learning framework , an lifelong learning framework acquires knowledge incrementally over consecutive learning tasks , continually building upon its experience . recent lifelong learning algorithms have achieved nearly identical performance to batch multi-task learning methods while reducing learning time by three orders of magnitude . in this paper , we further improve the scalability of lifelong learning by developing curriculum selection methods that enable an lifelong learning framework to actively select the next task to learn in order to maximize performance on future learning tasks . we demonstrate that active task selection is highly reliable and effective , allowing an lifelong learning framework to learn high performance models using up to 50 % fewer tasks than when the lifelong learning framework has no control over the task order . we also explore a variant of transfer learning in the lifelong learning setting in which the lifelong learning framework can focus knowledge acquisition toward a particular target task .",0
1854,"This paper presents a method for retrieving soccer highlights from real soccer videos using an adaptive time-frequency representation. The proposed method involves an automatic annotation of events in the soccer games, followed by a feature extraction procedure based on adaptive time-frequency decomposition. The feature extraction stage is performed using a matching pursuit concept. The resulting feature vectors are then used in a classification stage to match the soccer video to a multimedia database management system. The method is evaluated on real soccer videos and shows promising results for the retrieval of soccer highlights based on audio soundtrack.",1
1855,"the retrieval of soccer highlights is a suitable technique for video indexing , required by the multimedia database management or for the development of television on demand . for these purposes , it should be interesting to have an automatic annotation of events happened in soccer games . one solution consists in analyzing the audio soundtrack associated to the soccer video and to detect the interesting frames . in this paper we use the adaptive time-frequency decomposition of the soundtrack as a feature extraction procedure . this decomposition is based on the matching pursuit concept and a dictionary composed of gabor functions . the parameters provided by these transformations constitute the input of the classification stage . the results provided for real soccer video will prove the efficiency of the adaptive time-frequency representation as a feature extraction stage .",0
1856,This paper presents an evaluation of objective measures for quality assessment of reverberant speech. The study compares various objective quality measures with subjective rating scales to determine the effectiveness of the former in assessing the overall speech quality of reverberant speech. The evaluation focuses on the impact of the reverberation tail effect and speech coloration on the quality of speech. The study also considers the use of dereverberation algorithms and PESQ-based measures for objective quality assessment. The results of the evaluation provide insights into the efficacy of different objective quality measures in evaluating the quality of reverberant speech.,1
1857,"in this paper , we evaluate the performance of existing and new objective measures in terms of predicting the quality of reverberant speech and speech enhanced by dereverberation algorithms . we use subjective quality ratings designed to evaluate the quality of speech along three dimensions : speech coloration , reverberation tail effect and overall speech quality . experimental results assess the correlations between the proposed objective quality measures and the three subjective rating scales and suggest that pesq-based measures can very reliably predict the quality of reverberant and dereverberated speech .",0
1858,"This paper presents a new nonlinear filtering method using kriging, with an application to system inversion. The proposed approach involves the use of parametric behavioral models and prediction methods to estimate the system state. The focus is on nonlinear systems where traditional linear filtering methods may not be effective. The study shows the effectiveness of the proposed approach through various experiments and evaluations. The results indicate that the use of kriging for nonlinear filtering improves the accuracy of predictions and provides a better estimate of the system state. Overall, the study demonstrates the potential of the proposed approach for various applications, including system inversion and prediction.",1
1859,"prediction by kriging does not rely on any specific model structure , and is thus much more flexible than approaches based on parametric behavioural models . since accurate predictions are obtained for extremely short training sequences , it generally performs better than prediction methods using parametric models . application to nonlinear system inversion is considered",0
1860,"This paper discusses the use of graph cuts in energy minimization problems. Specifically, the focus is on algorithms that employ pairwise and triplewise pixel interactions, as well as k-wise pixel interactions. The paper describes an algebraic approach to constructing approximate algorithms, and it evaluates the performance of several different graph cut-based methods. Ultimately, the paper aims to settle the question of what is possible when using graph cuts for energy minimization in computer vision applications.",1
1861,"the recent explosion of interest in graph cut methods in computer vision naturally spawns the question : what energy functions can be minimized via graph cuts ? this question was first attacked by two papers of kolmogorov and zabih -lsb- 23 , 24 -rsb- , in which they dealt with functions with pair-wise and triplewise pixel interactions . in this work , we extend their results in two directions . first , we examine the case of k-wise pixel interactions ; the results are derived from a purely algebraic approach . second , we discuss the applicability of provably approximate algorithms . both of these developments should help researchers best understand what can and can not be achieved when designing graph cut based algorithms .",0
1862,"This paper investigates the concepts of sampling and quantization using a digital storage oscilloscope (DSO). The focus is on aliasing, which occurs when the sampling rate is too low, and quantization, which refers to the number of bits used to represent each sample. The authors present a laboratory exercise that explores these concepts and discuss the display capabilities of the DSO, which allow for visual inspection of the effects of different sampling rates and quantization levels on the signal. The paper highlights the importance of understanding these concepts in signal processing and provides a practical and hands-on approach for teaching them.",1
1863,"the objective of this paper is to remforce students understanding of sampling through experimentation . the scope of this paper is very narrow , focusing on a single laboratory exercise for learning about sampling , which is a critically important topic for students to comprehend . the laboratory exercise makes an ideal platform for studying sampling , aliasing , and quantization , because not only does the laboratory exercise have a built in a/d converter and all the supporting electronics , but laboratory exercise has the display capabilities in both the time and frequency domains . additionally , students obtain a better understanding of test and measurement equipment such as the laboratory exercise .",0
1864,"In this paper, the authors propose a new algorithm called Sparse Variable PCA (SVP-PCA) for performing principal component analysis with an L1 penalty on the loadings vector. The algorithm is based on steepest descent on a Grassman manifold and aims to achieve sparsity in the loadings vector. The authors compare their algorithm to the vector L1 penalized PCA criterion and show that it outperforms this criterion in terms of both sparsity and reconstruction error. They also show that",1
1865,recently there has developed considerable interest in using sparse-ness with pca or-thogonality . almost all previous methods concentrate on zeroing out some loadings . here we develop a new approach which zeros out whole variables automatically . we formulate a vector l1 penalized pca criterion and optimize vector l1 penalized pca criterion by steepest descent along geodesic on a grassman manifold . this ensures that each step obeys pca or-thogonality as well as an invariance property of the criterion . we show in simulations that vector l1 penalized pca criterion outperforms a previous svpca algorithm and apply vector l1 penalized pca criterion to a real high dimensional functional magnetic resonance imaging -lrb- fmri -rrb- data .,0
1866,"This paper investigates the application of Meddis' inner hair-cell model to predict subjective speech quality. The study uses mean opinion scores (MOS) as a measure of subjective speech quality and instrumental speech-quality measure as an objective measure. Additionally, the study examines the relationship between auditory-nerve ring-patterns and instrumental speech-quality measure using the auditory-nerve ring-patterns as a predictor variable. The results suggest that the inner hair-cell model can be used to predict subjective speech quality with a high degree of accuracy. Furthermore, the study provides evidence that the auditory-nerve ring-patterns are useful in predicting instrumental speech-quality measure. Overall, this research has important implications for improving speech quality in various applications.",1
1867,this paper demonstrates how an instrumental speech-quality measure based on the comparison of auditory-nerve ring-patterns can be constructed . four available subjective tests prove that the mean opinion scores mos estimated by the objective measure are in good agreement with the subjectively obtained results .,0
1868,"This paper presents a novel approach for efficiently partitioning Boolean CNF formulas into two sets: the Maximal Satisfiable Subset (MSS) and the Minimal Correction Subset (CoMSS). The MSS represents the largest possible subset of clauses that can be satisfied, while the CoMSS represents the smallest set of clauses that need to be modified to satisfy the formula. The proposed method is experimentally efficient and outperforms previous approaches. The approach is based on artificial intelligence (AI) techniques and specifically focuses on the use of CoMSS to improve performance. The results show that the approach is effective in reducing the search space for MSS and CoMSS and can lead to significant improvements in solving Boolean CNF formulas. The approach has wide-ranging applications in various fields that rely on Boolean formulas, including hardware verification and software testing.",1
1869,"the concepts of mss -lrb- maximal satisfiable subset -rrb- and comss -lrb- also called minimal correction subset -rrb- play a key role in many a.i. approaches and techniques . in this paper , a novel algorithm for partitioning a boolean cnf formula into one minimal correction subset -rrb- and the corresponding comss is introduced . extensive empirical evaluation shows that it is more robust and more efficient on most instances than currently available techniques .",0
1870,"This paper proposes an unsupervised language model adaptation method for automatic speech recognition (ASR) of French broadcast news using web 2.0. The approach involves an un-supervised text collection and decoding strategy that extracts time-and topic-relevant text data from French broadcast news shows using a tf-idf-based topic words extraction technique. The extracted text data is then normalized and used to develop a language model using the Rapid Language Adaptation Toolkit. The language model is interpolated with an existing language model to improve the ASR word error rates. The approach is evaluated using the Quaero project RSS feeds and Twitter data. The results show that the proposed method outperforms the baseline system and achieves significant improvements in ASR word error rates. The study provides evidence that web 2.0 can be a valuable resource for language model adaptation in ASR systems. Overall, the proposed method has important implications for the development of ASR systems for French broadcast news and can be applied to other languages and domains as well.",1
1871,"we improve the automatic speech recognition of broadcast news using paradigms from web 2.0 to obtain time-and topic-relevant text data for language modeling . we elaborate an un-supervised text collection and decoding strategy that includes crawling appropriate texts from rss feeds , complementing un-supervised text collection and decoding strategy with texts from twitter , language model and vocabulary adaptation , as well as a 2-pass decoding . the word error rates of the tested french broadcast news shows from europe 1 are reduced by almost 32 % relative with an underlying language model from the globalphone project -lsb- 1 -rsb- and by almost 4 % with an underlying language model from the quaero project . the un-supervised text collection and decoding strategy that we use for the text normalization , the collection of rss feeds together with the text on the related websites , a tf-idf-based topic words extraction , as well as the opportunity for language model interpolation are available in our rapid language adaptation toolkit -lsb- 2 -rsb- -lsb- 3 -rsb- .",0
1872,"This paper proposes a method for building deep dependency structures using a wide-coverage Combinatory Categorial Grammar (CCG) parser. The approach involves using a treebank of CCG normal-form derivations to extract local predicate-argument dependencies and build long-range dependency structures. The wide-coverage CCG parser is used to generate a wide-coverage statistical parser that can handle coordination, extraction, and control phenomena. The proposed method is compared to existing wide-coverage tree-bank parsers and shows improvements in accuracy and coverage. The study also compares labelled dependencies to unlabelled dependencies and shows that the former leads to better results. The results suggest that the proposed method can be used to generate deep dependency structures for various languages and domains. Overall, the study has important implications for natural language processing and can help improve the accuracy and coverage of dependency parsing.",1
1873,"this paper describes a wide-coverage statistical parser that uses combinatory categorial grammar to derive dependency structures . the wide-coverage statistical parser differs from most existing wide-coverage tree-bank parsers in capturing the long-range dependencies inherent in constructions such as coordination , extraction , raising and control , as well as the standard local predicate-argument dependencies . a set of dependency structures used for training and testing the wide-coverage statistical parser is obtained from a treebank of ccg normal-form derivations , which have been derived -lrb- semi - -rrb- automatically from the penn treebank . the wide-coverage statistical parser correctly recovers over 80 % of labelled dependencies , and around 90 % of unlabelled dependencies .",0
1874,"This paper presents an objective quality estimation method for wide-band speech using a narrow-band prior. The approach involves using a non-intrusive wide-band quality assessment algorithm as a baseline and incorporating a quality prior obtained from a subjectively labelled narrow-band database. The quality prior is used to estimate the narrow-band quality of the wide-band signal, which is then used to improve the objective models for wide-band quality assessment. The proposed method is compared to a baseline wide-band system and shows improvements in accuracy. The study demonstrates the importance of incorporating a narrow-band prior in wide-band quality assessment models and highlights the limitations of using objective models without considering subjective evaluations. The results suggest that the proposed method can be used to improve the quality of wide-band speech signals for various applications. Overall, the study has important implications for speech signal processing and can help improve the accuracy of wide-band quality assessment algorithms.",1
1875,a fundamental challenge in the design of objective models for estimation of speech signal quality lies in the shortage of subjectively labelled databases . this problem is particularly relevant when developing quality assessment models for wide-band -lrb- 16 khz sampling rate -rrb- signals where databases are scarce . we explore the possibility for seamlessly integrating a quality prior in the form of a narrow-band quality estimate into the framework of a non-intrusive wide-band quality assessment algorithm . experimental results confirm that the proposed non-intrusive wide-band quality assessment algorithm can be used to improve performance over a baseline wide-band system without a narrow-band prior .,0
1876,"This paper investigates the classification of clear and conversational speech based on acoustic features, specifically spectral and prosodic features. The study compares the predictive power of spectral cues and prosodic features in different speaking styles, including conversational and hyper-articulated styles. The study uses decision tree classifiers and multi-layer perceptrons to classify the speech samples. The results show that the decision tree classifiers achieve higher classification accuracies than the multi-layer perceptrons. Moreover, the study shows that prosodic features have a stronger predictive power in classifying conversational speech, while spectral cues perform better in classifying hyper-articulated speech. The findings suggest that the acoustic features can be used to differentiate between clear and conversational speech in various applications, including speech recognition and speaker identification. The study demonstrates the importance of considering different speaking styles in the classification of speech and highlights the potential of using prosodic features in the classification of conversational speech. Overall, the study contributes to the development of acoustic-based classification methods for speech analysis.",1
1877,"this paper reports an investigation of features relevant for classifying two speaking styles , namely , conversational speaking style and clear -lrb- e.g. hyper-articulated -rrb- speaking style . spectral and prosodic features were automatically extracted from speech and classified using decision tree classifiers and multi-layer perceptrons to achieve accuracies of about 71 % and 77 % respectively . more interestingly , we found that out of the 56 features only about 9 features are needed to capture the most predictive power . while perceptual studies have shown that spectral cues are more useful than prosodic features for intel-ligibility -lsb- 1 -rsb- , here we find prosodic features are more important for classification .",0
1878,"This paper presents an efficient task sub-delegation approach for crowdsourcing systems. The proposed approach is reputation-aware and employs reputation-based decision-making models for making sub-delegation decisions under resource-constrained conditions. It also leverages multi-agent trust networks to facilitate the delegation process. The approach is designed to address high workload conditions and uses intelligent agent-based techniques to manage worker reputation. The paper compares its reputation-aware approach with existing reputation-based approaches, and shows its effectiveness in improving the quality of task sub-delegation decisions. Results show that the proposed approach achieves higher classification accuracies, while reducing the workload on trustee agents. The approach is evaluated using a trust network derived from the Epinions dataset. Overall, the paper demonstrates that the proposed reputation-aware task sub-delegation approach is an effective means of addressing resource-constrained conditions in crowdsourcing systems.",1
1879,"reputation-based approaches allow a crowdsourcing system to identify reliable workers to whom tasks can be delegated . in crowdsourcing system that can be modeled as multi-agent trust networks consist of resource constrained trustee agents -lrb- i.e. , workers -rrb- , workers may need to further sub-delegate tasks to others if they determine that they can not complete all pending tasks before the stipulated deadlines . existing reputation-based decision-making models can not help workers decide when and to whom to sub-delegate tasks . in this paper , we proposed a reputation aware task sub-delegation approach to bridge this gap . by jointly considering a worker 's reputation , workload , the price of its effort and its trust relationships with others , reputation aware task sub-delegation approach can be implemented as an intelligent agent to help workers make sub-delegation decisions in a distributed manner . the resulting task allocation maximizes social welfare through efficient utilization of the collective capacity of a crowd , and provides provable performance guarantees . experimental comparisons with state-of-the-art approaches based on the epinions trust network demonstrate significant advantages of reputation aware task sub-delegation approach under high workload conditions .",0
1880,"This paper presents a study on improving the performance of speech recognition systems using small microphone arrays. The proposed approach uses missing data techniques to enhance the reliability of the input signal by masking unreliable frequency bands. The study compares the performance of a baseline missing data system and a microphone array enhancement system. Results show that the proposed approach significantly improves the accuracy of the decoded sequence, leading to better speech recognition performance of small microphone arrays. This study demonstrates the potential of missing data techniques for enhancing speech recognition systems, especially in resource-constrained scenarios where small microphone arrays are commonly used.",1
1881,"traditional microphone array speech recognition systems simply recognise the enhanced output of the array . as the level of signal enhancement depends on the number of microphones , such microphone array speech recognition systems do not achieve acceptable speech recognition performance for arrays having only a few microphones . for small microphone arrays , we instead propose using the enhanced output to estimate a reliability mask , which is then used in missing data speech recognition . in missing data speech recognition , the decoded sequence depends on the reliability mask of each input feature . this reliability mask is usually based on the signal to noise ratio in each frequency band . in this paper , we use the energy difference between the noisy input and the enhanced output of a small microphone array to determine the frequency band reliability . recognition experiments with a small array demonstrate the effectiveness of the technique , compared to both traditional microphone array enhancement and a baseline missing data system .",0
1882,"This paper proposes a novel method for vectorizing engineering drawings that involves dimension recognition and geometry reconstruction. The method aims to rectify deviations in entity dimensions and detect sets of dimensions in low quality drawings. It uses recognized dimension annotations and a coordinate grid structure, along with two-dimensional spatial constraints, to reconstruct dimension frames and symbols. Symbol recognition is also employed to improve the reconstruction algorithm. The proposed method is evaluated on scanned engineering drawings and shows promising results in terms of accuracy and efficiency.",1
1883,"this paper presents a novel approach for recognizing and interpreting dimensions in engineering drawings . it starts by detecting potential dimension frames , each comprising only the line and text components of a dimension , then verifies them by detecting the dimension symbols . by removing the prerequisite of symbol recognition from detection of dimension sets , our method is capable of handling low quality drawings . we also propose a reconstruction algorithm for rebuilding the drawing entities based on the recognized dimension annotations . a coordinate grid structure is introduced to represent and analyze two-dimensional spatial constraints between entities ; this simplifies and unifies the process of rectifying deviations of entity dimensions induced during scanning and vectorization .",0
1884,"This paper proposes a hidden semi-Markov model (HSMM) based approach for speech synthesis. HSMMs are used to model the probability distribution of state durations in a speech parameter vector sequence, which allows for more accurate modeling of rhythm and tempo in synthesized speech. The proposed HSMM-based speech synthesis system utilizes single Gaussian distributions to model state duration probability distributions. The system is trained on a set of sentences and can be used to synthesize speech for new sentences. Experimental results show that the HSMM-based approach outperforms traditional HMM-based speech synthesis systems.",1
1885,"in the present paper , a hidden-semi markov model -lrb- hsmm training -rrb- based speech synthesis system is proposed . in a hidden markov model -lrb- hmm -rrb- based speech synthesis system which we have proposed , rhythm and tempo are controlled by state duration probability distributions modeled by single gaussian distributions . to synthesis speech , it constructs a sentence hmm corresponding to an arbitralily given text and determine state durations maximizing their probabilities , then a speech parameter vector sequence is generated for the given state sequence . however , there is an inconsistency : although the speech is synthesized from hmms with explicit state duration probability distributions , hmms are trained without them . in the present paper , we introduce an hsmm training , which is an hmm with explicit state duration probability distributions , into the hmm-based speech synthesis system . experimental results show that the use of hsmm training improves the naturalness of the synthesized speech .",0
1886,"This paper presents an improved Bayesian learning approach for hidden Markov models (HMMs) in speaker adaptation. The proposed approach includes a posteriori learning algorithm, transfer vector interpolation scheme, and transformation functions to improve adaptation methods for speaker-independent HMM parameters. The proposed approach also considers unseen units, and includes map adaptation and adaptation data for effective speaker adaptation. The results show that the proposed method outperforms existing adaptation approaches in terms of HMM parameter estimation and speaker adaptation.",1
1887,"we propose an improved maximum a posteriori learning algorithm of continuous-density hidden markov model parameters for speaker adaptation . the algorithm is developed by sequentially combining three adaptation approaches . first , the clusters of speaker-independent hmm parameters are locally transformed through a group of transformation functions . then , the transformed hmm parameters are globally smoothed via the map adaptation . within the map adaptation , the parameters of unseen units in adaptation data are further adapted by employing the transfer vector interpolation scheme . experiments show that the combined algorithm converges rapidly and outperforms those other adaptation methods .",0
1888,"This paper presents a new approach for recognizing visual objects based on their compositional structure. Specifically, the authors propose a hierarchy of relevant compositions and a structured object model based on a graphical model representation. By learning the compositional nature of visual objects, the system achieves high accuracy on large standard benchmark datasets. The authors also introduce a new modeling strategy that includes feature sharing and complex probability distributions. The approach is supervised and allows for the inference of statistical models for the recognition of object categories. The results demonstrate the effectiveness of the proposed approach for recognizing objects based on their compositional nature.",1
1889,the compositional nature of visual objects significantly limits their representation complexity and renders learning of structured object models tractable . adopting this mod-eling strategy we both -lrb- i -rrb- automatically decompose objects into a hierarchy of relevant compositions and we -lrb- ii -rrb- learn such a compositional representation for each category without supervision . the compositional representation supports feature sharing already on the lowest level of small image patches . compositions are represented as probability distributions over their constituent parts and the relations between them . the global shape of objects is captured by a graphical model which combines all compositions . inference based on the underlying statistical model is then employed to obtain a category level object recognition system . experiments on large standard benchmark datasets underline the competitive recognition performance of this graphical model and graphical model provide insights into the learned compo-sitional structure of objects .,0
1890,This paper presents an approach for action recognition in videos using dense trajectories and hybrid classification architectures. The method employs unsupervised representations of hand-crafted spatio-temporal features to extract spatio-temporal patterns from video data. These features are then used to build a hybrid model that combines supervised deep networks and manually labeled images. The proposed approach achieves state-of-the-art results on several large-scale action recognition datasets. The experimental results demonstrate that the hand-crafted features and the deep learning models together enhance the performance of the recognition system. The proposed approach is a promising solution for image classification and action recognition in videos.,1
1891,"action recognition in videos is a challenging task due to the complexity of the spatio-temporal patterns to model and the difficulty to acquire and learn on large quantities of video data . deep learning , although a breakthrough for image classification and showing promise for videos , has still not clearly superseded action recognition methods using hand-crafted features , even when training on massive datasets . in this paper , we introduce hybrid video classification architec-tures based on carefully designed unsupervised representations of hand-crafted spatio-temporal features classified by supervised deep networks . as we show in our experiments on five popular benchmarks for action recognition , our hybrid model combines the best of both worlds : it is data efficient -lrb- trained on 150 to 10000 short clips -rrb- and yet improves significantly on the state of the art , including recent deep models trained on millions of manually labelled images and videos .",0
1892,"This paper proposes a multi-scale audio indexing approach for translingual spoken document retrieval, with a focus on Voice of America Mandarin news broadcasts. The approach involves Chinese word tokenization at both word and subword scales, handling homophone ambiguity and overlapping subword n-grams. The use of lattice structures and a Mei syllable recognizer enables effective handling of out-of-vocabulary words. The multi-scale paradigm involves indexing both word and subword units, and the approach is evaluated for speech recognition using a Chinese speech recognition system. Additionally, the paper discusses the use of information retrieval technologies and machine translation for spoken document retrieval. The proposed approach shows promising results for translingual spoken document retrieval.",1
1893,"mei -lrb- mandarin-english information -rrb- is an english-chinese crosslingual spoken document retrieval -lrb- cl-sdr -rrb- system developed during the johns hopkins university summer workshop 2000 . we integrate speech recognition , machine translation , and information retrieval technologies to perform cl-sdr . mei -lrb- mandarin-english information -rrb- advocates a multi-scale paradigm , where both chinese words and subwords -lrb- characters and syllables -rrb- are used in retrieval . the use of subword units can complement the word unit in handling the problems of chinese word tokenization ambiguity , chinese homophone ambiguity , and out-of-vocabulary words in audio indexing . this paper focuses on multi-scale audio indexing in mei -lrb- mandarin-english information -rrb- . experiments are based on the topic detection and tracking corpora -lrb- tdt-2 and tdt-3 -rrb- , where we indexed voice of america mandarin news broadcasts by speech recognition on both the word and subword scales . in this paper , we discuss the development of the mei syllable recognizer , the representations of spoken documents using overlapping subword n-grams and lattice structures . results show that augmenting words with subwords is beneficial to cl-sdr performance .",0
1894,"This paper proposes a robust speech understanding system that utilizes conceptual relational grammar (CRG) for natural language parsing, speech recognition with Hidden Markov Models (HMM), and database retrieval. The system aims to handle various database query tasks through spoken language processing. The system is designed to use an ATIS database, and it features a data retrieval system and a speech recognizer as its key components. The study shows that the CRG-based natural language parser, along with the HMM-based speech recognizer, can accurately process spoken language and enable database retrieval. The proposed system provides a promising approach to robust spoken language understanding for various applications.",1
1895,"we describe a robust speech understanding system based on our newly developed approach to spoken language processing . we show that a robust nlu system can be rapidly developed using a relatively simple speech recognizer to provide sufficient information for database retrieval by spoken language processing . our experimental robust speech understanding system consists of three components : a speech recognizer based on hmm , a natural language parser based on conceptual relational grammar and a data retrieval system based on the atis database . with the use of the robust speech understanding system , database query tasks can be successfully performed .",0
1896,"This paper presents a new direct equalization method for symmetrical twisted pair transmission channels. The method is a digital adaptive channel equalizer that reduces inter-symbol interference and improves signal processing. Compared to analog equalization approaches, this method has reduced system complexity and is better suited for local distribution networks that use unshielded twisted pair transmission media. The equalizer can be used in both the transmitter and analog receiver to enhance the quality of transmission channels. The proposed approach offers a new solution for analog equalization that could potentially improve the performance of communication systems.",1
1897,"the unshielded twisted pair can be used as a transmission media for local distribution networks . to maintain a high transmission throughput , an analog or a digital adaptive channel equalizer is usually required in the receiver to minimize the effect of inter-symbol interference . under the observation that the high sampling rate high precision a/d and subsequent digital adaptive signal processing is an expensive approach , a direct equalization method , where the equalizer is implemented in the transmitter , is proposed for symmetrical twisted pair transmission channels . this direct equalization method can also be applied to the analog equalization approach for reduced system complexity .",0
1898,"This paper proposes a method for accelerating semi-supervised spectral learning algorithms in data mining tasks, such as text clustering. The authors focus on the problem of eigen-gap, which can occur when using spectral methods for clustering. They propose a coherence measure to evaluate the quality of the clustering results and use partial supervision to improve the accuracy of the clustering. The paper provides theoretical intuitions and experimental results to support the effectiveness of the proposed method. The authors also highlight the importance of background knowledge and context in achieving better results with semi-supervised learning algorithms.",1
1899,"semi-supervised learning algorithms commonly incorporate the available background knowledge such that an expression of the derived model 's quality is improved . depending on the specific context quality can take several forms and can be related to the generalization performance or to a simple clustering coherence measure . recently , a novel perspective of semi-supervised learning has been put forward , that associates semi-supervised clustering with the efficiency of spectral methods . more precisely , it has been demonstrated that the appropriate use of partial supervision can bias the data laplacian matrix such that the necessary eigenvec-tor computations are provably accelerated . this result allows data mining practitioners to use background knowledge not only for improving the quality of clustering results , but also for accelerating the required eigenvec-tor computations . in this paper we initially provide a high level overview of the relevant efficiency maximizing semi-supervised methods such that their theoretical intuitions are comprehensively outlined . consecutively , we demonstrate how these methods can be extended to handle multiple clusters and also discuss possible issues that may arise in the continuous semi-supervised solution . finally , we illustrate the proposed extensions empirically in the context of text clustering .",0
1900,"This paper presents a sparse space-time deconvolution method for analyzing calcium image sequences. The method addresses the problem of estimating the activity of cells in an image sequence by jointly estimating the cell locations, shapes, spike timings, and impulse responses. A unified formulation is proposed that allows the optimization problem to be solved efficiently, even in the presence of large amounts of noise. The proposed method is evaluated using both real and synthetic data, and is shown to outperform existing heuristic pre- or postprocessing approaches in terms of accuracy and robustness. The results demonstrate the potential of the proposed method for improving the analysis of calcium imaging data in neuroscience research.",1
1901,"we describe a unified formulation and algorithm to find an extremely sparse representation for calcium image sequences in terms of cell locations , cell shapes , spike timings and impulse responses . solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art , without the need for heuristic pre-or postprocessing . experiments on real and synthetic data demonstrate the viability of the proposed method .",0
1902,"This paper proposes an aggregated cross-validation (AGCV) method and demonstrates its efficient application to Gaussian mixture optimization. The AGCV method is a combination of likelihood-based and CV-based methods that estimates the ability score of a model to generalize to new data using a held-out subset. The AGCV method is applied to Gaussian mixture optimization algorithms and is shown to outperform both CV and minimum description length (MDL) based methods in terms of model selection. The paper also discusses the use of the AGCV method for large models such as Gaussian mixture hidden Markov models and its application to speech recognition. Finally, the paper proposes a bagging-like approach to enhance the AGCV method. Experimental results show that the proposed AGCV method with the bagging-like approach can reduce word error rates in speech recognition and improve the model structure.",1
1903,"we have previously proposed a cross-validation -lrb- cv -rrb- based gaussian mixture optimization method that efficiently optimizes the model structure based on cv likelihood . in this study , we propose aggregated cross-validation that introduces a bagging-like approach in the cv framework to reinforce the model selection ability . while a single model is used in cv to evaluate a held-out subset , aggregated cross-validation uses multiple models to reduce the variance in the score estimation . by integrating aggregated cross-validation instead of cv in the gaussian mixture optimization algorithm , an agcv likelihood based gaussian mixture optimization algorithm is obtained . the agcv likelihood based gaussian mixture optimization algorithm works efficiently by using sufficient statistics and can be applied to large models such as gaussian mixture hmm . the proposed agcv likelihood based gaussian mixture optimization algorithm is evaluated by speech recognition experiments on oral presentations and it is shown that lower word error rates are obtained by the agcv likelihood based gaussian mixture optimization algorithm when compared to cv and mdl based methods .",0
1904,"This paper presents a novel method for cosegmentation of image pairs by incorporating a global constraint into Markov random fields (MRFs). The proposed method employs appearance histograms as a term encoding spatial coherency and incorporates a global constraint into the MRF energy function. The global constraint enforces spatial consistency in the cosegmentation process, resulting in more accurate segmentation results. The proposed method is evaluated on both rigid and non-rigid object cosegmentation and achieves state-of-the-art performance. The optimization scheme is based on graph cuts, and a trust region approach is used to solve the NP-hard optimization problem. The proposed method has potential applications in object-driven image retrieval, interactive image editing, and video tracking.",1
1905,"we introduce the term cosegmentation which denotes the task of segmenting simultaneously the common parts of an image pair . a generative model for cosegmentation is presented . inference in the generative model leads to minimizing an energy with an mrf term encoding spatial coherency and a global constraint which attempts to match the appearance histograms of the common parts . this energy has not been proposed previously and its optimization is challenging and np-hard . for this term cosegmentation a novel optimization scheme which we call trust region graph cuts is presented . we demonstrate that this optimization scheme has the potential to improve a wide range of research : object driven image retrieval , video tracking and segmentation , and interactive image editing . the power of the optimization scheme lies in its generality , the common part can be a rigid/non-rigid object -lrb- or scene -rrb- , observed from different viewpoints or even similar objects of the same class .",0
1906,"In this paper, we propose a method for building joint spaces for relation extraction from textual features, utilizing relation specific term embeddings. We show that our approach can effectively capture the relationships between terms and their contexts, and can significantly improve performance on relation extraction tasks. We leverage both labeled data and unsupervised methods to construct a joint space for term pairs, and present a closed-form solution for constructing the joint space. We evaluate our method on DBpedia and medical relations datasets and demonstrate its effectiveness compared to state-of-the-art methods. Our approach offers a promising direction for relation extraction by incorporating joint space learning with relation-specific information.",1
1907,"in this paper , we present a novel approach for relation extraction using only term pairs as the input without textual features . we aim to build a single joint space for each relation which is then used to produce relation specific term embeddings . the proposed method fits particularly well for domains in which similar arguments are often associated with similar relations . it can also handle the situation when the labeled data is limited . the proposed method is evaluated both theoretically with a proof for the closed-form solution and experimentally with promising results on both dbpedia and medical relations .",0
1908,This paper proposes a method for enhancing the harmonic content of speech using a dynamic programming pitch tracking algorithm. The algorithm estimates the pitch of voiced or voiceless speech and separates the target speech from interfering speech. The proposed method utilizes pitch candidates obtained from the algorithm to enhance the harmonic content of the speech signal. The method takes into account the multi-channel autocorrelation-based estimator of the speech signal to improve the accuracy of the pitch estimates. The resulting enhancement in the harmonic content is achieved by utilizing the pitch information to better separate the interfering speech from the target speech. The proposed method is shown to be effective in enhancing the harmonic content of noisy speech.,1
1909,"for pitch tracking of a single speaker , a common requirement is to find the optimal path through a set of voiced or voiceless pitch estimates over a sequence of time frames . dynamic programming algorithms have been applied before to this problem . here , the pitch candidates are provided by a multi-channel autocorrelation-based estimator , and dynamic programming algorithms is extended to pitch tracking of multiple concurrent speakers . we use the resulting pitch information to enhance harmonic content in noisy speech and to obtain separations of target from interfering speech .",0
1910,This paper proposes an approach to accelerate generative models for 3D point cloud data. The method uses parallel hierarchical expectation maximization algorithms to generate compact representations of point cloud data. The approach also involves the dynamic creation of voxel grids using deterministic spatial subdivision methods such as octree and ndt-based methods. Local mixture modeling is used to improve spatial perception applications such as maximum likelihood segmentation and run-time occupancy calculations. The paper shows that the proposed generative models outperform existing methods in terms of model size and fidelity. The approach also involves sparsification techniques such as stochastic sampling and parametric sparsity to reduce the number of parameters in the model. The paper demonstrates the effectiveness of the proposed approach on 3D point cloud data using probabilistic subdivisions of voxel grids and octrees as deterministic structures.,1
1911,"finding meaningful , structured representations of 3d point cloud data has become a core task for spatial perception applications . in this paper we introduce a method for constructing compact generative representations of pcd at multiple levels of detail . as opposed to deterministic structures such as voxel grids or octrees , we propose probabilistic subdivisions of the data through local mixture modeling , and show how these subdivisions can provide a maximum likelihood segmentation of the data . the final representation is hierarchical , compact , para-metric , and statistically derived , facilitating run-time occupancy calculations through stochastic sampling . unlike traditional deterministic spatial subdivision methods , our technique enables dynamic creation of voxel grids according the application 's best needs . in contrast to other gener-ative models for 3d point cloud data , we explicitly enforce sparsity among points and mixtures , a technique which we call expectation sparsification . this leads to a highly parallel hierarchical expectation maximization algorithm well-suited for the 3d point cloud data and real-time execution . we explore the trade-offs between model fidelity and model size at various levels of detail , our tests showing favorable performance when compared to octree and ndt-based methods .",0
1912,"This paper discusses various aspects of modern exploitation environments for multi-modal/multi-media corpora. The focus is on synchronized media and text streams, which are valuable language resources that require appropriate metadata descriptions and collaborative annotation. The paper examines the use of distributed environments for multi-media corpora and discusses the challenges and opportunities they present. The authors argue that effective exploitation of multi-modal/multi-media corpora requires the integration of various technologies and techniques, including collaborative annotation and metadata descriptions.",1
1913,"this paper wants to discuss several aspects of multimodal/multimedia language resources such as the use of metadata descriptions for easy location purposes , their collaborative annotation and exploitation via internet , the generation of synchronized media and text streams in distributed environments , and general annotation formats . these aspects that although they may be discussed independently have to fit together seamlessly to offer users an adequate exploitation environment that is up to the huge amount of data that is available in modern multi-media corpora and is able to exploit fully the current technology advancements .",0
1914,This paper proposes an iterative algorithm for channel estimation in multiuser DS-CDMA systems that employ aperiodic spreading codes. The problem of blind channel estimation is addressed using a maximum likelihood (ML) estimator. The algorithm is designed for medium SNR values and considers the multiuser parameter estimation problem. Multiuser detectors are used to detect the multiuser digital signals. The proposed algorithm involves alternating optimization to find the global maxima of the ML function. Computer simulations are used to evaluate the performance of the algorithm. The results show that the proposed algorithm is effective in channel parameter estimation and multiuser detection for CDMA communications.,1
1915,"for high performance cdma communications , multiuser detection is often required to suppress the multiple access interference mai . most multiuser detectors rely on accurate channel information to recover the multiuser digital signals . this paper studies the blind channel estimation problem for ds-cdma systems using aperiodic spreading codes . the maximum likelihood ml estimator is formulated for channel estimation . we rst convert the multiuser parameter estimation problem into a set of single user optimization problems via alternating optimization , and then determine the channel parameters for each user using an iterative algorithm derived . it is shown by computer simulation that this iterative algorithm can reach global maxima almost always under medium snr values .",0
1916,"This paper presents a method for estimating dependency and significance for high-dimensional data, with a focus on its applications in signal processing. The authors propose nonparametric tests to assess the strength of the dependency structure between high-dimensional observations, taking into account the data association problem. The proposed method is designed to handle high-dimensional measurements, where traditional techniques may fail. By providing a way to estimate the dependency structure and significance, the proposed method can help identify meaningful relationships between variables and enable further analysis of the data. The authors demonstrate the effectiveness of their approach through experiments on synthetic and real-world datasets. The results show that the proposed method can accurately estimate the dependency structure and significance in high-dimensional data, making it a useful tool for signal processing applications.",1
1917,understanding the dependency structure of a set of variables is a key component in various signal processing applications which involve data association . the simple task of detecting whether any dependency exists is particularly difficult when models of the data are unknown or difficult to characterize because of high-dimensional measurements . we review the use of nonparametric tests for characterizing dependency and how to carry out these tests with high-dimensional observations . in addition we present a method to assess the significance of the tests .,0
1918,"This paper presents a new binary mask approach for improving speech intelligibility in noise-corrupted speech. The proposed method incorporates noise distortion constraints to avoid overestimating the noise in time-frequency (t-f) units. The binary mask is used to remove interfering noise and improve the quality of the speech signal. The effectiveness of the proposed method is demonstrated through experiments on speech intelligibility, showing significant improvements compared to traditional methods. The results show that the proposed binary mask approach can effectively improve speech intelligibility by better handling noise constraints in high-dimensional t-f units.",1
1919,"it has been shown that large gains in speech intelligibility can be obtained by using the binary mask approach which retains the time-frequency units of the mixture signal that are stronger than the interfering noise -lrb- masker -rrb- -lrb- i.e. , snr > 0 db -rrb- , and removes the t-f units where the interfering noise dominates . in this paper , we introduce a new binary mask for improving speech intelligibility based on noise distortion constraints . a binary mask is designed to retain noise overestimated t-f units while discarding noise underestimated t-f units . listening tests were conducted to evaluate the new binary mask in terms of intelligibility . results from the listening tests indicated that large gains in intelligibility can be achieved by the application of the proposed binary mask to noise-corrupted speech even at extremely low snr levels -lrb- -10 db -rrb- .",0
1920,"This paper presents an edge-directed color image super-resolution technique inspired by human perception. The proposed method combines edge-directed, reconstruction-based, and learning-based approaches in a multi-scale tensor voting framework. The framework incorporates perceptual grouping and segmentation with a dense voting field to obtain a multi-scale edge representation of the low-resolution color image. The edge-directed technique uses an edge-preserving smoothness prior and a back projection constraint to reconstruct a high-resolution color image. The computational framework is efficient and overcomes the time-consuming learning procedure of traditional image super-resolution techniques. Experimental results demonstrate the effectiveness of the proposed method in reconstructing high-resolution curves and preserving color channels.",1
1921,"inspired by multi-scale tensor voting , a computational framework for perceptual grouping and segmentation , we propose an edge-directed technique for color image super-resolution given a single low-resolution color image . our edge-directed technique combines the advantages of edge-directed , reconstruction-based and learning-based methods , and is unique in two ways . first , we consider simultaneously all the three color channels in our multi-scale tensor voting framework to produce a multi-scale edge representation to guide the process of high-resolution color image reconstruction , which is subject to the back projection constraint . fine details are inferred without noticeable blurry or ringing artifacts . second , the inference of high-resolution curves is achieved by multi-scale tensor voting , using the dense voting field as an edge-preserving smoothness prior which is derived geometrically without any time-consuming learning procedure . qualitative and quantitative results indicate that our edge-directed technique produces convincing results in complex test cases typically used by state-of-the-art image super-resolution techniques .",0
1922,"This paper presents a method for frequency analysis using non-uniform sampling, with application to active queue management in real-time systems. The proposed method approximates the Fourier transform of a continuous-time signal using non-uniformly sampled data. The method involves windowing the non-uniform data and applying a Fourier transform to each window, with analytical expressions derived for the resulting frequency-domain representations. The approach is particularly useful for real-life problems where the sample values are non-uniformly spaced, as in embedded systems or vibrational analysis. The proposed method provides an accurate and efficient means of performing frequency analysis for such problems, and has potential applications in a wide range of real-time applications.",1
1923,"in many real-time applications , sample values and time stamps are delivered in pairs , where sampling times are non-uniform . frequency analysis using non-uniform data occurs in various real life problems and embedded systems , such as vibra-tional analysis in cars and control of packet network queue lengths . our contribution is to first overview different ways to approximate the fourier transform , and secondly to give analytical expressions for how non-uniform sampling affects these approximations . the results are expressed in terms of frequency windows describing how a single frequency in the continuous time signal is smeared out in the frequency domain , or , more precisely , in the expected value of the fourier transform approximation .",0
1924,"This paper proposes a new method for speech recognition based on a generalization of Linear Discriminant Analysis (LDA) called Power Linear Discriminant Analysis (PLDA). The method is applied to Segmental Unit Input Hidden Markov Models (HMMs), and involves using heteroscedastic discriminant analysis to reduce the dimensionality of the data sets. The proposed approach is compared with traditional LDA and Principal Component Analysis (PCA) methods. Experimental results show that the PLDA-based approach outperforms both traditional LDA and PCA methods, indicating its effectiveness in improving the discriminative information of the features used in speech recognition. The proposed method has potential for real-world applications in speech recognition systems.",1
1925,"to precisely model the time dependency of features is one of the important issues for speech recognition . segmental unit input hmm with a dimensionality reduction method is widely used to address this issue . linear discriminant analysis and heteroscedastic discriminant analysis are classical and popular approaches to reduce dimensionality . however , it is difficult to find one particular criterion suitable for any kind of data set in carrying out dimension-ality reduction while preserving discriminative information . in this paper , we propose a new framework which we call power linear discriminant analysis . power linear discriminant analysis can describe various criteria including linear discriminant analysis and heteroscedastic discriminant analysis with one parameter . experimental results show that the power linear discriminant analysis is more effective than pca , linear discriminant analysis , and heteroscedastic discriminant analysis for various data sets .",0
1926,"I'm sorry, but the graph you provided is incomplete and does not contain any relevant relationships or entities for generating an abstract. Could you please provide a more complete graph or additional information?",1
1927,"in this work , a novel approach for nonlinear acoustic echo cancellation is proposed . the main innovative idea of the proposed method is to model only the small region of the echo path around the direct path by a group of parallel hammerstein models , to estimate a non-linear preprocessor by correlations between the linear kernels of the hammerstein submodels , and to describe the remaining echo path by a simple hammerstein model with the preprocessor determined in the aforementioned way . while the computational complexity of such a system increases only slightly in comparison to a linear echo canceller , experiments with speech recordings from a smartphone in different environments confirm a significantly increased echo cancellation performance .",0
1928,"This paper proposes a probabilistic contour discriminant for object localization. The method utilizes a probabilistic model to calculate the likelihood ratio of object presence in an image based on observation densities of features such as clutter and contour. The model is combined with sampling methods for initializing contour trackers, improving the accuracy of feature detection and localizing objects in cluttered scenes. Experimental results demonstrate the effectiveness of the proposed method in improving object localization performance.",1
1929,"a method of localising objects in images is proposed . possible conngurations are evaluated using the contour discriminant , a likelihood ratio which is derived from a probabilistic model of the feature detection process . we treat each step in this process probabilistically , including the occurrence of clutter features , and derive the observation densities for both correct \ target '' con-gurations and incorrect \ clutter '' conngurations . the contour discriminant distinguishes target objects from the background even in heavy clutter , making only the most general assumptions about the form that clutter might take . the method generates samples stochasti-cally to avoid the cost of processing an entire image , and promises to be particularly suited to the task of initialising contour trackers based on sampling methods .",0
1930,This paper presents a robust method for two-dimensional (2-d) frequency estimation using autocorrelation phase fitting. The goal is to estimate the local frequency in the presence of white Gaussian additive noise. The proposed method employs a 2-d phase unwrapping step and a 2-d frequency estimator based on autocorrelation estimation. The least square plane fitting is used to achieve high accuracy. The performance of the proposed method is evaluated using Cramer-Rao bounds and Monte Carlo simulations. The results show that the method has good robustness against noise ratio and can achieve accurate 2-d frequency estimation.,1
1931,"in this paper , the problem of two-dimensional frequency estimation of a complex sinusoid embedded in a white gaussian additive noise is addressed . a new frequency estimator based on a least square plane fitting of the estimated autocorrelation phase of the signal is derived . this algorithm requires a 2-d phase unwrapping step which can be easily done . this algorithm is shown to be unbiased and attains the cramer rao bounds for high signal to noise ratio -lrb- accuracy and robustness of this new 2-d frequency esti-mator are statistically assessed by monte carlo simulations . the results obtained show that a good local frequency estimation can be achieved with a very simple algorithm , and a very small amount of points used for the autocorrelation estimation .",0
1932,"This paper proposes a new approach to goal-driven learning for permutation channels, based on the concept of propagation redundancy. The goal is to improve learning strategies for tasks in the airspace domain, such as task orders. The proposed approach employs a meta-reasoner module with base reasoner capabilities to reason about learning paradigms and learning goals. The meta-reasoner uses meta-reasoning capabilities to optimize the learning strategies and adapt them to the specific task and domain. The paper also discusses the implementation of the approach in AI systems and evaluates its effectiveness using simulation experiments. The results show that the proposed approach can significantly improve the performance of goal-driven learning for permutation channels in the airspace domain.",1
1933,"goal driven learning -lrb- gdl -rrb- focuses on systems that determine by themselves what has to be learnt and how to learn goal driven learning . typically goal driven learning use meta-reasoning capabilities over a base reasoner , identifying learning goals and devising strategies . in this paper we present a novel goal driven learning to deal with complex ai systems where the meta-reasoning module has to analyze the reasoning trace of multiple components with potentially different learning paradigms . our goal driven learning works by distributing the generation of learning strategies among the different modules instead of centralizing goal driven learning in the meta-reasoner . we implemented our goal driven learning in the goal driven learning , that works in the airspace task orders domain , showing an increase in performance .",0
1934,"This paper explores the use of metadata to improve the recognition of named entities in spontaneous speech. The goal is to improve the recognition accuracies of LVCSR tasks for the Malach corpus of Holocaust testimonials, which is a challenging dataset due to errors in named-entity recognition and high word error rate. The proposed approach uses textual information, such as pre-interview questionnaires, to provide side information on the named entities and improve recognition accuracy on a speaker-by-speaker basis. The paper evaluates the effectiveness of the approach using experiments on the Malach corpus and shows that the use of side information can significantly improve recognition accuracies for named entities in spontaneous speech. The results demonstrate the potential of using metadata to improve recognition accuracy in challenging speech recognition tasks.",1
1935,"with improved recognition accuracies for lvcsr tasks , it has become possible to search large collections of spontaneous speech for a variety of information . the malach corpus of holocaust testimonials is one such collection , in which we are interested in automatically transcribing and retrieving portions that are relevant to named entities such as people , places , and organizations . since the testimonials were gathered from thousands of people in countries throughout europe , an extremely large number of potential named entities are possible , and this causes a well-known dilemma : increasing the size of the vocabulary allows for more of these words to be recognized , but also increases confusability , and can harm recognition performance . however , the malach corpus of holocaust testimonials , like many other collections , includes side information or malach corpus of holocaust testimonials that can be exploited to provide prior information on exactly which named entities are likely to appear . this paper proposes a method that capitalizes on this prior information to reduce named-entity recognition errors by over 50 % relative , and simultaneously decrease the overall word error rate by 7 % relative . the malach corpus of holocaust testimonials we use derives from a pre-interview questionaire that includes the names of friends , relatives , places visited , membership of organizations , synonyms of place names , and similar information . by augmenting the lexicon and language model with this information on a speaker-by-speaker basis , we are able to exploit the textual information that is already available in the corpus to facilitate much improved speech recognition .",0
1936,"This paper presents a novel method for optimizing classifier performance in real-world classification problems. The proposed method is based on an approximation to the Wilcoxon-Mann-Whitney statistic, which is used to construct a new objective function for the classifier. The goal is to improve the classification rate and ROC curve performance for real-world customer behavior prediction problems, such as those faced by wireless and cable service providers. The paper also discusses the use of gradient-based methods and cost functions, such as cross-entropy and mean squared error, in the optimization process. The proposed method is evaluated using experiments on several real-world classification problems, and the results show that it can significantly improve the performance of the classifier in terms of ROC curve and mean squared error. The paper demonstrates the effectiveness of using the Wilcoxon-Mann-Whitney statistic and its approximation in optimizing classifier performance for real-world classification problems.",1
1937,"when the goal is to achieve the best correct classification rate , cross entropy and mean squared error are typical cost functions used to optimize classifier performance . however , for many real-world classification problems , the roc curve is a more meaningful performance measure . we demonstrate that minimizing cross entropy or mean squared error does not necessarily maximize the area under the roc curve . we then consider alternative objective functions for training a classifier to maximize the roc curve directly . we propose an objective function that is an approximation to the wilcoxon-mann-whitney statistic , which is equivalent to the roc curve . the proposed objective function is differentiable , so gradient-based methods can be used to train the classifier . we apply the new objective function to real-world customer behavior prediction problems for a wireless service provider and a cable service provider , and achieve reliable improvements in the roc curve .",0
1938,"This paper presents new ℌ ∞ bounds for the recursive least squares algorithm that exploit the structure of the input data. The goal is to improve the analysis of the algorithm's performance in the presence of structured input data, such as human speech. The paper discusses the properties of the noise process and analyzes the convergence guarantees for the algorithm in the presence of structured input data. The proposed bounds are evaluated using experiments on additive noise and demonstrate that the use of structured input data can",1
1939,"the recursive least squares algorithm is well known and has been widely used for many years . most analyses of recursive least squares algorithm have assumed statistical properties of the data or the noise process , but recent robust h ∞ analyses have been used to bound the ratio of the performance of the algorithm to the total noise . in this paper , we provide an additive analysis bounding the difference between performance and noise . our additive analysis provides additional convergence guarantees in general , and particular benefits for structured input data . we illustrate the additive analysis using human speech and white noise .",0
1940,"This paper presents a novel approach for learning the structure of Bayesian networks with a bounded vertex cover number, which is a measure of the size of the smallest set of vertices that covers all edges in a graph. Our method utilizes integer linear programming to find the optimal network structure, and we show that it can be applied to both bounded tree-width and bounded vertex cover number networks. We demonstrate that our approach can solve the polynomial time Bayesian networks learning problem for networks with a bounded vertex cover number, which includes many practical scenarios. Additionally, we compare our method to existing approaches for learning bounded tree-width networks and show that it outperforms them in terms of both accuracy and efficiency. Our results suggest that our approach is a promising solution for tractable Bayesian network structure learning with a bounded vertex cover number.",1
1941,"both learning and inference tasks on bayesian networks are np-hard in general . bounded tree-width bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue ; however , while inference on bounded tree-width networks is tractable , the learning problem remains np-hard even for tree-width 2 . in this paper , we propose bounded vertex cover number bayesian networks as an alternative to bounded tree-width networks . in particular , we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k , in contrast to the general and bounded tree-width cases ; on the other hand , we also show that learning problem is w -lsb- 1 -rsb- - hard in parameter k. furthermore , we give an alternative way to learn bounded vertex cover number bayesian networks using integer linear programming , and show this is feasible in practice .",0
1942,"This paper investigates the role of competition and arbors in ocular dominance during cortical development. We explore how Hebbian and competitive Hebbian algorithms, along with competitive and interactive cortical influences, can be used to model pattern formation in the cortex. Our analysis reveals that competition plays a critical role in shaping ocular dominance, and that arbors are an essential component of the process. We show that both competition and arbors operate on a common footing in the development of cortical ocular dominance. Our results provide new insights into the mechanisms underlying cortical development and have important implications for understanding the visual system. Overall, our findings suggest that the competition and arbors model may be a valuable tool for investigating the development of other cortical features.",1
1943,"hebbian and competitive hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development . we analyse in theoretical detail a particular model -lrb- adapted from piepenbrock & ober-mayer , 1999 -rrb- for the development of id stripe-like patterns , which places competitive and interactive cortical influences , and free and restricted initial arborisation onto a common footing .",0
1944,"This paper investigates the universal approximation properties of oscillators for learning and approximating complex state space trajectories in both natural and artificial neural circuits. We demonstrate that the combination of fast and slow oscillations can be used to approximate a wide range of trajectories with high accuracy, even in the case of amorphous networks. Our analysis reveals that oscillators have the potential to learn and approximate trajectories with complex dynamics, including those with bounded frequencies. We also show that the use of oscillators can lead to significant improvements in the efficiency and speed of trajectory learning. Our results suggest that the combination of fast and slow oscillations may be a powerful tool for approximating and learning complex trajectories in neural circuits, with potential applications in fields such as robotics, control theory, and neuroscience. Overall, our findings contribute to a better understanding of the role of oscillators in trajectory learning and provide new insights into the design of efficient and effective learning algorithms.",1
1945,"natural and artificial neural circuits must be capable of traversing specific state space trajectories . a natural approach to this natural and artificial neural circuits is to learn the relevant trajectories from examples . unfortunately , gradient descent learning of complex trajectories in amorphous networks is unsuccessful . we suggest a possible approach where trajectories are realized by combining simple oscil-lators , in various modular ways . we contrast two regimes of fast and slow oscillations . in all cases , we show that banks of oscillators with bounded frequencies have universal approximation properties . open questions are also discussed briefly .",0
1946,"This paper investigates the properties of POMDP problems that make them amenable to approximation in polynomial time. Specifically, we explore the relationship between the belief-space properties of POMDP problems, such as the dimensionality of the belief space, the sparsity and smoothness of beliefs, and the structure of the state-transition matrix, and the efficiency of approximate solutions. We show that certain properties, such as circulant state-transition matrices and reachable belief spaces, can significantly simplify the POMDP planning problem and lead to efficient computation of the optimal reachable space and policy. Furthermore, we demonstrate that the use of point-based algorithms can exploit the sparsity and smoothness of beliefs to compute approximate solutions with high accuracy. Our results suggest that the combination of these properties can make some POMDP problems easier to approximate in polynomial time than others. Overall, our findings contribute to a better understanding of the computational complexity of POMDP problems and provide insights into the design of efficient algorithms for solving them.",1
1947,"point-based algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable markov decision processes -lrb- pomdps -rrb- in high dimensional belief spaces . in this work , we seek to understand the belief-space properties that allow some pomdp problems to be approximated efficiently and thus help to explain the point-based algorithms ' success often observed in the experiments . we show that an approximately optimal pomdp solution can be computed in time polynomial in the covering number of a reachable belief space , which is the subset of the belief space reachable from a given belief point . we also show that under the weaker condition of having a small covering number for an optimal reachable space , which is the subset of the belief space reachable under an optimal policy , computing an approximately optimal solution is np-hard . however , given a suitable set of points that '' cover '' an optimal reach-able space well , an approximate solution can be computed in polynomial time . the covering number highlights several interesting properties that reduce the complexity of pomdp planning in practice , e.g. , fully observed state variables , beliefs with sparse support , smooth beliefs , and circulant state-transition matrices .",0
1948,"This paper introduces probabilistic abstraction hierarchies, a framework for clustering real data that combines global optimization algorithms with a probabilistic model. Our framework addresses the challenge of clustering data that contains noise and local maxima by using a hierarchical approach that combines both global and local steps. We provide a theoretical analysis of our framework and demonstrate its effectiveness on both synthetic and real datasets. Our results show that the use of probabilistic abstraction hierarchies can lead to a significant improvement in clustering accuracy compared to traditional clustering algorithms. Additionally, our approach provides a taxonomy of the sensitivity of the clustering results to the parameters of the probabilistic model. Overall, our findings suggest that the use of probabilistic abstraction hierarchies can provide a powerful tool for clustering complex datasets and can have applications in fields such as data mining and machine learning.",1
1949,"many domains are naturally organized in an abstraction hierarchy or taxonomy , where the instances in '' nearby '' classes in the taxonomy are similar . in this paper , we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy , where each class is associated with a prob-abilistic model from which the data was generated . the probabilistic framework simultaneously optimizes three things : the assignment of data instances to clusters , the models associated with the clusters , and the structure of the abstraction hierarchy . a unique feature of our probabilistic framework is that probabilistic framework utilizes global optimization algorithms for both of the last two steps , reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms that only take local steps . we provide a theoretical analysis for our probabilistic framework , showing that probabilistic framework converges to a local maximum of the probability of model and data . we present experimental results on synthetic data , and on real data in the domains of gene expression and text .",0
1950,"This paper presents a method for recovering 3D geometry from uncalibrated perspective views using iterative methods and projective invariants. The approach is able to overcome the limitations of traditional camera calibration methods by using a set of projective invariants that are independent of the camera parameters. We describe the use of iterative methods to estimate the camera parameters and the 3D structure of the scene from image correspondences. Our approach is able to handle cases where the camera calibration model is restricted or not available. We show that the use of projective invariants and iterative methods can lead to accurate recovery of 3D geometry from uncalibrated cameras. Our results demonstrate the effectiveness of our approach on a range of synthetic and real-world datasets. Overall, our findings provide a valuable tool for 3D reconstruction from uncalibrated cameras and have applications in fields such as computer vision and robotics.",1
1951,"this paper considers the problem of computing placement of points in 3 dimensional space given two uncalibrated perspective views . the main theorem shows that the placement of the points is determined only up to an arbitrary projective transformation of 3-space . given additional ground control points , however , the location of the points and the camera parameters may be determined . the method is linear and non-iterative whereas previously known methods for solving the camera calibration and placement to take proper account of both ground-control points and image correspondences are unsatisfactory in requiring either iterative methods or model restrictions . as a result of the main theorem , it is possible to determine projective invariants of 3-d geometric configurations from two perspective views .",0
1952,"This paper presents a speaker clustering method for speech recognition that uses parameters characterizing vocal-tract dimensions. The proposed method aims to reduce recognition errors by clustering speakers based on their vocal-tract size related articulatory parameters. We use speaker-clustered tied-state Hidden Markov Models (HMMs) as a baseline, and explore the effectiveness of the proposed clustering method for improving recognition accuracy. Our approach is gender-dependent and designed specifically for Japanese phoneme recognition. We demonstrate that using vocal-tract parameters to cluster speakers leads to improved recognition accuracy and reduced recognition errors compared to the baseline approach. We also show that the acoustic criteria used for clustering and the number of speaker clusters have a significant impact on the overall performance of the system. Overall, our findings suggest that incorporating vocal-tract parameters in speaker clustering methods can lead to significant improvements in speech recognition performance.",1
1953,"we propose speaker clustering methods based on the vocal-tract-size related articulatory parameters associated with individual speakers . two parameters characterizing gross vocal-tract dimensions are rst derived from formants of speaker-specic japanese vowels , and are then used to cluster a total of 148 male japanese speakers . the resultant speaker clusters are found to be signicantly dierent from the speaker clusters obtained by conventional acoustic criteria . japanese phoneme recognition experiments are carried out using speaker-clustered tied-state hmms trained for each cluster . compared with the baseline gender dependent model , 5.7 % of recognition error reduction has been achieved based on the clustering method using vocal-tract parameters .",0
1954,"This paper proposes a method for unsupervised learning from narrated instruction videos, where both video and natural language narration are jointly modeled. To achieve this, the authors introduce an annotated dataset of internet instruction videos, and develop a joint model to learn from them in an unsupervised manner. The proposed method is evaluated on a task of learning to recognize car tire maintenance activities from narrated instruction videos, and is shown to outperform single-modality baselines. The results demonstrate the effectiveness of the joint modeling approach for learning from instructional videos.",1
1955,"we address the problem of automatically learning the main steps to complete a certain task , such as changing a car tire , from a set of narrated instruction videos . the contributions of this paper are threefold . first , we develop a joint model for video and natural language narration that takes advantage of the complementary nature of the two signals . second , we collect an annotated dataset of 57 internet instruction videos containing more than 350,000 frames for two tasks -lrb- changing car tire and cardiopulmonary resuscitation -rrb- . third , we experimentally demonstrate that the proposed joint model automatically discovers , in an unsupervised manner , the main steps to achieve each task and locate them within the input videos . the results further show that the proposed joint model outperforms single-modality baselines , demonstrating the benefits of joint modeling video and text .",0
1956,"This paper proposes a computationally efficient refinement of the fundamental frequency estimate for the Adaptive Harmonic Model (AHM). The AHM is a widely used speech analysis technique that estimates the fundamental frequency of a speech signal. The proposed refinement is based on an adaptive iterative refinement algorithm that combines the AHM with a peak picking approach. The refinement also includes an Adaptive Discrete Fourier Transform (ADFT) that enhances the performance of the AHM. The proposed refinement method improves the accuracy of the AHM's estimation of the fundamental frequency, while reducing the average time required for estimation. The method is evaluated on speech recordings and compared with a least squares solution approach. The results demonstrate the superiority of the proposed approach in terms of accuracy and computational efficiency.",1
1957,"the full-band adaptive harmonic model can be used by the adaptive iterative refinement algorithm to accurately model the perceived characteristics of a speech recording . however , the least squares solution used in the current ahm-air makes the f 0 refinement in air time consuming , limiting the use of this algorithm for large databases . in this paper , a peak picking approach is suggested as a substitution to the least squares solution . in order to integrate the adaptivity scheme of full-band adaptive harmonic model in the peak picking approach , an adaptive discrete fourier transform is also suggested in this paper , whose frequency basis can fully follow the frequency variations of the f 0 curve . evaluations have shown an average time reduction of 5.5 times compared to the ls solution approach , while the quality of the re-synthesis is preserved compared to the original ahm-air .",0
1958,"This paper presents an adaptive descriptor design for robust object recognition in the wild. The proposed approach utilizes domain adaptation and an adaptive approach to kernel learning to handle the complexity of digital images with different styles and effects. The method involves the use of g-incorporated kernel descriptor and post-processing functions such as photography effect filters and picture styles image descriptors. The approach also considers contrast, vignetting, and color tone in object recognition. The proposed method is evaluated on the Oxford Flower dataset, and results show that it outperforms existing gradient-based image descriptors in terms of recognition accuracy and complexity. The approach is computationally efficient and can be used for object recognition in various real-world scenarios.",1
1959,"digital images nowadays show large appearance vari-abilities on picture styles , in terms of color tone , contrast , vignetting , and etc. . these ` picture styles ' are directly related to the scene radiance , image pipeline of the camera , and post processing functions -lrb- e.g. , photography effect filters -rrb- . due to the complexity and nonlinearity of these factors , popular gradient-based image descriptors generally are not invariant to different picture styles , which could degrade the performance for object recognition . given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions , to find a robust object recognition system is useful and challenging . in this paper , we investigate the influence of picture styles on object recognition by making a connection between image de-scriptors and a pixel mapping function g , and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning , without estimating or specifying the image styles used in training and testing . we conduct experiments on the domain adaptation data set , the oxford flower data set , and several variants of the flower data set by introducing popular photography effects through post-processing . the results demonstrate that the proposed adaptive approach consistently yields recognition improvements over standard descriptors in all studied cases .",0
1960,"I apologize, but there seems to be an error in the graph you provided as it only has one node and one relation, both of which refer to ""within-class covariance normalization"". Could you please provide a corrected version of the graph or confirm if this is the correct information? Thank you.",1
1961,"nuisance attribute projection -lrb- nap -rrb- and within-class covariance normalization are two effective techniques for intersession variability compensation in svm based speaker verification systems . however , by normalizing or removing the nuisance subspace containing the session variability can not guarantee to enlarge the distance between speakers . in this paper , we investigated the probability of using linear discriminant analysis for discriminative training . to cope with the small sample size problem which prevents us from using linear discriminant analysis directly , we adapted the within-class covariance normalization , which first projects the whole feature space into a relatively low dimensional subspace by linear discriminant analysis , and then performs linear discriminant analysis in the subspace . by some modification , the within-class covariance normalization can be degenerated into a kind of within-class covariance normalization , which we called within-class covariance normalization . experiments on nist sre tasks showed that , the within-class covariance normalization outperformed the conventional within-class covariance normalization , especially in low dimensional feature space .",0
1962,"This paper proposes a new method for automatically classifying the depression state of individuals based on articulatory precision. The proposed method uses support vector machine classifiers and Gaussian mixture models to classify depression state using clinical measures of depression severity and vocal tract formant frequencies. The study focuses on the formant-based characterization of depression state, as changes in the formant frequencies are associated with neurophysiological changes in depression. The authors analyze the precision of articulatory features of sustained vowels and dynamic features of conversational speech to estimate the depression state of individuals. The proposed method achieves high accuracy in classifying depression state, with sensitivity, specificity, and area under the curve metrics outperforming previous methods. The results demonstrate the potential of using formant frequency tracks and articulatory features to classify depression state in audio recordings.",1
1963,"neurophysiological changes in the brain associated with major depression disorder can disrupt articulatory precision in speech production . motivated by this observation , we address the hypothesis that articulatory features , as manifested through formant frequency tracks , can help in automatically classifying depression state . specifically , we investigate the relative importance of vocal tract formant frequencies and their dynamic features from sustained vowels and conversational speech . using a database consisting of audio from 35 subjects with clinical measures of depression severity , we explore the performance of gaussian mixture model and support vector machine classifiers . with only formant frequencies and their dynamics given by velocity and acceleration , we show that depression state can be classified with an optimal sensitivity/specificity/area under the gaussian mixture model for gmms and support vector machine classifiers , respectively . future work will involve merging our formant-based characterization with vocal source and prosodic features .",0
1964,"This paper proposes a method for Word Sense Disambiguation (WSD) called ""Relative Selection"". The approach is based on analyzing the co-occurrence frequency matrix of words in raw corpora, and using this information to disambiguate word senses. The method takes into account the relative frequency of different senses of a word, as well as the co-occurrence patterns of related words, such as synonyms, hypernyms, and meronyms. The paper demonstrates the effectiveness of this approach on English data and shows that it outperforms previous methods in WSD. The results suggest that the Relative Selection method is a promising direction for future work in this area.",1
1965,"this paper describes a novel method for a word sense disam-biguation that utilizes relatives -lrb- i.e. synonyms , hypernyms , meronyms , etc in wordnet -rrb- of a target word and raw corpora . the method disam-biguates senses of a target word by selecting a relative that most probably occurs in a new sentence including the target word . only one co-occurrence frequency matrix is utilized to efficiently disambiguate senses of many target words . experiments on several english datum present that our proposed method achieves a good performance .",0
1966,"This paper presents a scalable distributed Kalman filtering approach for wireless sensor networks. The approach is based on a data-driven average consensus framework and a communication resource allocation policy to efficiently distribute the filtering computations across the network. The proposed architecture enables component-wise state estimation error through consensus, making it well-suited for scalable distributed applications. The paper highlights the benefits of the approach in terms of reducing communication overhead and computation time, while also demonstrating its effectiveness in addressing the classical problem of Kalman filtering in distributed settings. Overall, the approach provides a promising solution for scalable distributed filtering in wireless sensor networks.",1
1967,"kalman filtering is a classical problem of significant interest in the context of a distributed application for wireless sensor networks . in this paper we consider a specific algorithm for distributed kalman filtering proposed recently by olfati-saber -lsb- 1 -rsb- and present a scal-able wireless communication architecture suited for implementation in wireless sensor networks . the proposed scal-able wireless communication architecture uses a data driven average consensus framework . this allows us to explicitly characterize the delay vs. estimate accuracy tradeoff in filtering . by exploiting the structure of the distributed filtering computations , we derive an optimal communication resource allocation policy for minimizing the component-wise state estimation error . furthermore , our scal-able wireless communication architecture is scalable in terms of the network size n . we provide simulation results demonstrating the performance of our scal-able wireless communication architecture .",0
1968,"In this paper, we investigate the use of shape priors in variational image segmentation. We explore the benefits of incorporating global and local optimality, convexity, and Lipschitz continuity to achieve globally optimal solutions. We discuss the implicit representation of shape using probabilistic and mild regularity assumptions, as well as the statistical shape priors. We employ the level set method to track pixel movements and deformations in shape during segmentation. Our results demonstrate the effectiveness of shape priors in achieving accurate and efficient image segmentation.",1
1969,"in this work , we introduce a novel implicit representation of shape which is based on assigning to each pixel a probability that this pixel is inside the shape . this probabilis-tic representation of shape resolves two important drawbacks of alternative implicit shape representations such as the level set method : firstly , the space of shapes is convex in the sense that arbitrary convex combinations of a set of shapes again correspond to a valid shape . secondly , we prove that the introduction of shape priors into variational image segmentation leads to functionals which are convex with respect to shape deformations . for a large class of commonly considered -lrb- spatially continuous -rrb- functionals , we prove that -- under mild regularity assumptions -- segmentation and tracking with statistical shape priors can be performed in a globally optimal manner . in experiments on tracking a walking person through a cluttered scene we demonstrate the advantage of global versus local optimality .",0
1970,"This paper presents a method for approximate signal processing using incremental refinement and deadline-based algorithms. The method is based on multi-stage incremental refinement algorithms that aim to optimize a tradeoff between computational cost and solution quality. The paper proposes design criteria for such algorithms and presents an analysis of their computational complexity. The proposed method provides a way to perform approximate signal processing while meeting real-time constraints imposed by deadlines. The results show that the method can achieve high-quality solutions while minimizing computational cost, making it a promising approach for resource-constrained systems.",1
1971,"a framework for approximate signal processing is introduced which can be used to design novel classes of algorithms for performing dft and stft calculations . in particular , we focus on the derivation of multi-stage incremen-tal reenement algorithms that meet a variety of design criteria on the tradeoo achieved at each stage between solution quality and computational cost .",0
1972,"This paper presents an improved speaker model migration technique for speaker recognition technology by using stochastic synthesis of feature sequences. The proposed statistical migration technique addresses the legacy problems of parametrically-obsolete models and migrated accounts mismatch. It compares the migrated models using the baseline mean-only migration technique with the new method that incorporates Gaussian mixture models and covariance information. Evaluation of the proposed technique is conducted using the NIST 2003 Cellular Task and shows that it outperforms the mean-only method in terms of accuracy. The paper also highlights the importance of priors and the use of Gaussian means in model migration. Overall, the results demonstrate that the stochastic synthesis of feature sequences can effectively improve the speaker model migration for speaker recognition technology.",1
1973,"model migration in speaker recognition is a task of converting parametrically-obsolete models to new structures and configurations without the requirement to store the original speech waveforms or feature vector sequences along with the model migration . the need for model migration arises in large-scale deployments of speaker recognition technology in which the potential for legacy problems increases as the evolving technology may require configuration changes thus invalidating already existing user voice accounts . a migration may represent the only alternative to otherwise costly user re-enrollment or waveform storage and , as a new research problem , presents the challenge of developing algorithms to minimize the loss in accuracy in the migrated accounts . this paper reports on further enhancements of a statistical migration technique based on gaussian mixture models , introduced previously . the present statistical migration technique is based on a stochastic synthesis of feature sequences from obsolete model migration that are subsequently used to create the new model migration . here , in addition to gaussian means and priors , as utilized in the previous contribution , also the covariances are included resulting in significant performance gains in the migrated models , compared to the mean-only method . overall , measured on the nist 2003 cellular task , the described statistical migration technique achieves a model migration incurring a loss in performance of 8-20 % relative to a full re-enrollment from waveforms , dependent on the type of mismatch between the obsolete and the new configuration . the inclusion of the covariance information is shown to reduce the loss of performance by a factor of 3-4 as compared to the baseline mean-only migration technique .",0
1974,"This paper proposes an interactive visualization tool for analyzing human-machine dialogs in directed-dialog and natural-language applications. The tool is designed to support empirical dialog trajectory analysis, fine-grained analysis of call data, and system evaluation for automated spoken dialog systems. The system incorporates stochastic finite state machines and an automatic tokenization procedure to provide a feed of call-logs for dialog system developers. The paper describes how the interactive tool can help diagnose problems and provide business intelligence, using systematic procedures for complexity reduction. The proposed tool can be used for a wide range of applications, including web dialogs.",1
1975,"automated spoken dialog automated spoken dialog systems require systematic procedures for evaluating performance and diagnosing problems . we present an interactive tool that provides graphical views of how callers navigate through such automated spoken dialog systems , enabling fine-grained analysis for system evaluation and business intelligence . the input is a feed of call-logs . the output is an empirical dialog trajectory analysis represented as stochastic finite state machines , accessible via the web . complexity is managed by an automatic tokenization procedure that hides fine details until needed . users can generate selective views of parts of the dialog at high resolution -lrb- with access to call data -rrb- , or zoom out to a summary . the interactive tool provides dialog system developers with all the information they need from a single source , and is in use with directed-dialog and natural-language applications .",0
1976,"This paper provides an overview of active contours and their applications in MRI, specifically in the cancellation of motion artifacts. Active contours, also known as snakes, are widely used in image processing and computer vision for object segmentation and boundary detection. The paper describes how active contours can be used to track and cancel motion artifacts in MRI images by modeling the motion parameters and incorporating them into the contour's energy function. The paper also discusses the challenges associated with motion artifact cancellation in MRI, including noise and patient motion, and how active contours can help mitigate these challenges. The proposed method is evaluated on subband images of noise-free MRIs corrupted by motion artifacts with known motion parameters. Results show that the proposed method outperforms other methods in canceling motion artifacts caused by both patient motion and relative motion between slices over time.",1
1977,"motion can be estimated by detectirtg the edges of a mof -rrb- iny object using actit -rrb- e contours , and registering them to , yether to obtain the motion model parameters . this idea can be applied to patient motion during the acquisition of an mri to eliminate motion artifacts in the image . the data obtained durin , y the mri acquzs-tio ~ ~ , the k-space , can be dil -rsb- ided into several . subbands such that each subband is acquired in a small fraction of the full ima , ying time . these sub bands create in -lsb- -rsb- -lrb- ~ riant tissue feature maps called subband images . usin , q acti ~ le contours , the relative motion is analyzed acros ~ th -lrb- ' diferent sub bor -rrb- , d image ~ to determine the motio ~ l parameters . usin , y these motion parameters at ispossible to correct the subbands , thus correctin , y the k ; - sp -lrb- ~ ce . this has the potential to yield clear , noise-free mr ima , ye.s .",0
1978,"This paper proposes a novel approach to learning a world model and planning with a self-organizing dynamic neural system. The system consists of a growing self-organizing layer and a motor layer connected with lateral connectivity. The system uses Hebbian ideas to adapt and learn state representations from perceptions. The motor signals and state transitions are used to plan behavior. The proposed system uses a connectionist architecture and dynamic field theory to achieve this. The paper provides an overview of the system, including its architecture, learning process, and planning process. The proposed system has potential applications in various fields, including robotics and artificial intelligence.",1
1979,"we present a connectionist architecture that can learn a model of the relations between perceptions and actions and use this model for behavior planning . state representations are learned with a growing self-organizing layer which is directly coupled to a perception and a motor layer . knowledge about possible state transitions is encoded in the lateral connectivity . motor signals modulate this lateral connectivity and a dynamic field on the layer organizes a planning process . all mechanisms are local and adaptation is based on hebbian ideas . the model is continuous in the action , perception , and time domain .",0
1980,"This paper proposes the use of semantic kernels for semantic parsing, specifically in the context of concept segmentation and labeling. The semantic kernels are based on semantic similarity measures between tree structures that represent the semantic information of the input sentences. These kernels are used to compute the similarity between different semantic parsing models in the restaurant domain. The paper introduces a new approach to construct semantic tree kernels using Brown clusters, which are used to group words with similar syntactic and semantic properties. The proposed approach achieves state-of-the-art results on the benchmark dataset for semantic parsing, and the paper provides a detailed analysis of the performance of the different semantic kernels. The experimental results demonstrate that the proposed approach is effective in improving the performance of the CSL parser for semantic parsing tasks.",1
1981,"we present an empirical study on the use of semantic information for concept seg-mentation and labeling , which is an important step for semantic parsing . we represent the alternative analyses output by a state-of-the-art csl parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and brown clusters , and another one using semantic similarity among the words composing the structure . the results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures out-perform state-of-the-art rerankers .",0
1982,"This paper presents a new approach to clustering, called ""outlier-aware robust clustering"", which takes into account the scarcity of outliers in a given domain. The traditional clustering algorithms are not designed to handle outlier data points effectively, which can often result in poorly defined clusters. The proposed method combines the advantages of fuzzy k-means and probabilistic clustering to achieve outlier-aware clustering schemes. The algorithm judiciously separates the well-separated subsets of input vectors into hard and fuzzy clusters, thus eliminating the need for separate clustering schemes for outliers. The paper also introduces a closed-form solution for computing the cluster centers, which reduces the computational complexity of the clustering process. The experimental results demonstrate that the proposed method outperforms the outlier-agnostic counterparts on model-incompatible inputs with sparsity constraints.",1
1983,"clustering is a basic task in a variety of machine learning applications . partitioning a set of input vectors into compact , well-separated subsets can be severely affected by the presence of model-incompatible inputs called outliers . the present paper develops robust clustering algorithms for jointly partitioning the data and identifying the outliers . the novel approach relies on translating scarcity of outliers to sparsity in a judiciously defined domain , to ro-bustify three widely used clustering schemes : hard k-means , fuzzy k-means , and probabilistic clustering . cluster centers and assignments are iteratively updated in closed form . the developed outlier-aware algorithms are guaranteed to converge , while their computational complexity is of the same order as their outlier-agnostic counterparts . preliminary simulations validate the analytical claims .",0
1984,"This paper describes the NCU Chinese Word Segmentation and Part-of-Speech Tagging system, which participated in the SIGHAN Bakeoff 2007. The system utilizes sequential tagging models, including a support vector machine based chunking model and a conditional random fields model for part-of-speech tagging. Predefined dictionaries are used to aid in word segmentation and the identification of word boundaries, and syntactic labels are assigned to segmented words. The system achieved high rankings in both the word segmentation and part-of-speech tagging tasks, demonstrating its effectiveness for Chinese language processing.",1
1985,"in chinese , most of the language processing starts from word segmentation and part-of-speech tagging . these two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word . in this paper , we present two distinct sequential tagging models for the above two tasks . the first sequential tagging models was basically similar to previous work which made use of conditional random fields and set of predefined dictionaries to recognize word boundaries . second , we revise and modify support vector machine based chunking model to label the pos tag in the pos tagging task . our support vector machine based chunking model in the ws task achieves moderately rank among all participants , while in the pos tagging task , it reaches very competitive results .",0
1986,"This paper proposes a novel method for instantaneous frequency (IF) estimation based on the robust spectrogram (RSPEC). The RSPEC-based IF estimator is designed to handle rare, high-magnitude noise values that are commonly found in nonstationary signals. By analyzing signals with a time-varying window length and an adaptive algorithm that balances the bias-variance trade-off, the proposed method achieves higher accuracy than the conventional m-periodogram. Experimental results demonstrate the effectiveness of the proposed approach in dealing with heavy-tailed distribution noise and in the analysis of nonstationary signals. Overall, the RSPEC-based IF estimator is a promising tool for accurate and robust analysis of signals.",1
1987,"robust m-periodogram is defined for the analysis of signals with heavy-tailed distribution noise . in the form of a robust spectrogram -lrb- rspec -rrb- rspec based instantaneous frequency estimator can be used for the analysis of nonstationary signals . in this paper a rspec based instantaneous frequency estimator , with a time-varying window length , is presented . the optimal choice of the window length can resolve the bias-variance trade-off in the rspec based if estimation . however , rspec based instantaneous frequency estimator depends on the unknown nonlinearity of the rspec based instantaneous frequency estimator . the rspec based instantaneous frequency estimator used in this paper is able to provide the accuracy close to the one that could be achieved if the rspec based instantaneous frequency estimator , to be estimated , were known in advance . simulations show good accuracy ability of the adaptive algorithm and good robustness property with respect to rare high magnitude noise values .",0
1988,"This paper evaluates a computationally simple method called vote-based random-projection combination for speech recognition. The method involves using random matrices for dimensionality reduction and sub-space projection of speech features, followed by combination through a voting scheme. The paper focuses on evaluating the accuracy of this method in word recognition tasks using the Euclidean distance metric. The results show that the random-projection-based features outperform the original features and that the accuracy is further improved by combining them. This demonstrates the potential of random-projection-based feature combination for improving speech recognition accuracy.",1
1989,"random projection has been suggested as a means of dimension-ality reduction , where the original data are projected onto a sub-space using a random matrix . it represents a computationally simple method that approximately preserves the euclidean distance of any two points through the projection . moreover , as we are able to produce various random matrices , there may be some possibility of finding a random matrix that gives a better speech recognition accuracy among these random matrices . in this paper , we investigate the feasibility of random projection for speech feature extraction . to obtain an optimal result from among many -lrb- infinite -rrb- random matrices , a vote-based random-projection combination is introduced in this paper , where vote-based random-projection combination is applied to random-projection-based features . its effectiveness is confirmed by word recognition experiments .",0
1990,"Abstract:

In this paper, we present a method towards internet-scale multi-view stereo, which addresses the challenging problem of 3D reconstruction from unstructured photo collections available on the internet. The proposed method utilizes multi-view stereo methods and global visibility constraints to address the overlapping clustering problem. To deal with low-quality reconstructions, we use a merging algorithm that leverages constrained optimization and filtering steps. Our approach is evaluated on a large dataset from flickr.com, demonstrating its ability to handle internet-scale data and produce accurate 3D reconstructions.",1
1991,"this paper introduces an approach for enabling existing multi-view stereo methods to operate on extremely large unstructured photo collections . the main idea is to decompose the collection into a set of overlapping sets of photos that can be processed in parallel , and to merge the resulting reconstructions . this overlapping clustering problem is formulated as a constrained optimization and solved iteratively . the merging algorithm , designed to be parallel and out-of-core , incorporates robust filtering steps to eliminate low-quality reconstructions and enforce global visibility constraints . the approach has been tested on several large datasets downloaded from flickr.com , including one with over ten thousand images , yielding a 3d reconstruction with nearly thirty million points .",0
1992,"This paper proposes an approach to address the problem of data distribution mismatches in visual recognition tasks by adapting the training data to better match the test data. The proposed method involves reshaping image or video datasets by identifying latent domains and formulating a nonparametric optimization procedure to adapt the training data to better fit the test data distribution. The approach considers maximum distinctiveness and maximum learnability objectives to build a discriminative model for object recognition and human activity recognition tasks. The method includes adaptation algorithms and filtering steps to handle low-quality reconstructions, global visibility constraints, and overlapping clustering problems. The paper also presents experimental results on adapting datasets for pose and background resolution, using Flickr.com as a case study.",1
1993,"in visual recognition problems , the common data distribution mismatches between training and testing make domain adaptation essential . however , image data is difficult to manually divide into the discrete domains required by adaptation algorithms , and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways -lrb- lighting , pose , background , resolution , etc. -rrb- we propose an approach to automatically discover latent domains in image or video datasets . our formulation imposes two key properties on domains : maximum distinctiveness and maximum learnability . by maximum distinctiveness , we require the underlying distributions of the identified domains to be different from each other to the maximum extent ; by maximum learnability , we ensure that a strong discriminative model can be learned from the domain . we devise a nonparametric formulation and efficient optimization procedure that can successfully discover domains among both training and test data . we extensively evaluate our approach on object recognition and human activity recognition tasks .",0
1994,"This paper proposes a new paradigm for video coding called the Smart decoder. The aim is to improve the coding efficiency of the High Efficiency Video Coding (HEVC) standard by reducing the average bitrate while maintaining the same video quality. The proposed Smart decoder approach focuses on exploiting the spatio-temporal redundancies present in video sequences to optimize the coding modes. It achieves this by introducing a causal reference signaling mechanism between the encoder and decoder. The Smart decoder method includes a set of coding modes that adapt to the video content, making it possible to achieve better coding efficiency. The paper evaluates the Smart decoder approach and shows that it can save an average bitrate of up to 50% compared to the HEVC standard, while maintaining similar video quality.",1
1995,"the coding efficiency of the new video coding standard , high efficiency video coding , is strongly associated with better use of spatio-temporal redundancies thanks to an increased number of competing coding modes . however , this competition involves a massive increase in signaling bitrate which becomes a possible limit for the next generation of en-coder . this paper proposes a new coding scheme that breaks with conventional approaches . coding scheme exploits a more complex decoder able to reproduce the choice of the encoder based on causal references , eliminating thus the need to signal coding modes and associated parameters . the general outline of this new coding scheme and a proposed implementation are described in this paper . experimental results under common test conditions report an average bitrate saving of 1.7 % at the same quality compared to hevc for a wide range of video sequences .",0
1996,"This paper proposes a novel approach for predicting the performance of the IDA* search algorithm using conditional distributions of heuristic values. The authors compare the performance of IDA* using static distributions of heuristic values with that of using conditional distributions, for both single-start states and random sample start states. They also examine the impact of inconsistent heuristics on the predictive accuracy of the two approaches. Results show that using conditional distributions can significantly improve the accuracy of performance prediction, particularly when the heuristic is inconsistent. This approach has the potential to enhance the efficiency of IDA* and improve its application in real-world problems.",1
1997,"-lrb- korf , reid , and edelkamp 2001 -rrb- introduced a formula to predict the number of nodes ida * will expand given the static distribution of heuristic values . their formula proved to be very accurate but it is only accurate under the following limitations : -lrb- 1 -rrb- the heuristic must be consistent ; -lrb- 2 -rrb- the prediction is for a large random sample of start states -lrb- or for large thresholds -rrb- . in this paper we generalize the static distribution to a conditional distribution of heuristic values . we then propose a new formula for predicting the performance of ida * that works well for inconsistent heuristics -lrb- zahavi et al. 2007 -rrb- and for any set of start states , not just a random sample . we also show how the formula can be enhanced to work well for single start states . experimental results demonstrate the accuracy of our method in all these situations .",0
1998,This paper proposes a novel technique for identifying multipath channel parameters using Particle Swarm Optimization (PSO) in the cross-ambiguity domain. The PSO-based technique is compared with the traditional Array Signal Processing (ASP) technique and the Generalized Expectation Maximization (GEM) technique. The optimization problem involves determining the transmitted signal and SNR values. The results show that the PSO-based technique outperforms the other two techniques in terms of identifying channel parameters accurately. The proposed technique provides an efficient and effective solution for channel identification in wireless communication systems.,1
1999,"in this paper , a new array signal processing technique by using particle swarm optimization is proposed to identify multipath channel parameters . the proposed array signal processing technique provides estimates to the channel parameters by finding a global minimum of an optimization problem . since the optimization problem is formulated in the cross-ambiguity function -lrb- caf -rrb- domain of the transmitted signal and the received array outputs , the proposed array signal processing technique is called as particle swarm optimization . the performance of the particle swarm optimization is compared with the space alternating generalized expectation maximization technique and with another recently proposed pso based technique for various snr values . simulation results indicate the superior performance of the pso based technique over mentioned techniques for all snr values .",0
